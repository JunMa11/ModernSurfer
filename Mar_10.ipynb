{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b28a44-af35-4a4c-bc96-a10896200291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Mar_28_02:18:24_PDT_2024\n",
      "Cuda compilation tools, release 12.4, V12.4.131\n",
      "Build cuda_12.4.r12.4/compiler.34097967_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f556c376-36bf-4418-b6f4-ea0958cc23c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TensorRT-Model-Optimizer'...\n",
      "remote: Enumerating objects: 4912, done.\u001b[K\n",
      "remote: Counting objects: 100% (1078/1078), done.\u001b[K\n",
      "remote: Compressing objects: 100% (811/811), done.\u001b[K\n",
      "remote: Total 4912 (delta 678), reused 436 (delta 254), pack-reused 3834 (from 1)\u001b[K\n",
      "Receiving objects: 100% (4912/4912), 23.68 MiB | 50.41 MiB/s, done.\n",
      "Resolving deltas: 100% (3431/3431), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/TensorRT-Model-Optimizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9d0b02-4ee0-4771-a418-9665fa3df4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu124\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (768.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.4/768.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting torch-tensorrt\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch_tensorrt-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting tensorrt\n",
      "  Downloading tensorrt-10.9.0.34.tar.gz (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "donePreparing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch) (2025.2.0)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting triton==3.2.0\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorrt\n",
      "  Downloading tensorrt-10.7.0.post1.tar.gz (35 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting tensorrt-cu12-bindings<10.8.0,>=10.7.0\n",
      "  Downloading tensorrt_cu12_bindings-10.7.0.post1-cp310-none-manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: packaging>=23 in /venv/main/lib/python3.10/site-packages (from torch-tensorrt) (24.2)\n",
      "Collecting tensorrt-cu12<10.8.0,>=10.7.0.post1\n",
      "  Downloading tensorrt_cu12-10.7.0.post1.tar.gz (18 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting tensorrt-cu12-libs<10.8.0,>=10.7.0\n",
      "  Downloading tensorrt_cu12_libs-10.7.0.post1.tar.gz (710 bytes)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "done5h  Getting requirements to build wheel ... \u001b[?25l\n",
      "donePreparing metadata (pyproject.toml) ... \u001b[?25l\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Building wheels for collected packages: tensorrt, tensorrt-cu12, tensorrt-cu12-libs\n",
      "doneng wheel for tensorrt (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for tensorrt: filename=tensorrt-10.7.0.post1-py2.py3-none-any.whl size=42186 sha256=326a5a1ca3fc3b7099594e7584e28731a4b1629cd6bb469ba591dec76d174a40\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/3a/4a/7cbc5f10f993bc284e6db8bb2e72cac9f1b5811da7026f72e3\n",
      "donerrt-cu12 (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for tensorrt-cu12: filename=tensorrt_cu12-10.7.0.post1-py2.py3-none-any.whl size=17648 sha256=9ed2bd2d6ad0564d9427f146fb4b7fac06832d51d90be2fea6df5d81819e7851\n",
      "  Stored in directory: /root/.cache/pip/wheels/13/40/e7/5dfa276afaf6a78b52d5728547a0fc994811f637f87e3c18e3\n",
      "doneilding wheel for tensorrt-cu12-libs (pyproject.toml) ... \u001b[?25l\n",
      "  Created wheel for tensorrt-cu12-libs: filename=tensorrt_cu12_libs-10.7.0.post1-py2.py3-none-manylinux_2_17_x86_64.whl size=2069981220 sha256=ab4b8ef46a113f2e704b4c755c4db89789d70069c806d50683e17811060a09e2\n",
      "  Stored in directory: /root/.cache/pip/wheels/f4/72/5a/fb0beda45fbeb8b24e490aa68c0e35a274179a04f5ae08e39a\n",
      "Successfully built tensorrt tensorrt-cu12 tensorrt-cu12-libs\n",
      "Installing collected packages: triton, tensorrt-cu12-bindings, nvidia-cusparselt-cu12, mpmath, tensorrt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, tensorrt-cu12-libs, tensorrt, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torch-tensorrt\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 tensorrt-10.7.0.post1 tensorrt-cu12-10.7.0.post1 tensorrt-cu12-bindings-10.7.0.post1 tensorrt-cu12-libs-10.7.0.post1 torch-2.6.0+cu124 torch-tensorrt-2.6.0+cu124 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch torch-tensorrt tensorrt --extra-index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f2fa40a-2144-487e-8ee1-9a468d5bc6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting nvidia-modelopt[all]\n",
      "  Downloading https://pypi.nvidia.com/nvidia-modelopt/nvidia_modelopt-0.25.0-py3-none-manylinux2014_x86_64.whl (616 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.5/616.5 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting cloudpickle>=1.6.0\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting nvidia-modelopt-core==0.25.0\n",
      "  Downloading https://pypi.nvidia.com/nvidia-modelopt-core/nvidia_modelopt_core-0.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /venv/main/lib/python3.10/site-packages (from nvidia-modelopt[all]) (4.67.1)\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting pydantic>=2.0\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting rich\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /venv/main/lib/python3.10/site-packages (from nvidia-modelopt[all]) (24.2)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting numpy<2\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting cppimport\n",
      "  Downloading cppimport-22.8.2.tar.gz (26 kB)\n",
      "  Installing build dependencies ..done\n",
      "done5h  Getting requirements to build wheel ... \u001b[?25l\n",
      "donePreparing metadata (pyproject.toml) ... \u001b[?25l\n",
      "Collecting onnxconverter-common\n",
      "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting regex\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting torchprofile>=0.0.4\n",
      "  Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
      "Collecting pynvml\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Collecting onnx\n",
      "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting accelerate>=0.27.2\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting onnx-graphsurgeon\n",
      "  Downloading https://pypi.nvidia.com/onnx-graphsurgeon/onnx_graphsurgeon-0.5.6-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 KB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pulp\n",
      "  Downloading PuLP-3.0.2-py3-none-any.whl (17.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "Collecting datasets>=2.16.1\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting onnxmltools\n",
      "  Downloading onnxmltools-1.13.0-py2.py3-none-any.whl (328 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting diffusers>=0.27.2\n",
      "  Downloading diffusers-0.32.2-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting cupy-cuda12x\n",
      "  Downloading cupy_cuda12x-13.4.0-cp310-cp310-manylinux2014_x86_64.whl (104.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: torch>=2.2 in /venv/main/lib/python3.10/site-packages (from nvidia-modelopt[all]) (2.6.0+cu124)\n",
      "Collecting transformers>=4.40.2\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub>=0.24.0 in /venv/main/lib/python3.10/site-packages (from nvidia-modelopt[all]) (0.28.1)\n",
      "Collecting safetensors\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting onnxruntime-gpu~=1.20.1\n",
      "  Downloading onnxruntime_gpu-1.20.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (291.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.5/291.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting peft>=0.12.0\n",
      "  Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 KB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from accelerate>=0.27.2->nvidia-modelopt[all]) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.10/site-packages (from accelerate>=0.27.2->nvidia-modelopt[all]) (6.0.2)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.11.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting fsspec[http]<=2024.12.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 KB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (2.32.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (3.17.0)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting Pillow\n",
      "  Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.24.0->nvidia-modelopt[all]) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.24.0->nvidia-modelopt[all]) (2025.2.0)\n",
      "Requirement already satisfied: sympy in /venv/main/lib/python3.10/site-packages (from onnxruntime-gpu~=1.20.1->nvidia-modelopt[all]) (1.13.1)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-6.30.0-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 KB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (11.2.1.3)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (9.1.0.70)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.5.8)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy->onnxruntime-gpu~=1.20.1->nvidia-modelopt[all]) (1.3.0)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting pybind11\n",
      "  Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 KB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting mako\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting fastrlock>=0.5\n",
      "  Downloading fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /venv/main/lib/python3.10/site-packages (from rich->nvidia-modelopt[all]) (2.19.1)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 KB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 KB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 KB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.5.0-py3-none-any.whl (15 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 KB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (3.4.1)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting zipp>=3.20\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch>=2.2->nvidia-modelopt[all]) (3.0.2)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 KB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 KB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.10/site-packages (from pandas->datasets>=2.16.1->nvidia-modelopt[all]) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.1->nvidia-modelopt[all]) (1.17.0)\n",
      "Building wheels for collected packages: cppimport\n",
      "doneng wheel for cppimport (pyproject.toml) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for cppimport: filename=cppimport-22.8.2-py3-none-any.whl size=17699 sha256=ab936c6c9d229e39a31018e4bbb62033a4f4dbf8a038ebac44783513a5de9aa5\n",
      "  Stored in directory: /root/.cache/pip/wheels/38/0d/cd/d3a3135529beaf8c8fde3957e56ff89c25cd14ab18358bd724\n",
      "Successfully built cppimport\n",
      "Installing collected packages: pytz, nvidia-ml-py, flatbuffers, fastrlock, zipp, xxhash, tzdata, safetensors, regex, pynvml, pydantic-core, pybind11, pyarrow, pulp, protobuf, propcache, Pillow, nvidia-modelopt-core, nvidia-cuda-runtime-cu12, numpy, ninja, multidict, mdurl, mako, humanfriendly, fsspec, frozenlist, dill, cloudpickle, attrs, async-timeout, annotated-types, aiohappyeyeballs, yarl, scipy, pydantic, pandas, onnx, multiprocess, markdown-it-py, importlib-metadata, cupy-cuda12x, cppimport, coloredlogs, aiosignal, tokenizers, rich, onnxruntime-gpu, onnxconverter-common, onnx-graphsurgeon, diffusers, aiohttp, transformers, torchvision, onnxmltools, nvidia-modelopt, accelerate, torchprofile, peft, datasets\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "Successfully installed Pillow-11.1.0 accelerate-1.4.0 aiohappyeyeballs-2.5.0 aiohttp-3.11.13 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-5.0.1 attrs-25.1.0 cloudpickle-3.1.1 coloredlogs-15.0.1 cppimport-22.8.2 cupy-cuda12x-13.4.0 datasets-3.3.2 diffusers-0.32.2 dill-0.3.8 fastrlock-0.8.3 flatbuffers-25.2.10 frozenlist-1.5.0 fsspec-2024.12.0 humanfriendly-10.0 importlib-metadata-8.6.1 mako-1.3.9 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 ninja-1.11.1.3 numpy-1.26.4 nvidia-cuda-runtime-cu12-12.4.127 nvidia-ml-py-12.570.86 nvidia-modelopt-0.25.0 nvidia-modelopt-core-0.25.0 onnx-1.17.0 onnx-graphsurgeon-0.5.6 onnxconverter-common-1.14.0 onnxmltools-1.13.0 onnxruntime-gpu-1.20.2 pandas-2.2.3 peft-0.14.0 propcache-0.3.0 protobuf-3.20.2 pulp-3.0.2 pyarrow-19.0.1 pybind11-2.13.6 pydantic-2.10.6 pydantic-core-2.27.2 pynvml-12.0.0 pytz-2025.1 regex-2024.11.6 rich-13.9.4 safetensors-0.5.3 scipy-1.15.2 tokenizers-0.21.0 torchprofile-0.0.4 torchvision-0.21.0 transformers-4.49.0 tzdata-2025.1 xxhash-3.5.0 yarl-1.18.3 zipp-3.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"nvidia-modelopt[all]\" -U --extra-index-url https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9348c3-7d8b-4856-9291-18045f828ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No custom ops found. If that's not correct, please make sure that the 'tensorrt' python package is correctly installed and that the paths to 'libcudnn*.so' and TensorRT 'lib/' are in 'LD_LIBRARY_PATH'. If the custom op is not directly available as a plugin in TensorRT, please also make sure that the path to the compiled '.so' TensorRT plugin is also being given via the  '--trt_plugins' flag (requires TRT 10+).\n",
      "INFO:root:Model /workspace/onnx_models/fast_unet_fp32.onnx with opset_version 16 is loaded.\n",
      "INFO:root:Quantization Mode: int8\n",
      "INFO:root:Quantizable op types in the model: ['Conv']\n",
      "INFO:root:Building non-residual Add input map ...\n",
      "INFO:root:Searching for hard-coded patterns like MHA, LayerNorm, etc. to avoid quantization.\n",
      "INFO:root:Building KGEN/CASK targeted partitions ...\n",
      "INFO:root:Classifying the partition nodes ...\n",
      "INFO:root:Total number of nodes: 91\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "\u001b[0;93m2025-03-11 05:18:21.091951264 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091968039 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091970424 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091972492 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091974646 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091976769 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091978587 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091980428 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091982512 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091984800 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091986705 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091988566 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091990533 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091992760 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091994731 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091996616 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.091998629 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092001703 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092003649 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092005585 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092007574 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092009450 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092011295 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092013068 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092015253 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092017791 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092019602 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092021457 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092023518 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092025663 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092027472 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092029310 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092031204 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092033609 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092035482 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092037422 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092039553 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092041439 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092043391 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092045436 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092047607 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092049945 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092051938 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092053849 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092056601 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092058478 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092060873 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092062836 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092064980 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092067275 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092069178 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092071108 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092073121 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092074929 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092076703 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092078565 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092080569 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092082893 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092084828 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092087836 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092089827 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092091622 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092093514 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092095431 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092097517 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092099792 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092101576 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092103378 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092105330 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092107187 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092108915 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092110675 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092112741 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092114933 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092116891 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092118823 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092120759 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092122547 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092124326 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092126119 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092128054 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092130378 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092132166 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092133928 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092135823 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092137654 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092139450 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092141247 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092143169 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092145484 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092147299 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092149059 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092151091 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092153038 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092154831 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092156606 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092158522 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092160741 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092162584 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092164361 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092166230 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092168071 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092169860 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092171615 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092173713 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.0.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092176148 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.0.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092178212 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.1.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092180064 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.1.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092184567 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.2.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092186561 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.2.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092188479 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.3.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092190333 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.3.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092192276 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.4.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092194509 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.4.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092196969 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.5.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092198776 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.5.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092200690 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.seg_layers.5.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-11 05:18:21.092202528 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.seg_layers.5.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "INFO:root:Deleting QDQ nodes from marked inputs to make certain operations fusible ...\n",
      "INFO:root:Total number of quantized nodes: 33\n",
      "INFO:root:Quantized type counts: {'Conv': 27, 'Concat': 6}\n",
      "INFO:root:Quantized onnx model is saved as /workspace/onnx_models/quant_fast_unet_int8.onnx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Example numpy file for single-input ONNX\n",
    "calib_data = np.random.randn(1, 1, 64, 256, 256)\n",
    "calib_data = calib_data.astype(np.float32)\n",
    "np.save(\"calib_data.npy\", calib_data)\n",
    "\n",
    "# Example numpy file for single/multi-input ONNX\n",
    "# Dict key should match the input names of ONNX\n",
    "# calib_data = {\n",
    "#     \"input_name\": np.random.randn(*shape),\n",
    "#     \"input_name2\": np.random.randn(*shape2),\n",
    "# }\n",
    "# np.savez(\"/workspace/calib_data.npz\", calib_data)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, './TensorRT-Model-Optimizer')\n",
    "\n",
    "import modelopt.onnx.quantization as moq\n",
    "import numpy as np\n",
    "\n",
    "calibration_data_path = 'calib_data.npy'\n",
    "# onnx_path = \"vit_base_patch16_224.onnx\"\n",
    "#\n",
    "calibration_data = np.load(calibration_data_path)\n",
    "\n",
    "moq.quantize(\n",
    "    onnx_path=\"/workspace/onnx_models/fast_unet_fp32.onnx\",\n",
    "    calibration_data=calibration_data,\n",
    "    calibration_method='max',\n",
    "    output_path=\"/workspace/onnx_models/quant_fast_unet_int8.onnx\",\n",
    "    quantize_mode=\"int8\",\n",
    "    high_precision_dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196535fe-84cb-411a-b9fe-57b915cde962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading onnx model from path: /workspace/onnx_models/quant_fast_unet_int8.onnx\n",
      "Compiling the TensorRT engine. This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:trtexec --onnx=/tmp/tmpxfybx2up/onnx/quant_fast_unet_int8.onnx --fp16 --int8 --saveEngine=/tmp/tmpxfybx2up/quant_fast_unet_int8/quant_fast_unet_int8.engine --skipInference --builderOptimizationLevel=4 --verbose --exportLayerInfo=/tmp/tmpxfybx2up/quant_fast_unet_int8/quant_fast_unet_int8.engine.graph.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing onnx model to path: /tmp/tmpxfybx2up/onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:build_engine: 431386.0092163086 ms\n",
      "INFO:root:trtexec --loadEngine=/tmp/tmpauz27w9h/engine --warmUp=500 --avgRuns=100 --iterations=100 --dumpProfile --separateProfileRun --exportProfile=/tmp/tmpauz27w9h/profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation completed.\n",
      "Size of the TensorRT engine: 49.57 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:profile_engine: 10231.824398040771 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference latency reported by device_model: 43.02462857142857 ms\n"
     ]
    }
   ],
   "source": [
    "from modelopt.torch._deploy._runtime import RuntimeRegistry\n",
    "from modelopt.torch._deploy.device_model import DeviceModel\n",
    "from modelopt.torch._deploy.utils import OnnxBytes\n",
    "# Configure deployment\n",
    "deployment = {\n",
    "    \"runtime\": \"TRT\",\n",
    "    \"version\": \"10.3\",\n",
    "    \"precision\": 'int8',\n",
    "}\n",
    "\n",
    "# Create an ONNX bytes object\n",
    "onnx_bytes = OnnxBytes('/workspace/onnx_models/quant_fast_unet_int8.onnx').to_bytes()\n",
    "\n",
    "# Get the runtime client\n",
    "client = RuntimeRegistry.get(deployment)\n",
    "\n",
    "# Compile the TRT model\n",
    "print(\"Compiling the TensorRT engine. This may take a few minutes...\")\n",
    "compiled_model = client.ir_to_compiled(onnx_bytes)\n",
    "print(\"Compilation completed.\")\n",
    "\n",
    "# Print size of the compiled model\n",
    "engine_size = len(compiled_model)\n",
    "print(f\"Size of the TensorRT engine: {engine_size / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# Create the device model\n",
    "device_model = DeviceModel(client, compiled_model, metadata={})\n",
    "print(f\"Inference latency reported by device_model: {device_model.get_latency()} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ab014c-f86b-4ef5-9273-f074230eea5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-11 05:21:26--  https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz\n",
      "Resolving developer.nvidia.com (developer.nvidia.com)... 23.53.4.17, 23.53.4.11\n",
      "Connecting to developer.nvidia.com (developer.nvidia.com)|23.53.4.17|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://developer.download.nvidia.com/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz [following]\n",
      "--2025-03-11 05:21:26--  https://developer.download.nvidia.com/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz\n",
      "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.53.4.17, 23.53.4.11\n",
      "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.53.4.17|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4381146404 (4.1G) [application/x-gzip]\n",
      "Saving to: ‘TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz’\n",
      "\n",
      "TensorRT-10.3.0.26. 100%[===================>]   4.08G  23.2MB/s    in 3m 20s  \n",
      "\n",
      "2025-03-11 05:24:46 (20.9 MB/s) - ‘TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz’ saved [4381146404/4381146404]\n",
      "\n",
      "TensorRT-10.3.0.26/\n",
      "TensorRT-10.3.0.26/data/\n",
      "TensorRT-10.3.0.26/data/mnist/\n",
      "TensorRT-10.3.0.26/data/mnist/2.pgm\n",
      "TensorRT-10.3.0.26/data/mnist/1.pgm\n",
      "TensorRT-10.3.0.26/data/mnist/mnist.onnx\n",
      "TensorRT-10.3.0.26/data/mnist/7.pgm\n",
      "TensorRT-10.3.0.26/data/mnist/6.pgm\n",
      "TensorRT-10.3.0.26/data/mnist/README.md\n",
      "TensorRT-10.3.0.26/data/mnist/5.pgm\n",
      "TensorRT-10.3.0.26/data/mnist/9.pgm\n",
      "TensorRT-10.3.0.26/data/mnist/4.pgm\n",
      "TensorRT-10.3.0.26/data/mnist/8.pgm\n",
      "TensorRT-10.3.0.26/data/mnist/3.pgm\n",
      "TensorRT-10.3.0.26/data/mnist/0.pgm\n",
      "TensorRT-10.3.0.26/data/int8_api/\n",
      "TensorRT-10.3.0.26/data/int8_api/README.md\n",
      "TensorRT-10.3.0.26/data/int8_api/resnet50_per_tensor_dynamic_range.txt\n",
      "TensorRT-10.3.0.26/data/int8_api/reference_labels.txt\n",
      "TensorRT-10.3.0.26/data/int8_api/airliner.ppm\n",
      "TensorRT-10.3.0.26/data/char-rnn/\n",
      "TensorRT-10.3.0.26/data/char-rnn/char-rnn.wts\n",
      "TensorRT-10.3.0.26/data/char-rnn/model/\n",
      "TensorRT-10.3.0.26/data/char-rnn/model/model-20080.data-00000-of-00001\n",
      "TensorRT-10.3.0.26/data/char-rnn/model/model-20080.meta\n",
      "TensorRT-10.3.0.26/data/char-rnn/model/model-20080.index\n",
      "TensorRT-10.3.0.26/data/char-rnn/model/checkpoint\n",
      "TensorRT-10.3.0.26/data/resnet50/\n",
      "TensorRT-10.3.0.26/data/resnet50/reflex_camera.jpeg\n",
      "TensorRT-10.3.0.26/data/resnet50/class_labels.txt\n",
      "TensorRT-10.3.0.26/data/resnet50/README.md\n",
      "TensorRT-10.3.0.26/data/resnet50/binoculars.jpeg\n",
      "TensorRT-10.3.0.26/data/resnet50/tabby_tiger_cat.jpg\n",
      "TensorRT-10.3.0.26/data/resnet50/ResNet50.onnx\n",
      "TensorRT-10.3.0.26/data/resnet50/airliner.ppm\n",
      "TensorRT-10.3.0.26/doc/\n",
      "TensorRT-10.3.0.26/doc/Readme.txt\n",
      "TensorRT-10.3.0.26/doc/Acknowledgements.txt\n",
      "TensorRT-10.3.0.26/include/\n",
      "TensorRT-10.3.0.26/include/NvInferRuntimeBase.h\n",
      "TensorRT-10.3.0.26/include/NvInferRuntimePlugin.h\n",
      "TensorRT-10.3.0.26/include/NvInferConsistency.h\n",
      "TensorRT-10.3.0.26/include/NvInferPluginUtils.h\n",
      "TensorRT-10.3.0.26/include/NvOnnxParser.h\n",
      "TensorRT-10.3.0.26/include/NvInferRuntime.h\n",
      "TensorRT-10.3.0.26/include/NvInferRuntimeCommon.h\n",
      "TensorRT-10.3.0.26/include/NvInferVersion.h\n",
      "TensorRT-10.3.0.26/include/NvInferLegacyDims.h\n",
      "TensorRT-10.3.0.26/include/NvInferPlugin.h\n",
      "TensorRT-10.3.0.26/include/NvInferConsistencyImpl.h\n",
      "TensorRT-10.3.0.26/include/NvInferSafeRuntime.h\n",
      "TensorRT-10.3.0.26/include/NvInferImpl.h\n",
      "TensorRT-10.3.0.26/include/NvInfer.h\n",
      "TensorRT-10.3.0.26/include/NvOnnxConfig.h\n",
      "TensorRT-10.3.0.26/samples/\n",
      "TensorRT-10.3.0.26/samples/sampleNamedDimensions/\n",
      "TensorRT-10.3.0.26/samples/sampleNamedDimensions/README.md\n",
      "TensorRT-10.3.0.26/samples/sampleNamedDimensions/sampleNamedDimensions.cpp\n",
      "TensorRT-10.3.0.26/samples/sampleNamedDimensions/create_model.py\n",
      "TensorRT-10.3.0.26/samples/sampleNamedDimensions/Makefile\n",
      "TensorRT-10.3.0.26/samples/sampleDynamicReshape/\n",
      "TensorRT-10.3.0.26/samples/sampleDynamicReshape/README.md\n",
      "TensorRT-10.3.0.26/samples/sampleDynamicReshape/sampleDynamicReshape.cpp\n",
      "TensorRT-10.3.0.26/samples/sampleDynamicReshape/Makefile\n",
      "TensorRT-10.3.0.26/samples/sampleINT8API/\n",
      "TensorRT-10.3.0.26/samples/sampleINT8API/sampleINT8API.cpp\n",
      "TensorRT-10.3.0.26/samples/sampleINT8API/README.md\n",
      "TensorRT-10.3.0.26/samples/sampleINT8API/Makefile\n",
      "TensorRT-10.3.0.26/samples/common/\n",
      "TensorRT-10.3.0.26/samples/common/parserOnnxConfig.h\n",
      "TensorRT-10.3.0.26/samples/common/bfloat16.cpp\n",
      "TensorRT-10.3.0.26/samples/common/logger.cpp\n",
      "TensorRT-10.3.0.26/samples/common/sampleOptions.cpp\n",
      "TensorRT-10.3.0.26/samples/common/EntropyCalibrator.h\n",
      "TensorRT-10.3.0.26/samples/common/sampleDevice.cpp\n",
      "TensorRT-10.3.0.26/samples/common/sampleReporting.h\n",
      "TensorRT-10.3.0.26/samples/common/sampleUtils.h\n",
      "TensorRT-10.3.0.26/samples/common/sampleConfig.h\n",
      "TensorRT-10.3.0.26/samples/common/argsParser.h\n",
      "TensorRT-10.3.0.26/samples/common/dumpTFWts.py\n",
      "TensorRT-10.3.0.26/samples/common/streamReader.h\n",
      "TensorRT-10.3.0.26/samples/common/sampleEngines.h\n",
      "TensorRT-10.3.0.26/samples/common/sampleEntrypoints.h\n",
      "TensorRT-10.3.0.26/samples/common/sampleUtils.cpp\n",
      "TensorRT-10.3.0.26/samples/common/sampleDevice.h\n",
      "TensorRT-10.3.0.26/samples/common/half.h\n",
      "TensorRT-10.3.0.26/samples/common/getOptions.cpp\n",
      "TensorRT-10.3.0.26/samples/common/getOptions.h\n",
      "TensorRT-10.3.0.26/samples/common/getopt.c\n",
      "TensorRT-10.3.0.26/samples/common/logging.h\n",
      "TensorRT-10.3.0.26/samples/common/getoptWin.h\n",
      "TensorRT-10.3.0.26/samples/common/BatchStream.h\n",
      "TensorRT-10.3.0.26/samples/common/safeCommon.h\n",
      "TensorRT-10.3.0.26/samples/common/ErrorRecorder.h\n",
      "TensorRT-10.3.0.26/samples/common/sampleReporting.cpp\n",
      "TensorRT-10.3.0.26/samples/common/logger.h\n",
      "TensorRT-10.3.0.26/samples/common/sampleOptions.h\n",
      "TensorRT-10.3.0.26/samples/common/common.h\n",
      "TensorRT-10.3.0.26/samples/common/sampleEngines.cpp\n",
      "TensorRT-10.3.0.26/samples/common/bfloat16.h\n",
      "TensorRT-10.3.0.26/samples/common/sampleInference.h\n",
      "TensorRT-10.3.0.26/samples/common/sampleInference.cpp\n",
      "TensorRT-10.3.0.26/samples/common/buffers.h\n",
      "TensorRT-10.3.0.26/samples/python/\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/compare_tf.py\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/create_onnx.py\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/infer.py\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/labels_coco.txt\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/README.md\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/build_engine.py\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/eval_coco.py\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/visualize.py\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/image_batcher.py\n",
      "TensorRT-10.3.0.26/samples/python/tensorflow_object_detection_api/onnx_utils.py\n",
      "TensorRT-10.3.0.26/samples/python/plugin_utils.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientnet/\n",
      "TensorRT-10.3.0.26/samples/python/efficientnet/compare_tf.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientnet/create_onnx.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientnet/infer.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientnet/eval_gt.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientnet/README.md\n",
      "TensorRT-10.3.0.26/samples/python/efficientnet/build_engine.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientnet/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/efficientnet/image_batcher.py\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/circ_pad_plugin_inetdef_cuda_python.py\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/circ_pad_plugin_torch.py\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/circ_pad_plugin_cupy.py\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/circ_pad_plugin_triton.py\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/README.md\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/circ_pad_plugin_cuda_python.py\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/circ_pad_plugin_multi_tactic.py\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/circ_pad_plugin_numba.py\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/circ_plugin_cpp/\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/circ_plugin_cpp/circ_pad_plugin.cu\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/circ_pad_plugin_cpp.py\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/circ_pad_example.png\n",
      "TensorRT-10.3.0.26/samples/python/python_plugin/CMakeLists.txt\n",
      "TensorRT-10.3.0.26/samples/python/simple_progress_monitor/\n",
      "TensorRT-10.3.0.26/samples/python/simple_progress_monitor/README.md\n",
      "TensorRT-10.3.0.26/samples/python/simple_progress_monitor/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/simple_progress_monitor/simple_progress_monitor.py\n",
      "TensorRT-10.3.0.26/samples/python/README.md\n",
      "TensorRT-10.3.0.26/samples/python/network_api_pytorch_mnist/\n",
      "TensorRT-10.3.0.26/samples/python/network_api_pytorch_mnist/README.md\n",
      "TensorRT-10.3.0.26/samples/python/network_api_pytorch_mnist/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/network_api_pytorch_mnist/model.py\n",
      "TensorRT-10.3.0.26/samples/python/network_api_pytorch_mnist/sample.py\n",
      "TensorRT-10.3.0.26/samples/python/yolov3_onnx/\n",
      "TensorRT-10.3.0.26/samples/python/yolov3_onnx/data_processing.py\n",
      "TensorRT-10.3.0.26/samples/python/yolov3_onnx/README.md\n",
      "TensorRT-10.3.0.26/samples/python/yolov3_onnx/onnx_to_tensorrt.py\n",
      "TensorRT-10.3.0.26/samples/python/yolov3_onnx/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/yolov3_onnx/download.yml\n",
      "TensorRT-10.3.0.26/samples/python/yolov3_onnx/coco_labels.txt\n",
      "TensorRT-10.3.0.26/samples/python/yolov3_onnx/yolov3_to_onnx.py\n",
      "TensorRT-10.3.0.26/samples/python/sample_weight_stripping/\n",
      "TensorRT-10.3.0.26/samples/python/sample_weight_stripping/README.md\n",
      "TensorRT-10.3.0.26/samples/python/sample_weight_stripping/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/sample_weight_stripping/build_engines.py\n",
      "TensorRT-10.3.0.26/samples/python/sample_weight_stripping/refit_engine_and_infer.py\n",
      "TensorRT-10.3.0.26/samples/python/common.py\n",
      "TensorRT-10.3.0.26/samples/python/onnx_custom_plugin/\n",
      "TensorRT-10.3.0.26/samples/python/onnx_custom_plugin/load_plugin_lib.py\n",
      "TensorRT-10.3.0.26/samples/python/onnx_custom_plugin/README.md\n",
      "TensorRT-10.3.0.26/samples/python/onnx_custom_plugin/plugin/\n",
      "TensorRT-10.3.0.26/samples/python/onnx_custom_plugin/plugin/customHardmaxPlugin.cpp\n",
      "TensorRT-10.3.0.26/samples/python/onnx_custom_plugin/plugin/customHardmaxPlugin.h\n",
      "TensorRT-10.3.0.26/samples/python/onnx_custom_plugin/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/onnx_custom_plugin/model.py\n",
      "TensorRT-10.3.0.26/samples/python/onnx_custom_plugin/test_custom_hardmax_plugin.py\n",
      "TensorRT-10.3.0.26/samples/python/onnx_custom_plugin/CMakeLists.txt\n",
      "TensorRT-10.3.0.26/samples/python/onnx_custom_plugin/sample.py\n",
      "TensorRT-10.3.0.26/samples/python/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/downloader.py\n",
      "TensorRT-10.3.0.26/samples/python/non_zero_plugin/\n",
      "TensorRT-10.3.0.26/samples/python/non_zero_plugin/README.md\n",
      "TensorRT-10.3.0.26/samples/python/non_zero_plugin/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/non_zero_plugin/non_zero_plugin.py\n",
      "TensorRT-10.3.0.26/samples/python/introductory_parser_samples/\n",
      "TensorRT-10.3.0.26/samples/python/introductory_parser_samples/onnx_resnet50.py\n",
      "TensorRT-10.3.0.26/samples/python/introductory_parser_samples/README.md\n",
      "TensorRT-10.3.0.26/samples/python/introductory_parser_samples/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/detectron2/\n",
      "TensorRT-10.3.0.26/samples/python/detectron2/create_onnx.py\n",
      "TensorRT-10.3.0.26/samples/python/detectron2/infer.py\n",
      "TensorRT-10.3.0.26/samples/python/detectron2/README.md\n",
      "TensorRT-10.3.0.26/samples/python/detectron2/build_engine.py\n",
      "TensorRT-10.3.0.26/samples/python/detectron2/eval_coco.py\n",
      "TensorRT-10.3.0.26/samples/python/detectron2/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/detectron2/visualize.py\n",
      "TensorRT-10.3.0.26/samples/python/detectron2/image_batcher.py\n",
      "TensorRT-10.3.0.26/samples/python/detectron2/onnx_utils.py\n",
      "TensorRT-10.3.0.26/samples/python/engine_refit_onnx_bidaf/\n",
      "TensorRT-10.3.0.26/samples/python/engine_refit_onnx_bidaf/data_processing.py\n",
      "TensorRT-10.3.0.26/samples/python/engine_refit_onnx_bidaf/README.md\n",
      "TensorRT-10.3.0.26/samples/python/engine_refit_onnx_bidaf/prepare_model.py\n",
      "TensorRT-10.3.0.26/samples/python/engine_refit_onnx_bidaf/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/engine_refit_onnx_bidaf/download.yml\n",
      "TensorRT-10.3.0.26/samples/python/engine_refit_onnx_bidaf/build_and_refit_engine.py\n",
      "TensorRT-10.3.0.26/samples/python/onnx_packnet/\n",
      "TensorRT-10.3.0.26/samples/python/onnx_packnet/README.md\n",
      "TensorRT-10.3.0.26/samples/python/onnx_packnet/post_processing.py\n",
      "TensorRT-10.3.0.26/samples/python/onnx_packnet/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/onnx_packnet/download.yml\n",
      "TensorRT-10.3.0.26/samples/python/onnx_packnet/convert_to_onnx.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/compare_tf.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/create_onnx.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/infer.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/labels_coco.txt\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/README.md\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/build_engine.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/eval_coco.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/requirements.txt\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/visualize.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/image_batcher.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/infer_tf.py\n",
      "TensorRT-10.3.0.26/samples/python/efficientdet/onnx_utils.py\n",
      "TensorRT-10.3.0.26/samples/python/common_runtime.py\n",
      "TensorRT-10.3.0.26/samples/sampleOnnxMnistCoordConvAC/\n",
      "TensorRT-10.3.0.26/samples/sampleOnnxMnistCoordConvAC/README.md\n",
      "TensorRT-10.3.0.26/samples/sampleOnnxMnistCoordConvAC/modify_onnx_ac.py\n",
      "TensorRT-10.3.0.26/samples/sampleOnnxMnistCoordConvAC/coord_conv.py\n",
      "TensorRT-10.3.0.26/samples/sampleOnnxMnistCoordConvAC/mnist_coord_conv_train.py\n",
      "TensorRT-10.3.0.26/samples/sampleOnnxMnistCoordConvAC/sampleOnnxMnistCoordConvAC.cpp\n",
      "TensorRT-10.3.0.26/samples/sampleOnnxMnistCoordConvAC/Makefile\n",
      "TensorRT-10.3.0.26/samples/sampleAlgorithmSelector/\n",
      "TensorRT-10.3.0.26/samples/sampleAlgorithmSelector/README.md\n",
      "TensorRT-10.3.0.26/samples/sampleAlgorithmSelector/sampleAlgorithmSelector.cpp\n",
      "TensorRT-10.3.0.26/samples/sampleAlgorithmSelector/Makefile\n",
      "TensorRT-10.3.0.26/samples/trtexec/\n",
      "TensorRT-10.3.0.26/samples/trtexec/profiler.py\n",
      "TensorRT-10.3.0.26/samples/trtexec/README.md\n",
      "TensorRT-10.3.0.26/samples/trtexec/prn_utils.py\n",
      "TensorRT-10.3.0.26/samples/trtexec/tracer.py\n",
      "TensorRT-10.3.0.26/samples/trtexec/trtexec.cpp\n",
      "TensorRT-10.3.0.26/samples/trtexec/Makefile\n",
      "TensorRT-10.3.0.26/samples/sampleOnnxMNIST/\n",
      "TensorRT-10.3.0.26/samples/sampleOnnxMNIST/README.md\n",
      "TensorRT-10.3.0.26/samples/sampleOnnxMNIST/sampleOnnxMNIST.cpp\n",
      "TensorRT-10.3.0.26/samples/sampleOnnxMNIST/Makefile\n",
      "TensorRT-10.3.0.26/samples/utils/\n",
      "TensorRT-10.3.0.26/samples/utils/timingCache.h\n",
      "TensorRT-10.3.0.26/samples/utils/fileLock.cpp\n",
      "TensorRT-10.3.0.26/samples/utils/timingCache.cpp\n",
      "TensorRT-10.3.0.26/samples/utils/fileLock.h\n",
      "TensorRT-10.3.0.26/samples/sampleIOFormats/\n",
      "TensorRT-10.3.0.26/samples/sampleIOFormats/sampleIOFormats.cpp\n",
      "TensorRT-10.3.0.26/samples/sampleIOFormats/README.md\n",
      "TensorRT-10.3.0.26/samples/sampleIOFormats/Makefile\n",
      "TensorRT-10.3.0.26/samples/sampleProgressMonitor/\n",
      "TensorRT-10.3.0.26/samples/sampleProgressMonitor/sampleProgressMonitor.cpp\n",
      "TensorRT-10.3.0.26/samples/sampleProgressMonitor/README.md\n",
      "TensorRT-10.3.0.26/samples/sampleProgressMonitor/Makefile\n",
      "TensorRT-10.3.0.26/samples/sampleCharRNN/\n",
      "TensorRT-10.3.0.26/samples/sampleCharRNN/README.md\n",
      "TensorRT-10.3.0.26/samples/sampleCharRNN/sampleCharRNN.cpp\n",
      "TensorRT-10.3.0.26/samples/sampleCharRNN/Makefile\n",
      "TensorRT-10.3.0.26/samples/Makefile.config\n",
      "TensorRT-10.3.0.26/samples/sampleNonZeroPlugin/\n",
      "TensorRT-10.3.0.26/samples/sampleNonZeroPlugin/nonZeroKernel.h\n",
      "TensorRT-10.3.0.26/samples/sampleNonZeroPlugin/sampleNonZeroPlugin.cpp\n",
      "TensorRT-10.3.0.26/samples/sampleNonZeroPlugin/README.md\n",
      "TensorRT-10.3.0.26/samples/sampleNonZeroPlugin/nonZeroKernel.cu\n",
      "TensorRT-10.3.0.26/samples/sampleNonZeroPlugin/Makefile\n",
      "TensorRT-10.3.0.26/samples/Makefile\n",
      "TensorRT-10.3.0.26/python/\n",
      "TensorRT-10.3.0.26/python/tensorrt-10.3.0-cp38-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt-10.3.0-cp311-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt_lean-10.3.0-cp38-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt-10.3.0-cp312-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt_dispatch-10.3.0-cp312-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt_lean-10.3.0-cp39-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt_dispatch-10.3.0-cp311-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt_lean-10.3.0-cp310-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt-10.3.0-cp39-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt-10.3.0-cp310-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt_dispatch-10.3.0-cp310-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt_lean-10.3.0-cp311-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt_dispatch-10.3.0-cp39-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt_dispatch-10.3.0-cp38-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/python/tensorrt_lean-10.3.0-cp312-none-linux_x86_64.whl\n",
      "TensorRT-10.3.0.26/bin\n",
      "TensorRT-10.3.0.26/lib\n",
      "TensorRT-10.3.0.26/targets/\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/include\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/samples\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/bin/\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/bin/trtexec\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_vc_plugin.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer.so.10\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_dispatch.so.10\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_vc_plugin.so.10.3.0\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_plugin_static.a\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_builder_resource.so.10.3.0\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libonnx_proto.a\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_static.a\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvonnxparser.so.10\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvonnxparser.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvonnxparser.so.10.3.0\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_vc_plugin.so.10\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer.so.10.3.0\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_plugin.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_lean.so.10.3.0\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_dispatch_static.a\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_dispatch.so.10.3.0\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/stubs/\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/stubs/libnvinfer_vc_plugin.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/stubs/libnvonnxparser.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/stubs/libnvinfer_plugin.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/stubs/libnvinfer.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/stubs/libnvinfer_dispatch.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/stubs/libnvinfer_lean.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_lean.so.10\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvonnxparser_static.a\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_plugin.so.10.3.0\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_vc_plugin_static.a\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_builder_resource_win.so.10.3.0\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_plugin.so.10\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_dispatch.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_lean.so\n",
      "TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_lean_static.a\n"
     ]
    }
   ],
   "source": [
    "!wget https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz\n",
    "!tar -xvzf TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317ce8fd-564c-476c-9f9b-ca424a00687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!TENSORRT_PATH=$(pwd)/TensorRT-10.3.0.26\n",
    "!echo \"export TENSORRT_HOME=${TENSORRT_PATH}\" >> ~/.bashrc\n",
    "!echo \"export PATH=\\$TENSORRT_HOME/bin:\\$PATH\" >> ~/.bashrc\n",
    "!echo \"export LD_LIBRARY_PATH=\\$TENSORRT_HOME/lib:\\$LD_LIBRARY_PATH\" >> ~/.bashrc\n",
    "!ln -sf /workspace/TensorRT-10.3.0.26/bin/trtexec /usr/local/bin/trtexec\n",
    "!source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b1429e0-052a-4440-81e0-38a80826e3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/TensorRT-10.3.0.26/targets/x86_64-linux-gnu/lib/libnvinfer_plugin.so.10\n",
      "/venv/main/lib/python3.10/site-packages/tensorrt_libs/libnvinfer_plugin.so.10\n"
     ]
    }
   ],
   "source": [
    "!find / -name libnvinfer_plugin.so.10 2>/dev/null\n",
    "!export LD_LIBRARY_PATH=/venv/main/lib/python3.10/site-packages/tensorrt_libs/lib:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cccba9e3-a4c8-40c5-8b2f-872062517c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033fb6ad-509a-4b11-98f8-9e820057c36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease               \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease   \n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Fetched 128 kB in 1s (204 kB/s)\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e45816ef-d6ac-4906-826c-6f7b4bc7a335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libnvinfer-bin libnvinfer-dev libnvinfer-dispatch-dev libnvinfer-dispatch10\n",
      "  libnvinfer-headers-dev libnvinfer-headers-plugin-dev libnvinfer-lean-dev\n",
      "  libnvinfer-lean10 libnvinfer-plugin-dev libnvinfer-plugin10\n",
      "  libnvinfer-samples libnvinfer-vc-plugin-dev libnvinfer-vc-plugin10\n",
      "  libnvinfer10 libnvonnxparsers-dev libnvonnxparsers10 python3-libnvinfer\n",
      "  python3-libnvinfer-dev python3-libnvinfer-dispatch python3-libnvinfer-lean\n",
      "The following NEW packages will be installed:\n",
      "  libnvinfer-bin libnvinfer-dev libnvinfer-dispatch-dev libnvinfer-dispatch10\n",
      "  libnvinfer-headers-dev libnvinfer-headers-plugin-dev libnvinfer-lean-dev\n",
      "  libnvinfer-lean10 libnvinfer-plugin-dev libnvinfer-plugin10\n",
      "  libnvinfer-samples libnvinfer-vc-plugin-dev libnvinfer-vc-plugin10\n",
      "  libnvinfer10 libnvonnxparsers-dev libnvonnxparsers10 python3-libnvinfer\n",
      "  python3-libnvinfer-dev python3-libnvinfer-dispatch python3-libnvinfer-lean\n",
      "  tensorrt\n",
      "0 upgraded, 21 newly installed, 0 to remove and 51 not upgraded.\n",
      "Need to get 4459 MB of archives.\n",
      "After this operation, 11.4 GB of additional disk space will be used.\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer10 10.9.0.34-1+cuda12.8 [1968 MB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-lean10 10.9.0.34-1+cuda12.8 [15.6 MB]\n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-plugin10 10.9.0.34-1+cuda12.8 [13.7 MB]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-vc-plugin10 10.9.0.34-1+cuda12.8 [234 kB]\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dispatch10 10.9.0.34-1+cuda12.8 [216 kB]\n",
      "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvonnxparsers10 10.9.0.34-1+cuda12.8 [1387 kB]\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-bin 10.9.0.34-1+cuda12.8 [457 kB]\n",
      "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-headers-dev 10.9.0.34-1+cuda12.8 [109 kB]\n",
      "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dev 10.9.0.34-1+cuda12.8 [2020 MB]\n",
      "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dispatch-dev 10.9.0.34-1+cuda12.8 [123 kB]\n",
      "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-headers-plugin-dev 10.9.0.34-1+cuda12.8 [6056 B]\n",
      "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-lean-dev 10.9.0.34-1+cuda12.8 [233 MB]\n",
      "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-plugin-dev 10.9.0.34-1+cuda12.8 [15.0 MB]\n",
      "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-vc-plugin-dev 10.9.0.34-1+cuda12.8 [97.4 kB]\n",
      "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvonnxparsers-dev 10.9.0.34-1+cuda12.8 [2117 kB]\n",
      "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-samples 10.9.0.34-1+cuda12.8 [187 MB]\n",
      "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer 10.9.0.34-1+cuda12.8 [785 kB]\n",
      "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer-lean 10.9.0.34-1+cuda12.8 [508 kB]\n",
      "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer-dispatch 10.9.0.34-1+cuda12.8 [508 kB]\n",
      "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer-dev 10.9.0.34-1+cuda12.8 [2962 B]\n",
      "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  tensorrt 10.9.0.34-1+cuda12.8 [2946 B]\n",
      "Fetched 4459 MB in 28s (162 MB/s)                                              \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libnvinfer10.\n",
      "(Reading database ... 40721 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libnvinfer10_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer10 (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-lean10.\n",
      "Preparing to unpack .../01-libnvinfer-lean10_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-lean10 (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-plugin10.\n",
      "Preparing to unpack .../02-libnvinfer-plugin10_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-plugin10 (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-vc-plugin10.\n",
      "Preparing to unpack .../03-libnvinfer-vc-plugin10_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-vc-plugin10 (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-dispatch10.\n",
      "Preparing to unpack .../04-libnvinfer-dispatch10_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-dispatch10 (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvonnxparsers10.\n",
      "Preparing to unpack .../05-libnvonnxparsers10_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvonnxparsers10 (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-bin.\n",
      "Preparing to unpack .../06-libnvinfer-bin_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-bin (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-headers-dev.\n",
      "Preparing to unpack .../07-libnvinfer-headers-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-headers-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-dev.\n",
      "Preparing to unpack .../08-libnvinfer-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-dispatch-dev.\n",
      "Preparing to unpack .../09-libnvinfer-dispatch-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-dispatch-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-headers-plugin-dev.\n",
      "Preparing to unpack .../10-libnvinfer-headers-plugin-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-headers-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-lean-dev.\n",
      "Preparing to unpack .../11-libnvinfer-lean-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-lean-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-plugin-dev.\n",
      "Preparing to unpack .../12-libnvinfer-plugin-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-vc-plugin-dev.\n",
      "Preparing to unpack .../13-libnvinfer-vc-plugin-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-vc-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvonnxparsers-dev.\n",
      "Preparing to unpack .../14-libnvonnxparsers-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvonnxparsers-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-samples.\n",
      "Preparing to unpack .../15-libnvinfer-samples_10.9.0.34-1+cuda12.8_all.deb ...\n",
      "Unpacking libnvinfer-samples (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer.\n",
      "Preparing to unpack .../16-python3-libnvinfer_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer-lean.\n",
      "Preparing to unpack .../17-python3-libnvinfer-lean_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer-lean (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer-dispatch.\n",
      "Preparing to unpack .../18-python3-libnvinfer-dispatch_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer-dispatch (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer-dev.\n",
      "Preparing to unpack .../19-python3-libnvinfer-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package tensorrt.\n",
      "Preparing to unpack .../20-tensorrt_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking tensorrt (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-headers-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-plugin10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-vc-plugin10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvonnxparsers10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-dispatch10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-dispatch-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-lean10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvonnxparsers-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer-dispatch (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-headers-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-lean-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer-lean (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-vc-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-bin (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-samples (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up tensorrt (10.9.0.34-1+cuda12.8) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get -y install tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01b4390-aca9-4bb1-aa90-ebd6e1db660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==1.0.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting accelerate==0.34.2\n",
      "  Using cached accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "Requirement already satisfied: annotated-types==0.7.0 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.7.0)\n",
      "Collecting cachetools==5.0.0\n",
      "  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting certifi==2021.10.8\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting charset-normalizer==2.0.12\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting chart-studio==1.1.0\n",
      "  Using cached chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
      "Collecting cloudpickle==3.0.0\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: coloredlogs==15.0.1 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (15.0.1)\n",
      "Collecting contextlib2==21.6.0\n",
      "  Using cached contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: cppimport==22.8.2 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (22.8.2)\n",
      "Collecting cycler==0.11.0\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting dicom2nifti==2.3.4\n",
      "  Using cached dicom2nifti-2.3.4.tar.gz (35 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting diffusers==0.30.3\n",
      "  Using cached diffusers-0.30.3-py3-none-any.whl (2.7 MB)\n",
      "Collecting einops==0.4.1\n",
      "  Using cached einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Collecting filelock==3.16.1\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting flatbuffers==24.3.25\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting fonttools==4.31.1\n",
      "  Using cached fonttools-4.31.1-py3-none-any.whl (899 kB)\n",
      "Collecting fsspec==2024.9.0\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Collecting future==0.18.2\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "Collecting google-auth==2.6.0\n",
      "  Using cached google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "Collecting google-auth-oauthlib==0.4.6\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting grpcio==1.44.0\n",
      "  Using cached grpcio-1.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Collecting huggingface-hub==0.25.0\n",
      "  Using cached huggingface_hub-0.25.0-py3-none-any.whl (436 kB)\n",
      "Requirement already satisfied: humanfriendly==10.0 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (10.0)\n",
      "Collecting idna==3.3\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting imageio==2.16.1\n",
      "  Using cached imageio-2.16.1-py3-none-any.whl (3.3 MB)\n",
      "Collecting importlib-metadata==4.11.2\n",
      "  Using cached importlib_metadata-4.11.2-py3-none-any.whl (17 kB)\n",
      "Collecting Jinja2==3.1.4\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting joblib==1.1.0\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting kiwisolver==1.4.0\n",
      "  Using cached kiwisolver-1.4.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Collecting linecache2==1.0.0\n",
      "  Using cached linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting Markdown==3.3.6\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: markdown-it-py==3.0.0 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 34)) (3.0.0)\n",
      "Collecting matplotlib==3.5.1\n",
      "  Using cached matplotlib-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
      "Requirement already satisfied: mdurl==0.1.2 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (0.1.2)\n",
      "Collecting MedPy==0.4.0\n",
      "  Using cached MedPy-0.4.0.tar.gz (151 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ml_collections==0.1.1\n",
      "  Using cached ml_collections-0.1.1.tar.gz (77 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "Collecting monai==1.3.2\n",
      "  Using cached monai-1.3.2-py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (1.3.0)\n",
      "Collecting netron==7.8.9\n",
      "  Using cached netron-7.8.9-py3-none-any.whl (1.7 MB)\n",
      "Collecting networkx==2.6.3\n",
      "  Using cached networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "Collecting nibabel==3.2.2\n",
      "  Using cached nibabel-3.2.2-py3-none-any.whl (3.3 MB)\n",
      "Collecting ninja==1.11.1.1\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "Collecting numpy==1.22.4\n",
      "  Using cached numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 50)) (9.1.0.70)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-modelopt==0.17.0\n",
      "  Using cached nvidia_modelopt-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.68\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting oauthlib==3.2.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting onnx==1.16.1\n",
      "  Using cached onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "Collecting onnx-graphsurgeon==0.5.2\n",
      "  Using cached onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: onnxconverter-common==1.14.0 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 62)) (1.14.0)\n",
      "Collecting onnxmltools==1.12.0\n",
      "  Using cached onnxmltools-1.12.0-py2.py3-none-any.whl (329 kB)\n",
      "Collecting onnxruntime-gpu==1.18.0\n",
      "  Using cached onnxruntime_gpu-1.18.0-cp310-cp310-manylinux_2_28_x86_64.whl (199.8 MB)\n",
      "Collecting onnxruntime_extensions==0.12.0\n",
      "  Using cached onnxruntime_extensions-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "Collecting opencv-python==4.5.5.64\n",
      "  Using cached opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
      "Collecting packaging==21.3\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting pandas==1.3.5\n",
      "  Using cached pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "Collecting patsy==0.5.2\n",
      "  Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB)\n",
      "Collecting Pillow==9.0.1\n",
      "  Using cached Pillow-9.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Collecting plotly==5.6.0\n",
      "  Using cached plotly-5.6.0-py2.py3-none-any.whl (27.7 MB)\n",
      "Collecting polygraphy==0.49.9\n",
      "  Using cached polygraphy-0.49.9-py2.py3-none-any.whl (346 kB)\n",
      "Requirement already satisfied: protobuf==3.20.2 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 73)) (3.20.2)\n",
      "Collecting psutil==6.0.0\n",
      "  Using cached psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\n",
      "Collecting ptflops==0.6.8\n",
      "  Using cached ptflops-0.6.8.tar.gz (12 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting PuLP==2.9.0\n",
      "  Using cached PuLP-2.9.0-py3-none-any.whl (17.7 MB)\n",
      "Collecting pyasn1==0.4.8\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting pyasn1-modules==0.2.8\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: pybind11==2.13.6 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 79)) (2.13.6)\n",
      "Collecting pydantic==2.9.2\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Collecting pydantic_core==2.23.4\n",
      "  Using cached pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Collecting pydicom==2.3.0\n",
      "  Using cached pydicom-2.3.0-py3-none-any.whl (2.0 MB)\n",
      "Collecting Pygments==2.18.0\n",
      "  Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "Collecting pynvml==11.5.3\n",
      "  Using cached pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
      "Collecting pyparsing==3.0.7\n",
      "  Using cached pyparsing-3.0.7-py3-none-any.whl (98 kB)\n",
      "Collecting python-dateutil==2.8.2\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting pytz==2021.3\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Collecting PyWavelets==1.3.0\n",
      "  Using cached PyWavelets-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "Collecting PyYAML==6.0\n",
      "  Using cached PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "Collecting regex==2024.9.11\n",
      "  Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "Collecting requests==2.27.1\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting requests-oauthlib==1.3.1\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting retrying==1.3.3\n",
      "  Using cached retrying-1.3.3.tar.gz (10 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting rich==13.8.1\n",
      "  Using cached rich-13.8.1-py3-none-any.whl (241 kB)\n",
      "Collecting rsa==4.8\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting safetensors==0.4.5\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Collecting scikit-image==0.19.2\n",
      "  Using cached scikit_image-0.19.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "Collecting scikit-learn==1.0.2\n",
      "  Using cached scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "Collecting scipy==1.10.1\n",
      "  Using cached scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "Collecting seaborn==0.11.2\n",
      "  Using cached seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Collecting SimpleITK\n",
      "  Downloading SimpleITK-2.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting six==1.16.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting sklearn==0.0\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting statsmodels==0.13.2\n",
      "  Downloading statsmodels-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting sympy==1.13.3\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m306.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tenacity==8.0.1\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting tensorboard==2.11.2\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m215.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server==0.6.1\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m132.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit==1.8.1\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 KB\u001b[0m \u001b[31m214.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboardX==2.5\n",
      "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 KB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thop==0.1.1.post2209072238\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Collecting threadpoolctl==3.1.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting tifffile==2021.11.2\n",
      "  Downloading tifffile-2021.11.2-py3-none-any.whl (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting timm==0.5.4\n",
      "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 KB\u001b[0m \u001b[31m205.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting tokenizers==0.19.1\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m261.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 116)) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchprofile==0.0.4 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 117)) (0.0.4)\n",
      "Collecting torchvision==0.19.1\n",
      "  Downloading torchvision-0.19.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting tqdm==4.63.0\n",
      "  Downloading tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 KB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting traceback2==1.4.0\n",
      "  Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting transformers==4.44.2\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m233.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.0.0\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting unittest2==1.1.0\n",
      "  Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 KB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting urllib3==1.26.8\n",
      "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 KB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting webcolors==1.11.1\n",
      "  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
      "Collecting Werkzeug==2.0.3\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.2/289.2 KB\u001b[0m \u001b[31m163.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: mako in /venv/main/lib/python3.10/site-packages (from cppimport==22.8.2->-r requirements.txt (line 11)) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub==0.25.0->-r requirements.txt (line 24)) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /venv/main/lib/python3.10/site-packages (from importlib-metadata==4.11.2->-r requirements.txt (line 28)) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from Jinja2==3.1.4->-r requirements.txt (line 29)) (3.0.2)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.10/site-packages (from nibabel==3.2.2->-r requirements.txt (line 43)) (59.6.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /venv/main/lib/python3.10/site-packages (from tensorboard==2.11.2->-r requirements.txt (line 107)) (0.45.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting argparse\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Building wheels for collected packages: dicom2nifti, future, MedPy, ml_collections, ptflops, retrying, sklearn\n",
      "doneng wheel for dicom2nifti (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for dicom2nifti: filename=dicom2nifti-2.3.4-py3-none-any.whl size=44527 sha256=811b478302d474c2e03bf247647f86c189838d4f0565d8296dd877a6715974d8\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/63/8d/9e2076d2dae7e027c9c916863b471c28b7d81cec4bb9dab982\n",
      "done wheel for future (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=7704518967f1ff74b3117fcbc571cf552e0eed13bd71aabe75f57ec53d3c8057\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/73/06/557dc4f4ef68179b9d763930d6eec26b88ed7c389b19588a1c\n",
      "doneel for MedPy (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for MedPy: filename=MedPy-0.4.0-py3-none-any.whl size=214965 sha256=2322b091272a2d3b6c71063c8066412d70adf754e129e38e9e5f583fe038e9a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/d4/32/c7/6380ab2edb8cca018d39a0f1d43250fd9791922c963117de46\n",
      "doneng wheel for ml_collections (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for ml_collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94525 sha256=c549c82d6359159105a51e2a5601a849996d4a9c456cb94060b87a77e384fdc1\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
      "doneng wheel for ptflops (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for ptflops: filename=ptflops-0.6.8-py3-none-any.whl size=11869 sha256=7e02e014c906bf4263a0ff721cdea5d9a6076c5ffa7595fb11bee586ea40dab8\n",
      "  Stored in directory: /root/.cache/pip/wheels/65/78/dd/ee26db7b47fe0095e58d1020e16c93dc3ee6553e8547cc736c\n",
      "doneng wheel for retrying (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11446 sha256=0e19e32c22a3a3adb3f6066a9a77d449700d9b1f5b9eb0a8e1d49b833e6053ad\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/0a/27/62f77273aa12fefb20b9b277f1e937f98ae16dbd04ffa2b93a\n",
      "doneng wheel for sklearn (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=64084ebe63fe8f21c0a0329fe88f4ebd6bf4ed3e4b7834bf677f1a5d34ae8125\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/13/01/6f3a7fd641f90e1f6c8c7cded057f3394f451f340371c68f3d\n",
      "Successfully built dicom2nifti future MedPy ml_collections ptflops retrying sklearn\n",
      "Installing collected packages: tensorboard-plugin-wit, SimpleITK, pytz, pyasn1, onnxruntime_extensions, ninja, netron, linecache2, flatbuffers, einops, certifi, argparse, Werkzeug, webcolors, urllib3, traceback2, tqdm, threadpoolctl, tensorboard-data-server, tenacity, sympy, six, safetensors, rsa, regex, PyYAML, pyparsing, pynvml, Pygments, pydicom, pydantic_core, pyasn1-modules, PuLP, psutil, polygraphy, Pillow, oauthlib, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, Markdown, kiwisolver, joblib, Jinja2, importlib-metadata, idna, future, fsspec, fonttools, filelock, cycler, contextlib2, cloudpickle, charset-normalizer, cachetools, unittest2, triton, tifffile, tensorboardX, scipy, rich, retrying, requests, PyWavelets, python-dateutil, pydantic, plotly, patsy, packaging, opencv-python, onnx, nvidia-cusparse-cu12, imageio, grpcio, google-auth, absl-py, scikit-learn, scikit-image, requests-oauthlib, pandas, onnxruntime-gpu, onnxmltools, onnx-graphsurgeon, nvidia-modelopt, nvidia-cusolver-cu12, nibabel, ml_collections, MedPy, matplotlib, huggingface-hub, chart-studio, torch, tokenizers, statsmodels, sklearn, seaborn, google-auth-oauthlib, diffusers, dicom2nifti, transformers, torchvision, thop, tensorboard, ptflops, monai, accelerate, timm\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.1\n",
      "    Uninstalling pytz-2025.1:\n",
      "      Successfully uninstalled pytz-2025.1\n",
      "  Attempting uninstall: ninja\n",
      "    Found existing installation: ninja 1.11.1.3\n",
      "    Uninstalling ninja-1.11.1.3:\n",
      "      Successfully uninstalled ninja-1.11.1.3\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 25.2.10\n",
      "    Uninstalling flatbuffers-25.2.10:\n",
      "      Successfully uninstalled flatbuffers-25.2.10\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.1.31\n",
      "    Uninstalling certifi-2025.1.31:\n",
      "      Successfully uninstalled certifi-2025.1.31\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.5.3\n",
      "    Uninstalling safetensors-0.5.3:\n",
      "      Successfully uninstalled safetensors-0.5.3\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.11.6\n",
      "    Uninstalling regex-2024.11.6:\n",
      "      Successfully uninstalled regex-2024.11.6\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: pynvml\n",
      "    Found existing installation: pynvml 12.0.0\n",
      "    Uninstalling pynvml-12.0.0:\n",
      "      Successfully uninstalled pynvml-12.0.0\n",
      "  Attempting uninstall: Pygments\n",
      "    Found existing installation: Pygments 2.19.1\n",
      "    Uninstalling Pygments-2.19.1:\n",
      "      Successfully uninstalled Pygments-2.19.1\n",
      "  Attempting uninstall: pydantic_core\n",
      "    Found existing installation: pydantic_core 2.27.2\n",
      "    Uninstalling pydantic_core-2.27.2:\n",
      "      Successfully uninstalled pydantic_core-2.27.2\n",
      "  Attempting uninstall: PuLP\n",
      "    Found existing installation: PuLP 3.0.2\n",
      "    Uninstalling PuLP-3.0.2:\n",
      "      Successfully uninstalled PuLP-3.0.2\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 6.1.1\n",
      "    Uninstalling psutil-6.1.1:\n",
      "      Successfully uninstalled psutil-6.1.1\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 11.1.0\n",
      "    Uninstalling pillow-11.1.0:\n",
      "      Successfully uninstalled pillow-11.1.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.4.2\n",
      "    Uninstalling networkx-3.4.2:\n",
      "      Successfully uninstalled networkx-3.4.2\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 3.1.6\n",
      "    Uninstalling Jinja2-3.1.6:\n",
      "      Successfully uninstalled Jinja2-3.1.6\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.6.1\n",
      "    Uninstalling importlib_metadata-8.6.1:\n",
      "      Successfully uninstalled importlib_metadata-8.6.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.17.0\n",
      "    Uninstalling filelock-3.17.0:\n",
      "      Successfully uninstalled filelock-3.17.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 3.1.1\n",
      "    Uninstalling cloudpickle-3.1.1:\n",
      "      Successfully uninstalled cloudpickle-3.1.1\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.1\n",
      "    Uninstalling charset-normalizer-3.4.1:\n",
      "      Successfully uninstalled charset-normalizer-3.4.1\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.9.4\n",
      "    Uninstalling rich-13.9.4:\n",
      "      Successfully uninstalled rich-13.9.4\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.6\n",
      "    Uninstalling pydantic-2.10.6:\n",
      "      Successfully uninstalled pydantic-2.10.6\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: onnx\n",
      "    Found existing installation: onnx 1.17.0\n",
      "    Uninstalling onnx-1.17.0:\n",
      "      Successfully uninstalled onnx-1.17.0\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: onnxruntime-gpu\n",
      "    Found existing installation: onnxruntime-gpu 1.20.2\n",
      "    Uninstalling onnxruntime-gpu-1.20.2:\n",
      "      Successfully uninstalled onnxruntime-gpu-1.20.2\n",
      "  Attempting uninstall: onnxmltools\n",
      "    Found existing installation: onnxmltools 1.13.0\n",
      "    Uninstalling onnxmltools-1.13.0:\n",
      "      Successfully uninstalled onnxmltools-1.13.0\n",
      "  Attempting uninstall: onnx-graphsurgeon\n",
      "    Found existing installation: onnx_graphsurgeon 0.5.6\n",
      "    Uninstalling onnx_graphsurgeon-0.5.6:\n",
      "      Successfully uninstalled onnx_graphsurgeon-0.5.6\n",
      "  Attempting uninstall: nvidia-modelopt\n",
      "    Found existing installation: nvidia-modelopt 0.25.0\n",
      "    Uninstalling nvidia-modelopt-0.25.0:\n",
      "      Successfully uninstalled nvidia-modelopt-0.25.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.28.1\n",
      "    Uninstalling huggingface-hub-0.28.1:\n",
      "      Successfully uninstalled huggingface-hub-0.28.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.32.2\n",
      "    Uninstalling diffusers-0.32.2:\n",
      "      Successfully uninstalled diffusers-0.32.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.49.0\n",
      "    Uninstalling transformers-4.49.0:\n",
      "      Successfully uninstalled transformers-4.49.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0\n",
      "    Uninstalling torchvision-0.21.0:\n",
      "      Successfully uninstalled torchvision-0.21.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.4.0\n",
      "    Uninstalling accelerate-1.4.0:\n",
      "      Successfully uninstalled accelerate-1.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch-tensorrt 2.6.0+cu124 requires packaging>=23, but you have packaging 21.3 which is incompatible.\n",
      "torch-tensorrt 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\n",
      "datasets 3.3.2 requires requests>=2.32.2, but you have requests 2.27.1 which is incompatible.\n",
      "datasets 3.3.2 requires tqdm>=4.66.3, but you have tqdm 4.63.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Jinja2-3.1.4 Markdown-3.3.6 MedPy-0.4.0 Pillow-9.0.1 PuLP-2.9.0 PyWavelets-1.3.0 PyYAML-6.0 Pygments-2.18.0 SimpleITK-2.4.1 Werkzeug-2.0.3 absl-py-1.0.0 accelerate-0.34.2 argparse-1.4.0 cachetools-5.0.0 certifi-2021.10.8 charset-normalizer-2.0.12 chart-studio-1.1.0 cloudpickle-3.0.0 contextlib2-21.6.0 cycler-0.11.0 dicom2nifti-2.3.4 diffusers-0.30.3 einops-0.4.1 filelock-3.16.1 flatbuffers-24.3.25 fonttools-4.31.1 fsspec-2024.9.0 future-0.18.2 google-auth-2.6.0 google-auth-oauthlib-0.4.6 grpcio-1.44.0 huggingface-hub-0.25.0 idna-3.3 imageio-2.16.1 importlib-metadata-4.11.2 joblib-1.1.0 kiwisolver-1.4.0 linecache2-1.0.0 matplotlib-3.5.1 ml_collections-0.1.1 monai-1.3.2 netron-7.8.9 networkx-2.6.3 nibabel-3.2.2 ninja-1.11.1.1 numpy-1.22.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-modelopt-0.17.0 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 oauthlib-3.2.0 onnx-1.16.1 onnx-graphsurgeon-0.5.2 onnxmltools-1.12.0 onnxruntime-gpu-1.18.0 onnxruntime_extensions-0.12.0 opencv-python-4.5.5.64 packaging-21.3 pandas-1.3.5 patsy-0.5.2 plotly-5.6.0 polygraphy-0.49.9 psutil-6.0.0 ptflops-0.6.8 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydantic-2.9.2 pydantic_core-2.23.4 pydicom-2.3.0 pynvml-11.5.3 pyparsing-3.0.7 python-dateutil-2.8.2 pytz-2021.3 regex-2024.9.11 requests-2.27.1 requests-oauthlib-1.3.1 retrying-1.3.3 rich-13.8.1 rsa-4.8 safetensors-0.4.5 scikit-image-0.19.2 scikit-learn-1.0.2 scipy-1.10.1 seaborn-0.11.2 six-1.16.0 sklearn-0.0 statsmodels-0.13.2 sympy-1.13.3 tenacity-8.0.1 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorboardX-2.5 thop-0.1.1.post2209072238 threadpoolctl-3.1.0 tifffile-2021.11.2 timm-0.5.4 tokenizers-0.19.1 torch-2.4.1 torchvision-0.19.1 tqdm-4.63.0 traceback2-1.4.0 transformers-4.44.2 triton-3.0.0 unittest2-1.1.0 urllib3-1.26.8 webcolors-1.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233167c0-a91a-4329-bf58-1051e6a13381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///workspace/nnUNet\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "done5h  Checking if build backend supports build_editable ... \u001b[?25l\n",
      "doneGetting requirements to build editable ... \u001b[?25l\n",
      "donePreparing editable metadata (pyproject.toml) ... \u001b[?25l\n",
      "Requirement already satisfied: seaborn in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (0.11.2)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: einops in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (0.4.1)\n",
      "Requirement already satisfied: tifffile in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (2021.11.2)\n",
      "Collecting numpy>=1.24\n",
      "  Using cached numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "Collecting imagecodecs\n",
      "  Downloading imagecodecs-2024.12.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: torch>=2.1.2 in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (2.4.1)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (1.3.5)\n",
      "Requirement already satisfied: nibabel in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (3.2.2)\n",
      "Collecting batchgenerators>=0.25\n",
      "  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 KB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "donePreparing metadata (setup.py) ... \u001b[?25l\n",
      "\u001b[?25hRequirement already satisfied: scipy in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (4.63.0)\n",
      "Collecting acvl-utils<0.3,>=0.2\n",
      "  Downloading acvl_utils-0.2.5.tar.gz (29 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (3.5.1)\n",
      "Requirement already satisfied: dicom2nifti in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (2.3.4)\n",
      "Collecting scikit-image>=0.19.3\n",
      "  Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m224.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: SimpleITK>=2.2.1 in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (2.4.1)\n",
      "Collecting dynamic-network-architectures<0.4,>=0.3.1\n",
      "  Downloading dynamic_network_architectures-0.3.1.tar.gz (20 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "\u001b[?25hCollecting batchgeneratorsv2>=0.2\n",
      "  Downloading batchgeneratorsv2-0.2.3.tar.gz (35 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "doneGetting requirements to build wheel ... \u001b[?25l\n",
      "donePreparing metadata (pyproject.toml) ... \u001b[?25l\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (1.0.2)\n",
      "Collecting yacs\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from nnunetv2==2.5.2) (2.27.1)\n",
      "Collecting blosc2>=3.0.0b4\n",
      "  Downloading blosc2-3.0.0b4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting connected-components-3d\n",
      "  Downloading connected_components_3d-3.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m229.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: future in /venv/main/lib/python3.10/site-packages (from batchgenerators>=0.25->nnunetv2==2.5.2) (0.18.2)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /venv/main/lib/python3.10/site-packages (from batchgenerators>=0.25->nnunetv2==2.5.2) (9.0.1)\n",
      "Requirement already satisfied: threadpoolctl in /venv/main/lib/python3.10/site-packages (from batchgenerators>=0.25->nnunetv2==2.5.2) (3.1.0)\n",
      "Requirement already satisfied: unittest2 in /venv/main/lib/python3.10/site-packages (from batchgenerators>=0.25->nnunetv2==2.5.2) (1.1.0)\n",
      "Collecting fft-conv-pytorch\n",
      "  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting tifffile\n",
      "  Downloading tifffile-2025.2.18-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.4/226.4 KB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lazy-loader>=0.4\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging>=21 in /venv/main/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2==2.5.2) (21.3)\n",
      "Collecting imageio!=2.35.0,>=2.33\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 KB\u001b[0m \u001b[31m155.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pillow>=7.1.2\n",
      "  Using cached pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "Collecting networkx>=3.0\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (3.16.1)\n",
      "Requirement already satisfied: triton==3.0.0 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2==2.5.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /venv/main/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.2->nnunetv2==2.5.2) (12.6.68)\n",
      "Requirement already satisfied: pydicom>=1.3.0 in /venv/main/lib/python3.10/site-packages (from dicom2nifti->nnunetv2==2.5.2) (2.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /venv/main/lib/python3.10/site-packages (from matplotlib->nnunetv2==2.5.2) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /venv/main/lib/python3.10/site-packages (from matplotlib->nnunetv2==2.5.2) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /venv/main/lib/python3.10/site-packages (from matplotlib->nnunetv2==2.5.2) (3.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /venv/main/lib/python3.10/site-packages (from matplotlib->nnunetv2==2.5.2) (1.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /venv/main/lib/python3.10/site-packages (from matplotlib->nnunetv2==2.5.2) (4.31.1)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.10/site-packages (from nibabel->nnunetv2==2.5.2) (59.6.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /venv/main/lib/python3.10/site-packages (from pandas->nnunetv2==2.5.2) (2021.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->nnunetv2==2.5.2) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /venv/main/lib/python3.10/site-packages (from requests->nnunetv2==2.5.2) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->nnunetv2==2.5.2) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->nnunetv2==2.5.2) (2021.10.8)\n",
      "Requirement already satisfied: joblib>=0.11 in /venv/main/lib/python3.10/site-packages (from scikit-learn->nnunetv2==2.5.2) (1.1.0)\n",
      "Requirement already satisfied: PyYAML in /venv/main/lib/python3.10/site-packages (from yacs->nnunetv2==2.5.2) (6.0)\n",
      "Collecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.10.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (397 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.3/397.3 KB\u001b[0m \u001b[31m171.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ndindex\n",
      "  Downloading ndindex-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.5/475.5 KB\u001b[0m \u001b[31m180.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting msgpack\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 KB\u001b[0m \u001b[31m152.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->nnunetv2==2.5.2) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch>=2.1.2->nnunetv2==2.5.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy->torch>=2.1.2->nnunetv2==2.5.2) (1.3.0)\n",
      "Requirement already satisfied: traceback2 in /venv/main/lib/python3.10/site-packages (from unittest2->batchgenerators>=0.25->nnunetv2==2.5.2) (1.4.0)\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting anyio\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 KB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: linecache2 in /venv/main/lib/python3.10/site-packages (from traceback2->unittest2->batchgenerators>=0.25->nnunetv2==2.5.2) (1.0.0)\n",
      "Collecting sniffio>=1.1\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /venv/main/lib/python3.10/site-packages (from anyio->httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2==2.5.2) (1.2.2)\n",
      "Building wheels for collected packages: nnunetv2, acvl-utils, batchgenerators, batchgeneratorsv2, dynamic-network-architectures\n",
      "doneng editable for nnunetv2 (pyproject.toml) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for nnunetv2: filename=nnunetv2-2.5.2-0.editable-py3-none-any.whl size=12327 sha256=830bcfbe7c11c4802b189950714962de158f3a9ef4448044c078d2e597eae70b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-a_8i2pgt/wheels/5c/83/0d/43c0bd2f78e7bbeadb1f1e281270cf569dede5cc64137b775a\n",
      "doneng wheel for acvl-utils (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for acvl-utils: filename=acvl_utils-0.2.5-py3-none-any.whl size=27238 sha256=d5b0d44303594ad0d36105a8d3ca2d8cff653ccb0db0316a0214994ea0a9ccff\n",
      "  Stored in directory: /root/.cache/pip/wheels/11/68/32/42aed9ef6a7c3ff868f84b243120481ef8a19d0ddbc2551133\n",
      "doneng wheel for batchgenerators (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93106 sha256=49071adc0eb73dd7c903385abe6369283e65f76fdccb6dd3fe9de924c34c2747\n",
      "  Stored in directory: /root/.cache/pip/wheels/be/1b/30/b3f066999ad01855fc903fe7c93c25682333dd5645d5c75434\n",
      "doneng wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.2.3-py3-none-any.whl size=47527 sha256=cb304b131cc9834a72aa13af28f9a59afaf9978557e4bd7d6f821730781134b0\n",
      "  Stored in directory: /root/.cache/pip/wheels/d5/88/d9/e23210f09e2f05424773dd2eceb3cb74a3eb7f1f6e7c072254\n",
      "doneng wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.3.1-py3-none-any.whl size=30081 sha256=f09bd9268cbb542732e83229199d64cda241a2d90fc6ad4c1730f27b8ba45925\n",
      "  Stored in directory: /root/.cache/pip/wheels/55/1b/13/a6419c8dbf998b9343710355ec3edc5c8e24d9b7b22eec95fb\n",
      "Successfully built nnunetv2 acvl-utils batchgenerators batchgeneratorsv2 dynamic-network-architectures\n",
      "Installing collected packages: py-cpuinfo, argparse, yacs, sniffio, pillow, numpy, networkx, ndindex, msgpack, h11, graphviz, tifffile, scipy, numexpr, lazy-loader, imageio, imagecodecs, httpcore, connected-components-3d, anyio, scikit-image, httpx, fft-conv-pytorch, dynamic-network-architectures, blosc2, batchgenerators, batchgeneratorsv2, acvl-utils, nnunetv2\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.0.1\n",
      "    Uninstalling Pillow-9.0.1:\n",
      "      Successfully uninstalled Pillow-9.0.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.6.3\n",
      "    Uninstalling networkx-2.6.3:\n",
      "      Successfully uninstalled networkx-2.6.3\n",
      "  Attempting uninstall: tifffile\n",
      "    Found existing installation: tifffile 2021.11.2\n",
      "    Uninstalling tifffile-2021.11.2:\n",
      "      Successfully uninstalled tifffile-2021.11.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "  Attempting uninstall: imageio\n",
      "    Found existing installation: imageio 2.16.1\n",
      "    Uninstalling imageio-2.16.1:\n",
      "      Successfully uninstalled imageio-2.16.1\n",
      "  Attempting uninstall: scikit-image\n",
      "    Found existing installation: scikit-image 0.19.2\n",
      "    Uninstalling scikit-image-0.19.2:\n",
      "      Successfully uninstalled scikit-image-0.19.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch-tensorrt 2.6.0+cu124 requires packaging>=23, but you have packaging 21.3 which is incompatible.\n",
      "torch-tensorrt 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\n",
      "nvidia-modelopt 0.17.0 requires numpy<2, but you have numpy 2.2.3 which is incompatible.\n",
      "datasets 3.3.2 requires requests>=2.32.2, but you have requests 2.27.1 which is incompatible.\n",
      "datasets 3.3.2 requires tqdm>=4.66.3, but you have tqdm 4.63.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed acvl-utils-0.2.5 anyio-4.8.0 argparse-1.4.0 batchgenerators-0.25.1 batchgeneratorsv2-0.2.3 blosc2-3.0.0b4 connected-components-3d-3.23.0 dynamic-network-architectures-0.3.1 fft-conv-pytorch-1.2.0 graphviz-0.20.3 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 imagecodecs-2024.12.30 imageio-2.37.0 lazy-loader-0.4 msgpack-1.1.0 ndindex-1.9.2 networkx-3.4.2 nnunetv2-2.5.2 numexpr-2.10.2 numpy-2.2.3 pillow-11.1.0 py-cpuinfo-9.0.0 scikit-image-0.25.2 scipy-1.15.2 sniffio-1.3.1 tifffile-2025.2.18 yacs-0.1.8\n",
      "Requirement already satisfied: cupy-cuda12x in /venv/main/lib/python3.10/site-packages (13.4.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22 in /venv/main/lib/python3.10/site-packages (from cupy-cuda12x) (2.2.3)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /venv/main/lib/python3.10/site-packages (from cupy-cuda12x) (0.8.3)\n"
     ]
    }
   ],
   "source": [
    "!cd nnUNet && pip install -e .\n",
    "!pip install cupy-cuda12x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e013b401-ab18-4c52-b73b-f67152175201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2.0.0\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch-tensorrt 2.6.0+cu124 requires packaging>=23, but you have packaging 21.3 which is incompatible.\n",
      "torch-tensorrt 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\n",
      "datasets 3.3.2 requires requests>=2.32.2, but you have requests 2.27.1 which is incompatible.\n",
      "datasets 3.3.2 requires tqdm>=4.66.3, but you have tqdm 4.63.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fcd8752-0211-4f65-a3ae-c45bda421c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT-LLM is not installed. Please install TensorRT-LLM or set TRTLLM_PLUGINS_PATH to the directory containing libnvinfer_plugin_tensorrt_llm.so to use converters for torch.distributed ops\n",
      "[03/11/2025-06:45:19] [TRT] [W] Functionality provided through tensorrt.plugin module is experimental.\n",
      "Loading extension modelopt_round_and_pack_ext...\n",
      "\n",
      "Loading onnx model from path: /workspace/onnx_models/quant_fast_unet_int8.onnx\n",
      "Compiling the TensorRT engine. This may take a few minutes...\n",
      "Writing onnx model to path: /tmp/tmp6excjbxs/onnx\n",
      "INFO:root:trtexec --onnx=/tmp/tmp6excjbxs/onnx/quant_fast_unet_int8.onnx --fp16 --int8 --saveEngine=/tmp/tmp6excjbxs/quant_fast_unet_int8/quant_fast_unet_int8.engine --skipInference --builderOptimizationLevel=4 --verbose --exportLayerInfo=/tmp/tmp6excjbxs/quant_fast_unet_int8/quant_fast_unet_int8.engine.graph.json\n",
      "INFO:root:build_engine: 431415.07482528687 ms\n",
      "Compilation completed.\n",
      "Size of the TensorRT engine: 49.52 MB\n",
      "INFO:root:trtexec --loadEngine=/tmp/tmp1ta7tp8w/engine --warmUp=500 --avgRuns=100 --iterations=100 --dumpProfile --separateProfileRun --exportProfile=/tmp/tmp1ta7tp8w/profile\n",
      "INFO:root:profile_engine: 10187.234878540039 ms\n",
      "Inference latency reported by device_model: 43.6715 ms\n",
      "data.shape: torch.Size([1, 206, 512, 512])                | 0/1 [00:00<?, ?it/s]\n",
      "Reshaped data from [206 512 512] to [103 247 247]\n",
      "reshaped_final_data shape: torch.Size([1, 103, 247, 247])\n",
      "[03/11/2025-06:52:44] [TRT] [E] IRuntime::deserializeCudaEngine: Error Code 6: API Usage Error (The engine plan file is not compatible with this version of TensorRT, expecting library version 10.7.0.23 got \n",
      ".\t.)\n",
      "  0%|                                                     | 0/1 [00:01<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/nnunet_infer_nii.py\", line 434, in <module>\n",
      "    seg = predictor.inference(image, props, args.use_softmax)\n",
      "  File \"/workspace/nnunet_infer_nii.py\", line 260, in inference\n",
      "    predicted_logits = self._internal_predict_sliding_window_return_logits(data, slicers,\n",
      "  File \"/workspace/nnunet_infer_nii.py\", line 238, in _internal_predict_sliding_window_return_logits\n",
      "    raise e\n",
      "  File \"/workspace/nnunet_infer_nii.py\", line 222, in _internal_predict_sliding_window_return_logits\n",
      "    prediction = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n",
      "  File \"/workspace/nnUNet/nnunetv2/inference/predict_from_raw_data.py\", line 543, in _internal_maybe_mirror_and_predict\n",
      "    prediction = self.network(x)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/modelopt/torch/_deploy/device_model.py\", line 42, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/modelopt/torch/_deploy/device_model.py\", line 102, in forward\n",
      "    np_outputs = self.client.inference(compiled_model=self.compiled_model, inputs=np_inputs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/modelopt/torch/_deploy/_runtime/runtime_client.py\", line 112, in inference\n",
      "    outputs = self._inference(compiled_model, inputs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/modelopt/torch/_deploy/_runtime/trt_client.py\", line 98, in _inference\n",
      "    self.inference_sessions[model_hash] = self.TRTSession(\n",
      "  File \"/venv/main/lib/python3.10/site-packages/modelopt/torch/_deploy/_runtime/trt_client.py\", line 112, in __init__\n",
      "    assert self.engine is not None, \"Engine deserialization failed.\"\n",
      "AssertionError: Engine deserialization failed.\n",
      "WARNING:py.warnings:/usr/lib/python3.10/tempfile.py:1008: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp0unl4dhu'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python nnunet_infer_nii.py -i sample_data/ -o ./seg --model_path model_weights/701/nnUNetTrainerMICCAI__nnUNetPlans__3d_fullres --run_engine_trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5720d14-b2a4-4dae-adb7-93ece78ee3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec --onnx=/workspace/onnx_models/quant_fast_unet_int8.onnx --fp16 --int8 --saveEngine=/workspace/tmp/quant_fast_unet_int8/quant_fast_unet_int8.engine --skipInference --builderOptimizationLevel=4 --verbose --exportLayerInfo=/workspace/tmp/quant_fast_unet_int8/quant_fast_unet_int8.engine.graph.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e71508-9577-43d0-9fdc-610c6fdb0f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec --loadEngine=/tmp/tmp3q3i8vtk/engine --warmUp=500 --avgRuns=100 --iterations=100 --dumpProfile --separateProfileRun --exportProfile=/tmp/tmp3q3i8vtk/profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e5cad-34e3-415b-9510-50f925dc4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12e155fe-48f6-403e-9a87-c9b28834d086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-11 06:39:39--  https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.7.0/tars/TensorRT-10.7.0.23.Linux.x86_64-gnu.cuda-12.6.tar.gz\n",
      "Resolving developer.nvidia.com (developer.nvidia.com)... 23.53.4.17, 23.53.4.11\n",
      "Connecting to developer.nvidia.com (developer.nvidia.com)|23.53.4.17|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://developer.download.nvidia.com/compute/machine-learning/tensorrt/10.7.0/tars/TensorRT-10.7.0.23.Linux.x86_64-gnu.cuda-12.6.tar.gz [following]\n",
      "--2025-03-11 06:39:39--  https://developer.download.nvidia.com/compute/machine-learning/tensorrt/10.7.0/tars/TensorRT-10.7.0.23.Linux.x86_64-gnu.cuda-12.6.tar.gz\n",
      "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.53.4.17, 23.53.4.11\n",
      "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.53.4.17|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 4446480887 (4.1G) [application/x-gzip]\n",
      "Saving to: ‘TensorRT-10.7.0.23.Linux.x86_64-gnu.cuda-12.6.tar.gz’\n",
      "\n",
      "TensorRT-10.7.0.23. 100%[===================>]   4.14G  22.5MB/s    in 3m 17s  \n",
      "\n",
      "2025-03-11 06:42:56 (21.6 MB/s) - ‘TensorRT-10.7.0.23.Linux.x86_64-gnu.cuda-12.6.tar.gz’ saved [4446480887/4446480887]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.7.0/tars/TensorRT-10.7.0.23.Linux.x86_64-gnu.cuda-12.6.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93fcf3f7-d867-43a1-b5ca-beb51d29b104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT-10.7.0.23/\n",
      "TensorRT-10.7.0.23/data/\n",
      "TensorRT-10.7.0.23/data/char-rnn/\n",
      "TensorRT-10.7.0.23/data/char-rnn/model/\n",
      "TensorRT-10.7.0.23/data/char-rnn/model/model-20080.data-00000-of-00001\n",
      "TensorRT-10.7.0.23/data/char-rnn/model/model-20080.meta\n",
      "TensorRT-10.7.0.23/data/char-rnn/model/model-20080.index\n",
      "TensorRT-10.7.0.23/data/char-rnn/model/checkpoint\n",
      "TensorRT-10.7.0.23/data/char-rnn/char-rnn.wts\n",
      "TensorRT-10.7.0.23/data/int8_api/\n",
      "TensorRT-10.7.0.23/data/int8_api/reference_labels.txt\n",
      "TensorRT-10.7.0.23/data/int8_api/resnet50_per_tensor_dynamic_range.txt\n",
      "TensorRT-10.7.0.23/data/int8_api/README.md\n",
      "TensorRT-10.7.0.23/data/int8_api/airliner.ppm\n",
      "TensorRT-10.7.0.23/data/mnist/\n",
      "TensorRT-10.7.0.23/data/mnist/9.pgm\n",
      "TensorRT-10.7.0.23/data/mnist/8.pgm\n",
      "TensorRT-10.7.0.23/data/mnist/1.pgm\n",
      "TensorRT-10.7.0.23/data/mnist/4.pgm\n",
      "TensorRT-10.7.0.23/data/mnist/mnist.onnx\n",
      "TensorRT-10.7.0.23/data/mnist/2.pgm\n",
      "TensorRT-10.7.0.23/data/mnist/3.pgm\n",
      "TensorRT-10.7.0.23/data/mnist/6.pgm\n",
      "TensorRT-10.7.0.23/data/mnist/README.md\n",
      "TensorRT-10.7.0.23/data/mnist/7.pgm\n",
      "TensorRT-10.7.0.23/data/mnist/0.pgm\n",
      "TensorRT-10.7.0.23/data/mnist/5.pgm\n",
      "TensorRT-10.7.0.23/data/resnet50/\n",
      "TensorRT-10.7.0.23/data/resnet50/class_labels.txt\n",
      "TensorRT-10.7.0.23/data/resnet50/binoculars.jpeg\n",
      "TensorRT-10.7.0.23/data/resnet50/reflex_camera.jpeg\n",
      "TensorRT-10.7.0.23/data/resnet50/tabby_tiger_cat.jpg\n",
      "TensorRT-10.7.0.23/data/resnet50/README.md\n",
      "TensorRT-10.7.0.23/data/resnet50/airliner.ppm\n",
      "TensorRT-10.7.0.23/data/resnet50/ResNet50.onnx\n",
      "TensorRT-10.7.0.23/bin\n",
      "TensorRT-10.7.0.23/lib\n",
      "TensorRT-10.7.0.23/doc/\n",
      "TensorRT-10.7.0.23/doc/Acknowledgements.txt\n",
      "TensorRT-10.7.0.23/doc/Readme.txt\n",
      "TensorRT-10.7.0.23/targets/\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/bin/\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/bin/trtexec\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvonnxparser.so.10\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_builder_resource_win.so.10.7.0\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_static.a\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer.so.10\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_dispatch.so.10.7.0\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_lean.so.10.7.0\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_dispatch_static.a\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_vc_plugin.so.10\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_vc_plugin.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvonnxparser.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_lean.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer.so.10.7.0\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_lean_static.a\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvonnxparser_static.a\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_lean.so.10\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_plugin.so.10.7.0\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_plugin.so.10\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_builder_resource.so.10.7.0\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_plugin_static.a\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_vc_plugin_static.a\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_plugin.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/stubs/\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/stubs/libnvinfer_vc_plugin.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/stubs/libnvonnxparser.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/stubs/libnvinfer_lean.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/stubs/libnvinfer_plugin.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/stubs/libnvinfer_dispatch.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/stubs/libnvinfer.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_dispatch.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_dispatch.so.10\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libonnx_proto.a\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer.so\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvinfer_vc_plugin.so.10.7.0\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/lib/libnvonnxparser.so.10.7.0\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/include\n",
      "TensorRT-10.7.0.23/targets/x86_64-linux-gnu/samples\n",
      "TensorRT-10.7.0.23/python/\n",
      "TensorRT-10.7.0.23/python/tensorrt_dispatch-10.7.0-cp311-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt_lean-10.7.0-cp312-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt-10.7.0-cp38-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt_lean-10.7.0-cp38-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt_dispatch-10.7.0-cp39-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt_lean-10.7.0-cp39-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/include/\n",
      "TensorRT-10.7.0.23/python/include/impl/\n",
      "TensorRT-10.7.0.23/python/include/impl/plugin.h\n",
      "TensorRT-10.7.0.23/python/tensorrt_dispatch-10.7.0-cp310-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt_dispatch-10.7.0-cp38-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt_lean-10.7.0-cp311-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt-10.7.0-cp310-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt_lean-10.7.0-cp310-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt-10.7.0-cp311-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt-10.7.0-cp39-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt-10.7.0-cp312-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/python/tensorrt_dispatch-10.7.0-cp312-none-linux_x86_64.whl\n",
      "TensorRT-10.7.0.23/include/\n",
      "TensorRT-10.7.0.23/include/NvOnnxConfig.h\n",
      "TensorRT-10.7.0.23/include/NvInfer.h\n",
      "TensorRT-10.7.0.23/include/NvInferPluginBase.h\n",
      "TensorRT-10.7.0.23/include/NvInferImpl.h\n",
      "TensorRT-10.7.0.23/include/NvInferRuntimePlugin.h\n",
      "TensorRT-10.7.0.23/include/NvInferRuntime.h\n",
      "TensorRT-10.7.0.23/include/NvInferRuntimeBase.h\n",
      "TensorRT-10.7.0.23/include/NvInferPluginUtils.h\n",
      "TensorRT-10.7.0.23/include/NvInferVersion.h\n",
      "TensorRT-10.7.0.23/include/NvInferRuntimeCommon.h\n",
      "TensorRT-10.7.0.23/include/NvInferPlugin.h\n",
      "TensorRT-10.7.0.23/include/NvOnnxParser.h\n",
      "TensorRT-10.7.0.23/include/NvInferLegacyDims.h\n",
      "TensorRT-10.7.0.23/samples/\n",
      "TensorRT-10.7.0.23/samples/sampleNonZeroPlugin/\n",
      "TensorRT-10.7.0.23/samples/sampleNonZeroPlugin/sampleNonZeroPlugin.cpp\n",
      "TensorRT-10.7.0.23/samples/sampleNonZeroPlugin/nonZeroKernel.h\n",
      "TensorRT-10.7.0.23/samples/sampleNonZeroPlugin/Makefile\n",
      "TensorRT-10.7.0.23/samples/sampleNonZeroPlugin/nonZeroKernel.cu\n",
      "TensorRT-10.7.0.23/samples/sampleNonZeroPlugin/README.md\n",
      "TensorRT-10.7.0.23/samples/trtexec/\n",
      "TensorRT-10.7.0.23/samples/trtexec/tracer.py\n",
      "TensorRT-10.7.0.23/samples/trtexec/prn_utils.py\n",
      "TensorRT-10.7.0.23/samples/trtexec/trtexec.cpp\n",
      "TensorRT-10.7.0.23/samples/trtexec/Makefile\n",
      "TensorRT-10.7.0.23/samples/trtexec/profiler.py\n",
      "TensorRT-10.7.0.23/samples/trtexec/README.md\n",
      "TensorRT-10.7.0.23/samples/sampleINT8API/\n",
      "TensorRT-10.7.0.23/samples/sampleINT8API/Makefile\n",
      "TensorRT-10.7.0.23/samples/sampleINT8API/sampleINT8API.cpp\n",
      "TensorRT-10.7.0.23/samples/sampleINT8API/README.md\n",
      "TensorRT-10.7.0.23/samples/sampleNamedDimensions/\n",
      "TensorRT-10.7.0.23/samples/sampleNamedDimensions/sampleNamedDimensions.cpp\n",
      "TensorRT-10.7.0.23/samples/sampleNamedDimensions/Makefile\n",
      "TensorRT-10.7.0.23/samples/sampleNamedDimensions/create_model.py\n",
      "TensorRT-10.7.0.23/samples/sampleNamedDimensions/README.md\n",
      "TensorRT-10.7.0.23/samples/sampleDynamicReshape/\n",
      "TensorRT-10.7.0.23/samples/sampleDynamicReshape/Makefile\n",
      "TensorRT-10.7.0.23/samples/sampleDynamicReshape/sampleDynamicReshape.cpp\n",
      "TensorRT-10.7.0.23/samples/sampleDynamicReshape/README.md\n",
      "TensorRT-10.7.0.23/samples/sampleAlgorithmSelector/\n",
      "TensorRT-10.7.0.23/samples/sampleAlgorithmSelector/Makefile\n",
      "TensorRT-10.7.0.23/samples/sampleAlgorithmSelector/sampleAlgorithmSelector.cpp\n",
      "TensorRT-10.7.0.23/samples/sampleAlgorithmSelector/README.md\n",
      "TensorRT-10.7.0.23/samples/common/\n",
      "TensorRT-10.7.0.23/samples/common/getOptions.h\n",
      "TensorRT-10.7.0.23/samples/common/sampleDevice.h\n",
      "TensorRT-10.7.0.23/samples/common/sampleInference.h\n",
      "TensorRT-10.7.0.23/samples/common/argsParser.h\n",
      "TensorRT-10.7.0.23/samples/common/logger.cpp\n",
      "TensorRT-10.7.0.23/samples/common/ErrorRecorder.h\n",
      "TensorRT-10.7.0.23/samples/common/sampleInference.cpp\n",
      "TensorRT-10.7.0.23/samples/common/parserOnnxConfig.h\n",
      "TensorRT-10.7.0.23/samples/common/sampleUtils.cpp\n",
      "TensorRT-10.7.0.23/samples/common/logging.h\n",
      "TensorRT-10.7.0.23/samples/common/dumpTFWts.py\n",
      "TensorRT-10.7.0.23/samples/common/bfloat16.h\n",
      "TensorRT-10.7.0.23/samples/common/EntropyCalibrator.h\n",
      "TensorRT-10.7.0.23/samples/common/sampleReporting.cpp\n",
      "TensorRT-10.7.0.23/samples/common/sampleOptions.h\n",
      "TensorRT-10.7.0.23/samples/common/sampleEngines.cpp\n",
      "TensorRT-10.7.0.23/samples/common/sampleEntrypoints.h\n",
      "TensorRT-10.7.0.23/samples/common/bfloat16.cpp\n",
      "TensorRT-10.7.0.23/samples/common/common.h\n",
      "TensorRT-10.7.0.23/samples/common/sampleDevice.cpp\n",
      "TensorRT-10.7.0.23/samples/common/sampleConfig.h\n",
      "TensorRT-10.7.0.23/samples/common/sampleReporting.h\n",
      "TensorRT-10.7.0.23/samples/common/getoptWin.h\n",
      "TensorRT-10.7.0.23/samples/common/logger.h\n",
      "TensorRT-10.7.0.23/samples/common/buffers.h\n",
      "TensorRT-10.7.0.23/samples/common/BatchStream.h\n",
      "TensorRT-10.7.0.23/samples/common/streamReader.h\n",
      "TensorRT-10.7.0.23/samples/common/sampleOptions.cpp\n",
      "TensorRT-10.7.0.23/samples/common/safeCommon.h\n",
      "TensorRT-10.7.0.23/samples/common/sampleEngines.h\n",
      "TensorRT-10.7.0.23/samples/common/half.h\n",
      "TensorRT-10.7.0.23/samples/common/sampleUtils.h\n",
      "TensorRT-10.7.0.23/samples/common/getOptions.cpp\n",
      "TensorRT-10.7.0.23/samples/common/getopt.c\n",
      "TensorRT-10.7.0.23/samples/utils/\n",
      "TensorRT-10.7.0.23/samples/utils/fileLock.cpp\n",
      "TensorRT-10.7.0.23/samples/utils/timingCache.h\n",
      "TensorRT-10.7.0.23/samples/utils/timingCache.cpp\n",
      "TensorRT-10.7.0.23/samples/utils/fileLock.h\n",
      "TensorRT-10.7.0.23/samples/python/\n",
      "TensorRT-10.7.0.23/samples/python/efficientnet/\n",
      "TensorRT-10.7.0.23/samples/python/efficientnet/infer.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientnet/create_onnx.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientnet/build_engine.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientnet/image_batcher.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientnet/compare_tf.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientnet/README.md\n",
      "TensorRT-10.7.0.23/samples/python/efficientnet/eval_gt.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientnet/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/onnx_packnet/\n",
      "TensorRT-10.7.0.23/samples/python/onnx_packnet/convert_to_onnx.py\n",
      "TensorRT-10.7.0.23/samples/python/onnx_packnet/download.yml\n",
      "TensorRT-10.7.0.23/samples/python/onnx_packnet/README.md\n",
      "TensorRT-10.7.0.23/samples/python/onnx_packnet/post_processing.py\n",
      "TensorRT-10.7.0.23/samples/python/onnx_packnet/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/non_zero_plugin/\n",
      "TensorRT-10.7.0.23/samples/python/non_zero_plugin/non_zero_plugin.py\n",
      "TensorRT-10.7.0.23/samples/python/non_zero_plugin/README.md\n",
      "TensorRT-10.7.0.23/samples/python/non_zero_plugin/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/downloader.py\n",
      "TensorRT-10.7.0.23/samples/python/onnx_custom_plugin/\n",
      "TensorRT-10.7.0.23/samples/python/onnx_custom_plugin/load_plugin_lib.py\n",
      "TensorRT-10.7.0.23/samples/python/onnx_custom_plugin/model.py\n",
      "TensorRT-10.7.0.23/samples/python/onnx_custom_plugin/CMakeLists.txt\n",
      "TensorRT-10.7.0.23/samples/python/onnx_custom_plugin/test_custom_hardmax_plugin.py\n",
      "TensorRT-10.7.0.23/samples/python/onnx_custom_plugin/README.md\n",
      "TensorRT-10.7.0.23/samples/python/onnx_custom_plugin/sample.py\n",
      "TensorRT-10.7.0.23/samples/python/onnx_custom_plugin/plugin/\n",
      "TensorRT-10.7.0.23/samples/python/onnx_custom_plugin/plugin/customHardmaxPlugin.h\n",
      "TensorRT-10.7.0.23/samples/python/onnx_custom_plugin/plugin/customHardmaxPlugin.cpp\n",
      "TensorRT-10.7.0.23/samples/python/onnx_custom_plugin/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/common.py\n",
      "TensorRT-10.7.0.23/samples/python/aliased_io_plugin/\n",
      "TensorRT-10.7.0.23/samples/python/aliased_io_plugin/aliased_io_plugin.py\n",
      "TensorRT-10.7.0.23/samples/python/aliased_io_plugin/aliased_io_gnn.png\n",
      "TensorRT-10.7.0.23/samples/python/aliased_io_plugin/README.md\n",
      "TensorRT-10.7.0.23/samples/python/aliased_io_plugin/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/detectron2/\n",
      "TensorRT-10.7.0.23/samples/python/detectron2/infer.py\n",
      "TensorRT-10.7.0.23/samples/python/detectron2/create_onnx.py\n",
      "TensorRT-10.7.0.23/samples/python/detectron2/build_engine.py\n",
      "TensorRT-10.7.0.23/samples/python/detectron2/image_batcher.py\n",
      "TensorRT-10.7.0.23/samples/python/detectron2/onnx_utils.py\n",
      "TensorRT-10.7.0.23/samples/python/detectron2/eval_coco.py\n",
      "TensorRT-10.7.0.23/samples/python/detectron2/README.md\n",
      "TensorRT-10.7.0.23/samples/python/detectron2/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/detectron2/visualize.py\n",
      "TensorRT-10.7.0.23/samples/python/simple_progress_monitor/\n",
      "TensorRT-10.7.0.23/samples/python/simple_progress_monitor/simple_progress_monitor.py\n",
      "TensorRT-10.7.0.23/samples/python/simple_progress_monitor/README.md\n",
      "TensorRT-10.7.0.23/samples/python/simple_progress_monitor/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/sample_weight_stripping/\n",
      "TensorRT-10.7.0.23/samples/python/sample_weight_stripping/build_engines.py\n",
      "TensorRT-10.7.0.23/samples/python/sample_weight_stripping/refit_engine_and_infer.py\n",
      "TensorRT-10.7.0.23/samples/python/sample_weight_stripping/README.md\n",
      "TensorRT-10.7.0.23/samples/python/sample_weight_stripping/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/circ_pad_plugin_numba.py\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/circ_pad_plugin_multi_tactic.py\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/circ_pad_plugin_torch.py\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/circ_pad_plugin_cpp.py\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/circ_pad_example.png\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/CMakeLists.txt\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/circ_pad_plugin_inetdef_cuda_python.py\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/circ_pad_plugin_triton.py\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/circ_pad_plugin_cuda_python.py\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/circ_plugin_cpp/\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/circ_plugin_cpp/circ_pad_plugin.cu\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/README.md\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/circ_pad_plugin_cupy.py\n",
      "TensorRT-10.7.0.23/samples/python/python_plugin/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/yolov3_onnx/\n",
      "TensorRT-10.7.0.23/samples/python/yolov3_onnx/onnx_to_tensorrt.py\n",
      "TensorRT-10.7.0.23/samples/python/yolov3_onnx/coco_labels.txt\n",
      "TensorRT-10.7.0.23/samples/python/yolov3_onnx/data_processing.py\n",
      "TensorRT-10.7.0.23/samples/python/yolov3_onnx/download.yml\n",
      "TensorRT-10.7.0.23/samples/python/yolov3_onnx/yolov3_to_onnx.py\n",
      "TensorRT-10.7.0.23/samples/python/yolov3_onnx/README.md\n",
      "TensorRT-10.7.0.23/samples/python/yolov3_onnx/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/introductory_parser_samples/\n",
      "TensorRT-10.7.0.23/samples/python/introductory_parser_samples/README.md\n",
      "TensorRT-10.7.0.23/samples/python/introductory_parser_samples/onnx_resnet50.py\n",
      "TensorRT-10.7.0.23/samples/python/introductory_parser_samples/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/engine_refit_onnx_bidaf/\n",
      "TensorRT-10.7.0.23/samples/python/engine_refit_onnx_bidaf/build_and_refit_engine.py\n",
      "TensorRT-10.7.0.23/samples/python/engine_refit_onnx_bidaf/data_processing.py\n",
      "TensorRT-10.7.0.23/samples/python/engine_refit_onnx_bidaf/download.yml\n",
      "TensorRT-10.7.0.23/samples/python/engine_refit_onnx_bidaf/prepare_model.py\n",
      "TensorRT-10.7.0.23/samples/python/engine_refit_onnx_bidaf/README.md\n",
      "TensorRT-10.7.0.23/samples/python/engine_refit_onnx_bidaf/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/quickly_deployable_plugins/\n",
      "TensorRT-10.7.0.23/samples/python/quickly_deployable_plugins/qdp_runner.py\n",
      "TensorRT-10.7.0.23/samples/python/quickly_deployable_plugins/oait_kernels.py\n",
      "TensorRT-10.7.0.23/samples/python/quickly_deployable_plugins/qdp_defs.py\n",
      "TensorRT-10.7.0.23/samples/python/quickly_deployable_plugins/README.md\n",
      "TensorRT-10.7.0.23/samples/python/quickly_deployable_plugins/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/infer.py\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/labels_coco.txt\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/create_onnx.py\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/build_engine.py\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/image_batcher.py\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/compare_tf.py\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/onnx_utils.py\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/eval_coco.py\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/README.md\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/tensorflow_object_detection_api/visualize.py\n",
      "TensorRT-10.7.0.23/samples/python/network_api_pytorch_mnist/\n",
      "TensorRT-10.7.0.23/samples/python/network_api_pytorch_mnist/model.py\n",
      "TensorRT-10.7.0.23/samples/python/network_api_pytorch_mnist/README.md\n",
      "TensorRT-10.7.0.23/samples/python/network_api_pytorch_mnist/sample.py\n",
      "TensorRT-10.7.0.23/samples/python/network_api_pytorch_mnist/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/common_runtime.py\n",
      "TensorRT-10.7.0.23/samples/python/README.md\n",
      "TensorRT-10.7.0.23/samples/python/plugin_utils.py\n",
      "TensorRT-10.7.0.23/samples/python/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/infer.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/labels_coco.txt\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/create_onnx.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/build_engine.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/image_batcher.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/compare_tf.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/infer_tf.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/onnx_utils.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/eval_coco.py\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/README.md\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/requirements.txt\n",
      "TensorRT-10.7.0.23/samples/python/efficientdet/visualize.py\n",
      "TensorRT-10.7.0.23/samples/sampleOnnxMNIST/\n",
      "TensorRT-10.7.0.23/samples/sampleOnnxMNIST/sampleOnnxMNIST.cpp\n",
      "TensorRT-10.7.0.23/samples/sampleOnnxMNIST/Makefile\n",
      "TensorRT-10.7.0.23/samples/sampleOnnxMNIST/README.md\n",
      "TensorRT-10.7.0.23/samples/Makefile\n",
      "TensorRT-10.7.0.23/samples/Makefile.config\n",
      "TensorRT-10.7.0.23/samples/sampleProgressMonitor/\n",
      "TensorRT-10.7.0.23/samples/sampleProgressMonitor/sampleProgressMonitor.cpp\n",
      "TensorRT-10.7.0.23/samples/sampleProgressMonitor/Makefile\n",
      "TensorRT-10.7.0.23/samples/sampleProgressMonitor/README.md\n",
      "TensorRT-10.7.0.23/samples/sampleOnnxMnistCoordConvAC/\n",
      "TensorRT-10.7.0.23/samples/sampleOnnxMnistCoordConvAC/sampleOnnxMnistCoordConvAC.cpp\n",
      "TensorRT-10.7.0.23/samples/sampleOnnxMnistCoordConvAC/coord_conv.py\n",
      "TensorRT-10.7.0.23/samples/sampleOnnxMnistCoordConvAC/Makefile\n",
      "TensorRT-10.7.0.23/samples/sampleOnnxMnistCoordConvAC/mnist_coord_conv_train.py\n",
      "TensorRT-10.7.0.23/samples/sampleOnnxMnistCoordConvAC/README.md\n",
      "TensorRT-10.7.0.23/samples/sampleOnnxMnistCoordConvAC/modify_onnx_ac.py\n",
      "TensorRT-10.7.0.23/samples/sampleCharRNN/\n",
      "TensorRT-10.7.0.23/samples/sampleCharRNN/sampleCharRNN.cpp\n",
      "TensorRT-10.7.0.23/samples/sampleCharRNN/Makefile\n",
      "TensorRT-10.7.0.23/samples/sampleCharRNN/README.md\n",
      "TensorRT-10.7.0.23/samples/sampleIOFormats/\n",
      "TensorRT-10.7.0.23/samples/sampleIOFormats/Makefile\n",
      "TensorRT-10.7.0.23/samples/sampleIOFormats/sampleIOFormats.cpp\n",
      "TensorRT-10.7.0.23/samples/sampleIOFormats/README.md\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf TensorRT-10.7.0.23.Linux.x86_64-gnu.cuda-12.6.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9e37dbf-6a6c-45fd-aaae-2479814ae436",
   "metadata": {},
   "outputs": [],
   "source": [
    "!TENSORRT_PATH=$(pwd)/TensorRT-10.7.0.23\n",
    "!echo \"export TENSORRT_HOME=${TENSORRT_PATH}\" >> ~/.bashrc\n",
    "!echo \"export PATH=\\$TENSORRT_HOME/bin:\\$PATH\" >> ~/.bashrc\n",
    "!echo \"export LD_LIBRARY_PATH=\\$TENSORRT_HOME/lib:\\$LD_LIBRARY_PATH\" >> ~/.bashrc\n",
    "!ln -sf /workspace/TensorRT-10.7.0.23/bin/trtexec /usr/local/bin/trtexec\n",
    "!source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b626533-7997-4f31-85b0-50d200241e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /venv/main/lib/python3.10/site-packages (2.4.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: torchvision in /venv/main/lib/python3.10/site-packages (0.19.1)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Collecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m229.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m185.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting triton==3.2.0\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m238.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m250.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /venv/main/lib/python3.10/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.68\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.68:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.68\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.1\n",
      "    Uninstalling torch-2.4.1:\n",
      "      Successfully uninstalled torch-2.4.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.19.1\n",
      "    Uninstalling torchvision-0.19.1:\n",
      "      Successfully uninstalled torchvision-0.19.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch-tensorrt 2.6.0+cu124 requires packaging>=23, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 torchvision-0.21.0 triton-3.2.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3d2482b-e083-4902-8442-a0403a37064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v100700] [b23] # trtexec\n",
      "=== Model Options ===\n",
      "  --onnx=<file>               ONNX model\n",
      "\n",
      "=== Build Options ===\n",
      "  --minShapes=spec                   Build with dynamic shapes using a profile with the min shapes provided\n",
      "  --optShapes=spec                   Build with dynamic shapes using a profile with the opt shapes provided\n",
      "  --maxShapes=spec                   Build with dynamic shapes using a profile with the max shapes provided\n",
      "  --minShapesCalib=spec              Calibrate with dynamic shapes using a profile with the min shapes provided\n",
      "  --optShapesCalib=spec              Calibrate with dynamic shapes using a profile with the opt shapes provided\n",
      "  --maxShapesCalib=spec              Calibrate with dynamic shapes using a profile with the max shapes provided\n",
      "                                     Note: All three of min, opt and max shapes must be supplied.\n",
      "                                           However, if only opt shapes is supplied then it will be expanded so\n",
      "                                           that min shapes and max shapes are set to the same values as opt shapes.\n",
      "                                           Input names can be wrapped with escaped single quotes (ex: 'Input:0').\n",
      "                                     Example input shapes spec: input0:1x3x256x256,input1:1x3x128x128\n",
      "                                     For scalars (0-D shapes), use input0:scalar or simply input0: with nothing after the colon.\n",
      "                                     Each input shape is supplied as a key-value pair where key is the input name and\n",
      "                                     value is the dimensions (including the batch dimension) to be used for that input.\n",
      "                                     Each key-value pair has the key and value separated using a colon (:).\n",
      "                                     Multiple input shapes can be provided via comma-separated key-value pairs, and each input name can\n",
      "                                     contain at most one wildcard ('*') character.\n",
      "  --inputIOFormats=spec              Type and format of each of the input tensors (default = all inputs in fp32:chw)\n",
      "                                     See --outputIOFormats help for the grammar of type and format list.\n",
      "                                     Note: If this option is specified, please set comma-separated types and formats for all\n",
      "                                           inputs following the same order as network inputs ID (even if only one input\n",
      "                                           needs specifying IO format) or set the type and format once for broadcasting.\n",
      "  --outputIOFormats=spec             Type and format of each of the output tensors (default = all outputs in fp32:chw)\n",
      "                                     Note: If this option is specified, please set comma-separated types and formats for all\n",
      "                                           outputs following the same order as network outputs ID (even if only one output\n",
      "                                           needs specifying IO format) or set the type and format once for broadcasting.\n",
      "                                     IO Formats: spec  ::= IOfmt[\",\"spec]\n",
      "                                                 IOfmt ::= type:fmt\n",
      "                                               type  ::= \"fp32\"|\"fp16\"|\"bf16\"|\"int32\"|\"int64\"|\"int8\"|\"uint8\"|\"bool\"\n",
      "                                               fmt   ::= (\"chw\"|\"chw2\"|\"chw4\"|\"hwc8\"|\"chw16\"|\"chw32\"|\"dhwc8\"|\n",
      "                                                          \"cdhw32\"|\"hwc\"|\"dla_linear\"|\"dla_hwc4\")[\"+\"fmt]\n",
      "  --memPoolSize=poolspec             Specify the size constraints of the designated memory pool(s)\n",
      "                                     Supports the following base-2 suffixes: B (Bytes), G (Gibibytes), K (Kibibytes), M (Mebibytes).\n",
      "                                     If none of suffixes is appended, the defualt unit is in MiB.\n",
      "                                     Note: Also accepts decimal sizes, e.g. 0.25M. Will be rounded down to the nearest integer bytes.\n",
      "                                     In particular, for dlaSRAM the bytes will be rounded down to the nearest power of 2.\n",
      "                                   Pool constraint: poolspec ::= poolfmt[\",\"poolspec]\n",
      "                                                      poolfmt ::= pool:size\n",
      "                                                    pool ::= \"workspace\"|\"dlaSRAM\"|\"dlaLocalDRAM\"|\"dlaGlobalDRAM\"|\"tacticSharedMem\"\n",
      "  --profilingVerbosity=mode          Specify profiling verbosity. mode ::= layer_names_only|detailed|none (default = layer_names_only).\n",
      "                                     Please only assign once.\n",
      "  --avgTiming=M                      Set the number of times averaged in each iteration for kernel selection (default = 8)\n",
      "  --refit                            Mark the engine as refittable. This will allow the inspection of refittable layers \n",
      "                                     and weights within the engine.\n",
      "  --stripWeights                     Strip weights from plan. This flag works with either refit or refit with identical weights. Default\n",
      "                                     to latter, but you can switch to the former by enabling both --stripWeights and --refit at the same\n",
      "                                     time.\n",
      "  --stripAllWeights                  Alias for combining the --refit and --stripWeights options. It marks all weights as refittable,\n",
      "                                     disregarding any performance impact. Additionally, it strips all refittable weights after the \n",
      "                                     engine is built.\n",
      "  --weightless                       [Deprecated] this knob has been deprecated. Please use --stripWeights\n",
      "  --versionCompatible, --vc          Mark the engine as version compatible. This allows the engine to be used with newer versions\n",
      "                                     of TensorRT on the same host OS, as well as TensorRT's dispatch and lean runtimes.\n",
      "  --pluginInstanceNorm, --pi         Set `kNATIVE_INSTANCENORM` to false in the ONNX parser. This will cause the ONNX parser to use\n",
      "                                     a plugin InstanceNorm implementation over the native implementation when parsing.\n",
      "  --useRuntime=runtime               TensorRT runtime to execute engine. \"lean\" and \"dispatch\" require loading VC engine and do\n",
      "                                     not support building an engine.\n",
      "                                           runtime::= \"full\"|\"lean\"|\"dispatch\"\n",
      "  --leanDLLPath=<file>               External lean runtime DLL to use in version compatiable mode.\n",
      "  --excludeLeanRuntime               When --versionCompatible is enabled, this flag indicates that the generated engine should\n",
      "                                     not include an embedded lean runtime. If this is set, the user must explicitly specify a\n",
      "                                     valid lean runtime to use when loading the engine.\n",
      "  --monitorMemory                    Enable memory monitor report for debugging usage. (default = disabled)\n",
      "  --sparsity=spec                    Control sparsity (default = disabled). \n",
      "                                   Sparsity: spec ::= \"disable\", \"enable\", \"force\"\n",
      "                                     Note: Description about each of these options is as below\n",
      "                                           disable = do not enable sparse tactics in the builder (this is the default)\n",
      "                                           enable  = enable sparse tactics in the builder (but these tactics will only be\n",
      "                                                     considered if the weights have the right sparsity pattern)\n",
      "                                           force   = enable sparse tactics in the builder and force-overwrite the weights to have\n",
      "                                                     a sparsity pattern (even if you loaded a model yourself)\n",
      "                                                     [Deprecated] this knob has been deprecated.\n",
      "                                                     Please use <polygraphy surgeon prune> to rewrite the weights.\n",
      "  --noTF32                           Disable tf32 precision (default is to enable tf32, in addition to fp32)\n",
      "  --fp16                             Enable fp16 precision, in addition to fp32 (default = disabled)\n",
      "  --bf16                             Enable bf16 precision, in addition to fp32 (default = disabled)\n",
      "  --int8                             Enable int8 precision, in addition to fp32 (default = disabled)\n",
      "  --fp8                              Enable fp8 precision, in addition to fp32 (default = disabled)\n",
      "  --int4                             Enable int4 precision, in addition to fp32 (default = disabled)\n",
      "  --best                             Enable all precisions to achieve the best performance (default = disabled)\n",
      "  --stronglyTyped                    Create a strongly typed network. (default = disabled)\n",
      "  --directIO                         [Deprecated] Avoid reformatting at network boundaries. (default = disabled)\n",
      "  --precisionConstraints=spec        Control precision constraint setting. (default = none)\n",
      "                                       Precision Constraints: spec ::= \"none\" | \"obey\" | \"prefer\"\n",
      "                                         none = no constraints\n",
      "                                         prefer = meet precision constraints set by --layerPrecisions/--layerOutputTypes if possible\n",
      "                                         obey = meet precision constraints set by --layerPrecisions/--layerOutputTypes or fail\n",
      "                                                otherwise\n",
      "  --layerPrecisions=spec             Control per-layer precision constraints. Effective only when precisionConstraints is set to\n",
      "                                   \"obey\" or \"prefer\". (default = none)\n",
      "                                   The specs are read left-to-right, and later ones override earlier ones. Each layer name can\n",
      "                                     contain at most one wildcard ('*') character.\n",
      "                                   Per-layer precision spec ::= layerPrecision[\",\"spec]\n",
      "                                                       layerPrecision ::= layerName\":\"precision\n",
      "                                                       precision ::= \"fp32\"|\"fp16\"|\"bf16\"|\"int32\"|\"int8\"\n",
      "  --layerOutputTypes=spec            Control per-layer output type constraints. Effective only when precisionConstraints is set to\n",
      "                                   \"obey\" or \"prefer\". (default = none\n",
      "                                   The specs are read left-to-right, and later ones override earlier ones. Each layer name can\n",
      "                                     contain at most one wildcard ('*') character. If a layer has more than\n",
      "                                   one output, then multiple types separated by \"+\" can be provided for this layer.\n",
      "                                   Per-layer output type spec ::= layerOutputTypes[\",\"spec]\n",
      "                                                         layerOutputTypes ::= layerName\":\"type\n",
      "                                                         type ::= \"fp32\"|\"fp16\"|\"bf16\"|\"int32\"|\"int8\"[\"+\"type]\n",
      "  --layerDeviceTypes=spec            Specify layer-specific device type.\n",
      "                                     The specs are read left-to-right, and later ones override earlier ones. If a layer does not have\n",
      "                                     a device type specified, the layer will opt for the default device type.\n",
      "                                   Per-layer device type spec ::= layerDeviceTypePair[\",\"spec]\n",
      "                                                         layerDeviceTypePair ::= layerName\":\"deviceType\n",
      "                                                           deviceType ::= \"GPU\"|\"DLA\"\n",
      "  --calib=<file>                     Read INT8 calibration cache file\n",
      "  --safe                             Enable build safety certified engine, if DLA is enable, --buildDLAStandalone will be specified\n",
      "                                     automatically (default = disabled)\n",
      "  --buildDLAStandalone               Enable build DLA standalone loadable which can be loaded by cuDLA, when this option is enabled, \n",
      "                                     --allowGPUFallback is disallowed and --skipInference is enabled by default. Additionally, \n",
      "                                     specifying --inputIOFormats and --outputIOFormats restricts I/O data type and memory layout\n",
      "                                     (default = disabled)\n",
      "  --allowGPUFallback                 When DLA is enabled, allow GPU fallback for unsupported layers (default = disabled)\n",
      "  --restricted                       Enable safety scope checking with kSAFETY_SCOPE build flag\n",
      "  --saveEngine=<file>                Save the serialized engine\n",
      "  --loadEngine=<file>                Load a serialized engine\n",
      "  --asyncFileReader=<file>           Load a serialized engine using async stream reader\n",
      "  --getPlanVersionOnly               Print TensorRT version when loaded plan was created. Works without deserialization of the plan.\n",
      "                                     Use together with --loadEngine. Supported only for engines created with 8.6 and forward.\n",
      "  --tacticSources=tactics            Specify the tactics to be used by adding (+) or removing (-) tactics from the default \n",
      "                                     tactic sources (default = all available tactics).\n",
      "                                     Note: Currently only cuDNN, cuBLAS, cuBLAS-LT, and edge mask convolutions are listed as optional\n",
      "                                           tactics.\n",
      "                                   Tactic Sources: tactics ::= [\",\"tactic]\n",
      "                                                     tactic  ::= (+|-)lib\n",
      "                                                   lib     ::= \"CUBLAS\"|\"CUBLAS_LT\"|\"CUDNN\"|\"EDGE_MASK_CONVOLUTIONS\"\n",
      "                                                               |\"JIT_CONVOLUTIONS\"\n",
      "                                     For example, to disable cudnn and enable cublas: --tacticSources=-CUDNN,+CUBLAS\n",
      "  --noBuilderCache                   Disable timing cache in builder (default is to enable timing cache)\n",
      "  --noCompilationCache               Disable Compilation cache in builder, and the cache is part of timing cache (default is to enable compilation cache)\n",
      "  --errorOnTimingCacheMiss           Emit error when a tactic being timed is not present in the timing cache (default = false)\n",
      "  --timingCacheFile=<file>           Save/load the serialized global timing cache\n",
      "  --preview=features                 Specify preview feature to be used by adding (+) or removing (-) preview features from the default\n",
      "                                   Preview Features: features ::= [\",\"feature]\n",
      "                                                       feature  ::= (+|-)flag\n",
      "                                                     flag     ::= \"aliasedPluginIO1003\"\n",
      "                                                                  |\"profileSharing0806\"\n",
      "  --builderOptimizationLevel         Set the builder optimization level. (default is 3)\n",
      "                                     Higher level allows TensorRT to spend more building time for more optimization options.\n",
      "                                     Valid values include integers from 0 to the maximum optimization level, which is currently 5.\n",
      "  --maxTactics                       Set the maximum number of tactics to time when there is a choice of tactics. (default is -1)\n",
      "                                     Larger number of tactics allow TensorRT to spend more building time on evaluating tactics.\n",
      "                                     Default value -1 means TensorRT can decide the number of tactics based on its own heuristic.\n",
      "  --hardwareCompatibilityLevel=mode  Make the engine file compatible with other GPU architectures. (default = none)\n",
      "                                   Hardware Compatibility Level: mode ::= \"none\" | \"ampere+\"\n",
      "                                         none = no compatibility\n",
      "                                         ampere+ = compatible with Ampere and newer GPUs\n",
      "  --runtimePlatform=platform         Set the target platform for runtime execution. (default = SameAsBuild)\n",
      "                                     When this option is enabled, --skipInference is enabled by default.\n",
      "                                   RuntimePlatfrom: platform ::= \"SameAsBuild\" | \"WindowsAMD64\"\n",
      "                                         SameAsBuild = no requirement for cross-platform compatibility.\n",
      "                                         WindowsAMD64 = set the target platform for engine execution as Windows AMD64 system\n",
      "  --tempdir=<dir>                    Overrides the default temporary directory TensorRT will use when creating temporary files.\n",
      "                                     See IRuntime::setTemporaryDirectory API documentation for more information.\n",
      "  --tempfileControls=controls        Controls what TensorRT is allowed to use when creating temporary executable files.\n",
      "                                     Should be a comma-separated list with entries in the format (in_memory|temporary):(allow|deny).\n",
      "                                     in_memory: Controls whether TensorRT is allowed to create temporary in-memory executable files.\n",
      "                                     temporary: Controls whether TensorRT is allowed to create temporary executable files in the\n",
      "                                                filesystem (in the directory given by --tempdir).\n",
      "                                     For example, to allow in-memory files and disallow temporary files:\n",
      "                                         --tempfileControls=in_memory:allow,temporary:deny\n",
      "                                     If a flag is unspecified, the default behavior is \"allow\".\n",
      "  --maxAuxStreams=N                  Set maximum number of auxiliary streams per inference stream that TRT is allowed to use to run \n",
      "                                     kernels in parallel if the network contains ops that can run in parallel, with the cost of more \n",
      "                                     memory usage. Set this to 0 for optimal memory usage. (default = using heuristics)\n",
      "  --profile                          Build with dynamic shapes using a profile with the min/max/opt shapes provided. Can be specified\n",
      "                                         multiple times to create multiple profiles with contiguous index.\n",
      "                                     (ex: --profile=0 --minShapes=<spec> --optShapes=<spec> --maxShapes=<spec> --profile=1 ...)\n",
      "  --calibProfile                     Select the optimization profile to calibrate by index. (default = 0)\n",
      "  --allowWeightStreaming             Enable a weight streaming engine. Must be specified with --stronglyTyped. TensorRT will disable\n",
      "                                     weight streaming at runtime unless --weightStreamingBudget is specified.\n",
      "  --markDebug                        Specify list of names of tensors to be marked as debug tensors. Separate names with a comma\n",
      "\n",
      "=== Inference Options ===\n",
      "  --shapes=spec               Set input shapes for dynamic shapes inference inputs.\n",
      "                              Note: Input names can be wrapped with escaped single quotes (ex: 'Input:0').\n",
      "                              Example input shapes spec: input0:1x3x256x256, input1:1x3x128x128\n",
      "                              For scalars (0-D shapes), use input0:scalar or simply input0: with nothing after the colon.\n",
      "                              Each input shape is supplied as a key-value pair where key is the input name and\n",
      "                              value is the dimensions (including the batch dimension) to be used for that input.\n",
      "                              Each key-value pair has the key and value separated using a colon (:).\n",
      "                              Multiple input shapes can be provided via comma-separated key-value pairs, and each input \n",
      "                              name can contain at most one wildcard ('*') character.\n",
      "  --loadInputs=spec           Load input values from files (default = generate random inputs). Input names can be wrapped with single quotes (ex: 'Input:0')\n",
      "                            Input values spec ::= Ival[\",\"spec]\n",
      "                                         Ival ::= name\":\"file\n",
      "                              Consult the README for more information on generating files for custom inputs.\n",
      "  --iterations=N              Run at least N inference iterations (default = 10)\n",
      "  --warmUp=N                  Run for N milliseconds to warmup before measuring performance (default = 200)\n",
      "  --duration=N                Run performance measurements for at least N seconds wallclock time (default = 3)\n",
      "                              If -1 is specified, inference will keep running unless stopped manually\n",
      "  --sleepTime=N               Delay inference start with a gap of N milliseconds between launch and compute (default = 0)\n",
      "  --idleTime=N                Sleep N milliseconds between two continuous iterations(default = 0)\n",
      "  --infStreams=N              Instantiate N execution contexts to run inference concurrently (default = 1)\n",
      "  --exposeDMA                 Serialize DMA transfers to and from device (default = disabled).\n",
      "  --noDataTransfers           Disable DMA transfers to and from device (default = enabled).\n",
      "  --useManagedMemory          Use managed memory instead of separate host and device allocations (default = disabled).\n",
      "  --useSpinWait               Actively synchronize on GPU events. This option may decrease synchronization time but increase CPU usage and power (default = disabled)\n",
      "  --threads                   Enable multithreading to drive engines with independent threads or speed up refitting (default = disabled) \n",
      "  --useCudaGraph              Use CUDA graph to capture engine execution and then launch inference (default = disabled).\n",
      "                              This flag may be ignored if the graph capture fails.\n",
      "  --timeDeserialize           Time the amount of time it takes to deserialize the network and exit.\n",
      "  --timeRefit                 Time the amount of time it takes to refit the engine before inference.\n",
      "  --separateProfileRun        Do not attach the profiler in the benchmark run; if profiling is enabled, a second profile run will be executed (default = disabled)\n",
      "  --skipInference             Exit after the engine has been built and skip inference perf measurement (default = disabled)\n",
      "  --persistentCacheRatio      Set the persistentCacheLimit in ratio, 0.5 represent half of max persistent L2 size (default = 0)\n",
      "  --useProfile                Set the optimization profile for the inference context (default = 0 ).\n",
      "  --allocationStrategy=spec   Specify how the internal device memory for inference is allocated.\n",
      "                            Strategy: spec ::= \"static\", \"profile\", \"runtime\"\n",
      "                                  static = Allocate device memory based on max size across all profiles.\n",
      "                                  profile = Allocate device memory based on max size of the current profile.\n",
      "                                  runtime = Allocate device memory based on the actual input shapes.\n",
      "  --saveDebugTensors          Specify list of names of tensors to turn on the debug state\n",
      "                              and filename to save raw outputs to.\n",
      "                              These tensors must be specified as debug tensors during build time.\n",
      "                            Input values spec ::= Ival[\",\"spec]\n",
      "                                         Ival ::= name\":\"file\n",
      "  --weightStreamingBudget     Set the maximum amount of GPU memory TensorRT is allowed to use for weights.\n",
      "                              It can take on the following values:\n",
      "                                -2: (default) Disable weight streaming at runtime.\n",
      "                                -1: TensorRT will automatically decide the budget.\n",
      "                                 0-100%: Percentage of streamable weights that reside on the GPU.\n",
      "                                         0% saves the most memory but will have the worst performance.\n",
      "                                         Requires the % character.\n",
      "                                >=0B: The exact amount of streamable weights that reside on the GPU. Supports the \n",
      "                                     following base-2 suffixes: B (Bytes), G (Gibibytes), K (Kibibytes), M (Mebibytes).\n",
      "\n",
      "=== Reporting Options ===\n",
      "  --verbose                   Use verbose logging (default = false)\n",
      "  --avgRuns=N                 Report performance measurements averaged over N consecutive iterations (default = 10)\n",
      "  --percentile=P1,P2,P3,...   Report performance for the P1,P2,P3,... percentages (0<=P_i<=100, 0 representing max perf, and 100 representing min perf; (default = 90,95,99%)\n",
      "  --dumpRefit                 Print the refittable layers and weights from a refittable engine\n",
      "  --dumpOutput                Print the output tensor(s) of the last inference iteration (default = disabled)\n",
      "  --dumpRawBindingsToFile     Print the input/output tensor(s) of the last inference iteration to file(default = disabled)\n",
      "  --dumpProfile               Print profile information per layer (default = disabled)\n",
      "  --dumpLayerInfo             Print layer information of the engine to console (default = disabled)\n",
      "  --dumpOptimizationProfile   Print the optimization profile(s) information (default = disabled)\n",
      "  --exportTimes=<file>        Write the timing results in a json file (default = disabled)\n",
      "  --exportOutput=<file>       Write the output tensors to a json file (default = disabled)\n",
      "  --exportProfile=<file>      Write the profile information per layer in a json file (default = disabled)\n",
      "  --exportLayerInfo=<file>    Write the layer information of the engine in a json file (default = disabled)\n",
      "\n",
      "=== System Options ===\n",
      "  --device=N                  Select cuda device N (default = 0)\n",
      "  --useDLACore=N              Select DLA core N for layers that support DLA (default = none)\n",
      "  --staticPlugins             Plugin library (.so) to load statically (can be specified multiple times)\n",
      "  --dynamicPlugins            Plugin library (.so) to load dynamically and may be serialized with the engine if they are included in --setPluginsToSerialize (can be specified multiple times)\n",
      "  --setPluginsToSerialize     Plugin library (.so) to be serialized with the engine (can be specified multiple times)\n",
      "  --ignoreParsedPluginLibs    By default, when building a version-compatible engine, plugin libraries specified by the ONNX parser \n",
      "                              are implicitly serialized with the engine (unless --excludeLeanRuntime is specified) and loaded dynamically. \n",
      "                              Enable this flag to ignore these plugin libraries instead.\n",
      "\n",
      "=== Help ===\n",
      "  --help, -h                  Print this message\n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v100700] [b23] # trtexec\n"
     ]
    }
   ],
   "source": [
    "!trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa6be50-aa31-4f4f-aa67-49846d410fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
