{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bf4d13-1fcb-4879-b7c6-cd2b901e81a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No custom ops found. If that's not correct, please make sure that the 'tensorrt' python is correctly installed and that the path to 'libcudnn*.so' is in PATH or LD_LIBRARY_PATH. If the custom op is not directly available as a plugin in TensorRT, please also make sure that the path to the compiled '.so' TensorRT plugin is also being given via the '--trt_plugins' flag (requires TRT 10+).\n",
      "INFO:root:Model onnx_models/fast_unet.onnx with opset_version 18 is loaded.\n",
      "INFO:root:Quantization Mode: int8\n",
      "INFO:root:Quantizable op types in the model: ['Conv']\n",
      "INFO:root:Building non-residual Add input map ...\n",
      "INFO:root:Searching for hard-coded patterns like MHA, LayerNorm, etc. to avoid quantization.\n",
      "INFO:root:Building KGEN/CASK targeted partitions ...\n",
      "INFO:root:Classifying the partition nodes ...\n",
      "INFO:root:Total number of nodes: 91\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensor data and making histogram ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:08<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal threshold for each tensor using 'entropy' algorithm ...\n",
      "Number of tensors : 27\n",
      "Number of histogram bins : 128 (The number may increase depends on the data it collects)\n",
      "Number of quantized bins : 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "INFO:root:Deleting QDQ nodes from marked inputs to make certain operations fusible ...\n",
      "INFO:root:Total number of quantized nodes: 33\n",
      "INFO:root:Quantized type counts: {'Conv': 27, 'Concat': 6}\n",
      "INFO:root:Quantized onnx model is saved as onnx_models/quant_fast_unet.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_node.op_type: Conv\n",
      "before: (14, 32, 1, 1, 1)\n",
      "after: (1, 1, 1, 32, 14)\n",
      "np_y_scale: (14,)\n",
      "next_node.op_type: Conv\n",
      "before: (320, 640, 3, 3, 3)\n",
      "after: (3, 3, 3, 640, 320)\n",
      "np_y_scale: (320,)\n",
      "next_node.op_type: Conv\n",
      "before: (320, 320, 3, 3, 3)\n",
      "after: (3, 3, 3, 320, 320)\n",
      "np_y_scale: (320,)\n",
      "next_node.op_type: Conv\n",
      "before: (320, 640, 3, 3, 3)\n",
      "after: (3, 3, 3, 640, 320)\n",
      "np_y_scale: (320,)\n",
      "next_node.op_type: Conv\n",
      "before: (320, 320, 3, 3, 3)\n",
      "after: (3, 3, 3, 320, 320)\n",
      "np_y_scale: (320,)\n",
      "next_node.op_type: Conv\n",
      "before: (256, 512, 3, 3, 3)\n",
      "after: (3, 3, 3, 512, 256)\n",
      "np_y_scale: (256,)\n",
      "next_node.op_type: Conv\n",
      "before: (256, 256, 3, 3, 3)\n",
      "after: (3, 3, 3, 256, 256)\n",
      "np_y_scale: (256,)\n",
      "next_node.op_type: Conv\n",
      "before: (128, 256, 3, 3, 3)\n",
      "after: (3, 3, 3, 256, 128)\n",
      "np_y_scale: (128,)\n",
      "next_node.op_type: Conv\n",
      "before: (128, 128, 3, 3, 3)\n",
      "after: (3, 3, 3, 128, 128)\n",
      "np_y_scale: (128,)\n",
      "next_node.op_type: Conv\n",
      "before: (64, 128, 3, 3, 3)\n",
      "after: (3, 3, 3, 128, 64)\n",
      "np_y_scale: (64,)\n",
      "next_node.op_type: Conv\n",
      "before: (64, 64, 3, 3, 3)\n",
      "after: (3, 3, 3, 64, 64)\n",
      "np_y_scale: (64,)\n",
      "next_node.op_type: Conv\n",
      "before: (32, 64, 1, 3, 3)\n",
      "after: (3, 3, 1, 64, 32)\n",
      "np_y_scale: (32,)\n",
      "next_node.op_type: Conv\n",
      "before: (32, 32, 1, 3, 3)\n",
      "after: (3, 3, 1, 32, 32)\n",
      "np_y_scale: (32,)\n",
      "next_node.op_type: Conv\n",
      "before: (32, 1, 1, 3, 3)\n",
      "after: (3, 3, 1, 1, 32)\n",
      "np_y_scale: (32,)\n",
      "next_node.op_type: Conv\n",
      "before: (32, 32, 1, 3, 3)\n",
      "after: (3, 3, 1, 32, 32)\n",
      "np_y_scale: (32,)\n",
      "next_node.op_type: Conv\n",
      "before: (64, 32, 3, 3, 3)\n",
      "after: (3, 3, 3, 32, 64)\n",
      "np_y_scale: (64,)\n",
      "next_node.op_type: Conv\n",
      "before: (64, 64, 3, 3, 3)\n",
      "after: (3, 3, 3, 64, 64)\n",
      "np_y_scale: (64,)\n",
      "next_node.op_type: Conv\n",
      "before: (128, 64, 3, 3, 3)\n",
      "after: (3, 3, 3, 64, 128)\n",
      "np_y_scale: (128,)\n",
      "next_node.op_type: Conv\n",
      "before: (128, 128, 3, 3, 3)\n",
      "after: (3, 3, 3, 128, 128)\n",
      "np_y_scale: (128,)\n",
      "next_node.op_type: Conv\n",
      "before: (256, 128, 3, 3, 3)\n",
      "after: (3, 3, 3, 128, 256)\n",
      "np_y_scale: (256,)\n",
      "next_node.op_type: Conv\n",
      "before: (256, 256, 3, 3, 3)\n",
      "after: (3, 3, 3, 256, 256)\n",
      "np_y_scale: (256,)\n",
      "next_node.op_type: Conv\n",
      "before: (320, 256, 3, 3, 3)\n",
      "after: (3, 3, 3, 256, 320)\n",
      "np_y_scale: (320,)\n",
      "next_node.op_type: Conv\n",
      "before: (320, 320, 3, 3, 3)\n",
      "after: (3, 3, 3, 320, 320)\n",
      "np_y_scale: (320,)\n",
      "next_node.op_type: Conv\n",
      "before: (320, 320, 3, 3, 3)\n",
      "after: (3, 3, 3, 320, 320)\n",
      "np_y_scale: (320,)\n",
      "next_node.op_type: Conv\n",
      "before: (320, 320, 3, 3, 3)\n",
      "after: (3, 3, 3, 320, 320)\n",
      "np_y_scale: (320,)\n",
      "next_node.op_type: Conv\n",
      "before: (320, 320, 3, 3, 3)\n",
      "after: (3, 3, 3, 320, 320)\n",
      "np_y_scale: (320,)\n",
      "next_node.op_type: Conv\n",
      "before: (320, 320, 3, 3, 3)\n",
      "after: (3, 3, 3, 320, 320)\n",
      "np_y_scale: (320,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Example numpy file for single-input ONNX\n",
    "calib_data = np.random.randn(1, 1, 64, 256, 256)\n",
    "calib_data = calib_data.astype(np.float32)\n",
    "np.save(\"calib_data.npy\", calib_data)\n",
    "\n",
    "# Example numpy file for single/multi-input ONNX\n",
    "# Dict key should match the input names of ONNX\n",
    "# calib_data = {\n",
    "#     \"input_name\": np.random.randn(*shape),\n",
    "#     \"input_name2\": np.random.randn(*shape2),\n",
    "# }\n",
    "# np.savez(\"/workspace/calib_data.npz\", calib_data)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, './TensorRT-Model-Optimizer')\n",
    "\n",
    "import modelopt.onnx.quantization as moq\n",
    "import numpy as np\n",
    "\n",
    "calibration_data_path = 'calib_data.npy'\n",
    "# onnx_path = \"vit_base_patch16_224.onnx\"\n",
    "#\n",
    "calibration_data = np.load(calibration_data_path)\n",
    "\n",
    "moq.quantize(\n",
    "    onnx_path=\"onnx_models/fast_unet.onnx\",\n",
    "    calibration_data=calibration_data,\n",
    "    output_path=\"onnx_models/quant_fast_unet.onnx\",\n",
    "    quantize_mode=\"int8\",\n",
    "    # high_precision_dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240c7daa-a977-451d-b116-84a3ee4e4540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v100800] [b43] # /usr/src/tensorrt/bin/trtexec --onnx=onnx_models/quant_fast_unet.onnx --saveEngine=onnx_models/quant_fast_unet.engine --best\n",
      "[02/28/2025-06:18:14] [I] === Model Options ===\n",
      "[02/28/2025-06:18:14] [I] Format: ONNX\n",
      "[02/28/2025-06:18:14] [I] Model: onnx_models/quant_fast_unet.onnx\n",
      "[02/28/2025-06:18:14] [I] Output:\n",
      "[02/28/2025-06:18:14] [I] === Build Options ===\n",
      "[02/28/2025-06:18:14] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default, tacticSharedMem: default\n",
      "[02/28/2025-06:18:14] [I] avgTiming: 8\n",
      "[02/28/2025-06:18:14] [I] Precision: FP32+FP16+BF16+INT8\n",
      "[02/28/2025-06:18:14] [I] LayerPrecisions: \n",
      "[02/28/2025-06:18:14] [I] Layer Device Types: \n",
      "[02/28/2025-06:18:14] [I] Calibration: Dynamic\n",
      "[02/28/2025-06:18:14] [I] Refit: Disabled\n",
      "[02/28/2025-06:18:14] [I] Strip weights: Disabled\n",
      "[02/28/2025-06:18:14] [I] Version Compatible: Disabled\n",
      "[02/28/2025-06:18:14] [I] ONNX Plugin InstanceNorm: Disabled\n",
      "[02/28/2025-06:18:14] [I] TensorRT runtime: full\n",
      "[02/28/2025-06:18:14] [I] Lean DLL Path: \n",
      "[02/28/2025-06:18:14] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[02/28/2025-06:18:14] [I] Exclude Lean Runtime: Disabled\n",
      "[02/28/2025-06:18:14] [I] Sparsity: Disabled\n",
      "[02/28/2025-06:18:14] [I] Safe mode: Disabled\n",
      "[02/28/2025-06:18:14] [I] Build DLA standalone loadable: Disabled\n",
      "[02/28/2025-06:18:14] [I] Allow GPU fallback for DLA: Disabled\n",
      "[02/28/2025-06:18:14] [I] DirectIO mode: Disabled\n",
      "[02/28/2025-06:18:14] [I] Restricted mode: Disabled\n",
      "[02/28/2025-06:18:14] [I] Skip inference: Disabled\n",
      "[02/28/2025-06:18:14] [I] Save engine: onnx_models/quant_fast_unet.engine\n",
      "[02/28/2025-06:18:14] [I] Load engine: \n",
      "[02/28/2025-06:18:14] [I] Profiling verbosity: 0\n",
      "[02/28/2025-06:18:14] [I] Tactic sources: Using default tactic sources\n",
      "[02/28/2025-06:18:14] [I] timingCacheMode: local\n",
      "[02/28/2025-06:18:14] [I] timingCacheFile: \n",
      "[02/28/2025-06:18:14] [I] Enable Compilation Cache: Enabled\n",
      "[02/28/2025-06:18:14] [I] Enable Monitor Memory: Disabled\n",
      "[02/28/2025-06:18:14] [I] errorOnTimingCacheMiss: Disabled\n",
      "[02/28/2025-06:18:14] [I] Preview Features: Use default preview flags.\n",
      "[02/28/2025-06:18:14] [I] MaxAuxStreams: -1\n",
      "[02/28/2025-06:18:14] [I] BuilderOptimizationLevel: -1\n",
      "[02/28/2025-06:18:14] [I] MaxTactics: -1\n",
      "[02/28/2025-06:18:14] [I] Calibration Profile Index: 0\n",
      "[02/28/2025-06:18:14] [I] Weight Streaming: Disabled\n",
      "[02/28/2025-06:18:14] [I] Runtime Platform: Same As Build\n",
      "[02/28/2025-06:18:14] [I] Debug Tensors: \n",
      "[02/28/2025-06:18:14] [I] Input(s)s format: fp32:CHW\n",
      "[02/28/2025-06:18:14] [I] Output(s)s format: fp32:CHW\n",
      "[02/28/2025-06:18:14] [I] Input build shapes: model\n",
      "[02/28/2025-06:18:14] [I] Input calibration shapes: model\n",
      "[02/28/2025-06:18:14] [I] === System Options ===\n",
      "[02/28/2025-06:18:14] [I] Device: 0\n",
      "[02/28/2025-06:18:14] [I] DLACore: \n",
      "[02/28/2025-06:18:14] [I] Plugins:\n",
      "[02/28/2025-06:18:14] [I] setPluginsToSerialize:\n",
      "[02/28/2025-06:18:14] [I] dynamicPlugins:\n",
      "[02/28/2025-06:18:14] [I] ignoreParsedPluginLibs: 0\n",
      "[02/28/2025-06:18:14] [I] \n",
      "[02/28/2025-06:18:14] [I] === Inference Options ===\n",
      "[02/28/2025-06:18:14] [I] Batch: Explicit\n",
      "[02/28/2025-06:18:14] [I] Input inference shapes: model\n",
      "[02/28/2025-06:18:14] [I] Iterations: 10\n",
      "[02/28/2025-06:18:14] [I] Duration: 3s (+ 200ms warm up)\n",
      "[02/28/2025-06:18:14] [I] Sleep time: 0ms\n",
      "[02/28/2025-06:18:14] [I] Idle time: 0ms\n",
      "[02/28/2025-06:18:14] [I] Inference Streams: 1\n",
      "[02/28/2025-06:18:14] [I] ExposeDMA: Disabled\n",
      "[02/28/2025-06:18:14] [I] Data transfers: Enabled\n",
      "[02/28/2025-06:18:14] [I] Spin-wait: Disabled\n",
      "[02/28/2025-06:18:14] [I] Multithreading: Disabled\n",
      "[02/28/2025-06:18:14] [I] CUDA Graph: Disabled\n",
      "[02/28/2025-06:18:14] [I] Separate profiling: Disabled\n",
      "[02/28/2025-06:18:14] [I] Time Deserialize: Disabled\n",
      "[02/28/2025-06:18:14] [I] Time Refit: Disabled\n",
      "[02/28/2025-06:18:14] [I] NVTX verbosity: 0\n",
      "[02/28/2025-06:18:14] [I] Persistent Cache Ratio: 0\n",
      "[02/28/2025-06:18:14] [I] Optimization Profile Index: 0\n",
      "[02/28/2025-06:18:14] [I] Weight Streaming Budget: 100.000000%\n",
      "[02/28/2025-06:18:14] [I] Inputs:\n",
      "[02/28/2025-06:18:14] [I] Debug Tensor Save Destinations:\n",
      "[02/28/2025-06:18:14] [I] === Reporting Options ===\n",
      "[02/28/2025-06:18:14] [I] Verbose: Disabled\n",
      "[02/28/2025-06:18:14] [I] Averages: 10 inferences\n",
      "[02/28/2025-06:18:14] [I] Percentiles: 90,95,99\n",
      "[02/28/2025-06:18:14] [I] Dump refittable layers:Disabled\n",
      "[02/28/2025-06:18:14] [I] Dump output: Disabled\n",
      "[02/28/2025-06:18:14] [I] Profile: Disabled\n",
      "[02/28/2025-06:18:14] [I] Export timing to JSON file: \n",
      "[02/28/2025-06:18:14] [I] Export output to JSON file: \n",
      "[02/28/2025-06:18:14] [I] Export profile to JSON file: \n",
      "[02/28/2025-06:18:14] [I] \n",
      "[02/28/2025-06:18:14] [I] === Device Information ===\n",
      "[02/28/2025-06:18:14] [I] Available Devices: \n",
      "[02/28/2025-06:18:14] [I]   Device 0: \"NVIDIA GeForce RTX 4060 Ti\" UUID: GPU-74930255-dea9-5b13-8a8e-40af3141e53b\n",
      "[02/28/2025-06:18:14] [I] Selected Device: NVIDIA GeForce RTX 4060 Ti\n",
      "[02/28/2025-06:18:14] [I] Selected Device ID: 0\n",
      "[02/28/2025-06:18:14] [I] Selected Device UUID: GPU-74930255-dea9-5b13-8a8e-40af3141e53b\n",
      "[02/28/2025-06:18:14] [I] Compute Capability: 8.9\n",
      "[02/28/2025-06:18:14] [I] SMs: 34\n",
      "[02/28/2025-06:18:14] [I] Device Global Memory: 15958 MiB\n",
      "[02/28/2025-06:18:14] [I] Shared Memory per SM: 100 KiB\n",
      "[02/28/2025-06:18:14] [I] Memory Bus Width: 128 bits (ECC disabled)\n",
      "[02/28/2025-06:18:14] [I] Application Compute Clock Rate: 2.565 GHz\n",
      "[02/28/2025-06:18:14] [I] Application Memory Clock Rate: 9.001 GHz\n",
      "[02/28/2025-06:18:14] [I] \n",
      "[02/28/2025-06:18:14] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[02/28/2025-06:18:14] [I] \n",
      "[02/28/2025-06:18:14] [I] TensorRT version: 10.8.0\n",
      "[02/28/2025-06:18:14] [I] Loading standard plugins\n",
      "[02/28/2025-06:18:14] [I] [TRT] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 24, GPU 258 (MiB)\n",
      "[02/28/2025-06:18:16] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +2771, GPU +446, now: CPU 2997, GPU 704 (MiB)\n",
      "[02/28/2025-06:18:16] [I] Start parsing network model.\n",
      "[02/28/2025-06:18:16] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/28/2025-06:18:16] [I] [TRT] Input filename:   onnx_models/quant_fast_unet.onnx\n",
      "[02/28/2025-06:18:16] [I] [TRT] ONNX IR version:  0.0.10\n",
      "[02/28/2025-06:18:16] [I] [TRT] Opset version:    1\n",
      "[02/28/2025-06:18:16] [I] [TRT] Producer name:    onnx.quantize\n",
      "[02/28/2025-06:18:16] [I] [TRT] Producer version: 0.1.0\n",
      "[02/28/2025-06:18:16] [I] [TRT] Domain:           \n",
      "[02/28/2025-06:18:16] [I] [TRT] Model version:    0\n",
      "[02/28/2025-06:18:16] [I] [TRT] Doc string:       \n",
      "[02/28/2025-06:18:16] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/28/2025-06:18:16] [I] Finished parsing network model. Parse time: 0.0370866\n",
      "[02/28/2025-06:18:16] [W] [TRT] Calibrator won't be used in explicit quantization mode. Please insert Quantize/Dequantize layers to indicate which tensors to quantize/dequantize.\n",
      "[02/28/2025-06:18:16] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[02/28/2025-06:18:22] [I] [TRT] Compiler backend is used during engine build.\n",
      "[02/28/2025-06:22:31] [I] [TRT] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[02/28/2025-06:22:31] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[02/28/2025-06:22:32] [I] [TRT] Total Host Persistent Memory: 184928 bytes\n",
      "[02/28/2025-06:22:32] [I] [TRT] Total Device Persistent Memory: 4798464 bytes\n",
      "[02/28/2025-06:22:32] [I] [TRT] Max Scratch Memory: 272696832 bytes\n",
      "[02/28/2025-06:22:32] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 193 steps to complete.\n",
      "[02/28/2025-06:22:32] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 14.2681ms to assign 11 blocks to 193 nodes requiring 1258660352 bytes.\n",
      "[02/28/2025-06:22:32] [I] [TRT] Total Activation Memory: 1258659840 bytes\n",
      "[02/28/2025-06:22:32] [I] [TRT] Total Weights Memory: 46877828 bytes\n",
      "[02/28/2025-06:22:32] [I] [TRT] Compiler backend is used during engine execution.\n",
      "[02/28/2025-06:22:32] [I] [TRT] Engine generation completed in 256.166 seconds.\n",
      "[02/28/2025-06:22:32] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 2049 MiB\n",
      "[02/28/2025-06:22:32] [I] Engine built in 256.244 sec.\n",
      "[02/28/2025-06:22:32] [I] Created engine with size: 48.9171 MiB\n",
      "[02/28/2025-06:22:33] [I] [TRT] Loaded engine size: 48 MiB\n",
      "[02/28/2025-06:22:33] [I] Engine deserialized in 0.0317315 sec.\n",
      "[02/28/2025-06:22:33] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1205, now: CPU 0, GPU 1254 (MiB)\n",
      "[02/28/2025-06:22:33] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[02/28/2025-06:22:33] [I] Created execution context with device memory size: 1200.35 MiB\n",
      "[02/28/2025-06:22:33] [I] Using random values for input l_x_\n",
      "[02/28/2025-06:22:33] [I] Input binding for l_x_ with dimensions 1x1x64x256x256 is created.\n",
      "[02/28/2025-06:22:33] [I] Output binding for decoder_1 with dimensions 1x14x64x256x256 is created.\n",
      "[02/28/2025-06:22:33] [I] Starting inference\n",
      "[02/28/2025-06:22:36] [I] Warmup completed 3 queries over 200 ms\n",
      "[02/28/2025-06:22:36] [I] Timing trace has 32 queries over 3.20398 s\n",
      "[02/28/2025-06:22:36] [I] \n",
      "[02/28/2025-06:22:36] [I] === Trace details ===\n",
      "[02/28/2025-06:22:36] [I] Trace averages of 10 runs:\n",
      "[02/28/2025-06:22:36] [I] Average on 10 runs - GPU latency: 97.1207 ms - Host latency: 116.56 ms (enqueue 4.15608 ms)\n",
      "[02/28/2025-06:22:36] [I] Average on 10 runs - GPU latency: 97.1017 ms - Host latency: 116.512 ms (enqueue 4.43529 ms)\n",
      "[02/28/2025-06:22:36] [I] Average on 10 runs - GPU latency: 97.1013 ms - Host latency: 116.546 ms (enqueue 4.84692 ms)\n",
      "[02/28/2025-06:22:36] [I] \n",
      "[02/28/2025-06:22:36] [I] === Performance summary ===\n",
      "[02/28/2025-06:22:36] [I] Throughput: 9.98759 qps\n",
      "[02/28/2025-06:22:36] [I] Latency: min = 116.344 ms, max = 116.699 ms, mean = 116.53 ms, median = 116.525 ms, percentile(90%) = 116.62 ms, percentile(95%) = 116.657 ms, percentile(99%) = 116.699 ms\n",
      "[02/28/2025-06:22:36] [I] Enqueue Time: min = 1.81836 ms, max = 5.0896 ms, mean = 4.51232 ms, median = 4.79858 ms, percentile(90%) = 4.98853 ms, percentile(95%) = 5.08496 ms, percentile(99%) = 5.0896 ms\n",
      "[02/28/2025-06:22:36] [I] H2D Latency: min = 1.33965 ms, max = 1.38196 ms, mean = 1.35217 ms, median = 1.35263 ms, percentile(90%) = 1.35547 ms, percentile(95%) = 1.35669 ms, percentile(99%) = 1.38196 ms\n",
      "[02/28/2025-06:22:36] [I] GPU Compute Time: min = 97.0242 ms, max = 97.2351 ms, mean = 97.1046 ms, median = 97.1152 ms, percentile(90%) = 97.152 ms, percentile(95%) = 97.1592 ms, percentile(99%) = 97.2351 ms\n",
      "[02/28/2025-06:22:36] [I] D2H Latency: min = 17.9084 ms, max = 18.1857 ms, mean = 18.0728 ms, median = 18.0801 ms, percentile(90%) = 18.1158 ms, percentile(95%) = 18.1284 ms, percentile(99%) = 18.1857 ms\n",
      "[02/28/2025-06:22:36] [I] Total Host Walltime: 3.20398 s\n",
      "[02/28/2025-06:22:36] [I] Total GPU Compute Time: 3.10735 s\n",
      "[02/28/2025-06:22:36] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[02/28/2025-06:22:36] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v100800] [b43] # /usr/src/tensorrt/bin/trtexec --onnx=onnx_models/quant_fast_unet.onnx --saveEngine=onnx_models/quant_fast_unet.engine --best\n"
     ]
    }
   ],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec --onnx=onnx_models/quant_fast_unet.onnx --saveEngine=onnx_models/quant_fast_unet.engine --best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07576897-64f9-477f-9f1d-be2c9cbdca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v100800] [b43] # /usr/src/tensorrt/bin/trtexec --onnx=onnx_models/quant_fast_unet.onnx --saveEngine=onnx_models/quant_fast_unet_fp16.engine --fp16\n",
      "[02/28/2025-03:40:51] [I] === Model Options ===\n",
      "[02/28/2025-03:40:51] [I] Format: ONNX\n",
      "[02/28/2025-03:40:51] [I] Model: onnx_models/quant_fast_unet.onnx\n",
      "[02/28/2025-03:40:51] [I] Output:\n",
      "[02/28/2025-03:40:51] [I] === Build Options ===\n",
      "[02/28/2025-03:40:51] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default, tacticSharedMem: default\n",
      "[02/28/2025-03:40:51] [I] avgTiming: 8\n",
      "[02/28/2025-03:40:51] [I] Precision: FP32+FP16\n",
      "[02/28/2025-03:40:51] [I] LayerPrecisions: \n",
      "[02/28/2025-03:40:51] [I] Layer Device Types: \n",
      "[02/28/2025-03:40:51] [I] Calibration: \n",
      "[02/28/2025-03:40:51] [I] Refit: Disabled\n",
      "[02/28/2025-03:40:51] [I] Strip weights: Disabled\n",
      "[02/28/2025-03:40:51] [I] Version Compatible: Disabled\n",
      "[02/28/2025-03:40:51] [I] ONNX Plugin InstanceNorm: Disabled\n",
      "[02/28/2025-03:40:51] [I] TensorRT runtime: full\n",
      "[02/28/2025-03:40:51] [I] Lean DLL Path: \n",
      "[02/28/2025-03:40:51] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[02/28/2025-03:40:51] [I] Exclude Lean Runtime: Disabled\n",
      "[02/28/2025-03:40:51] [I] Sparsity: Disabled\n",
      "[02/28/2025-03:40:51] [I] Safe mode: Disabled\n",
      "[02/28/2025-03:40:51] [I] Build DLA standalone loadable: Disabled\n",
      "[02/28/2025-03:40:51] [I] Allow GPU fallback for DLA: Disabled\n",
      "[02/28/2025-03:40:51] [I] DirectIO mode: Disabled\n",
      "[02/28/2025-03:40:51] [I] Restricted mode: Disabled\n",
      "[02/28/2025-03:40:51] [I] Skip inference: Disabled\n",
      "[02/28/2025-03:40:51] [I] Save engine: onnx_models/quant_fast_unet_fp16.engine\n",
      "[02/28/2025-03:40:51] [I] Load engine: \n",
      "[02/28/2025-03:40:51] [I] Profiling verbosity: 0\n",
      "[02/28/2025-03:40:51] [I] Tactic sources: Using default tactic sources\n",
      "[02/28/2025-03:40:51] [I] timingCacheMode: local\n",
      "[02/28/2025-03:40:51] [I] timingCacheFile: \n",
      "[02/28/2025-03:40:51] [I] Enable Compilation Cache: Enabled\n",
      "[02/28/2025-03:40:51] [I] Enable Monitor Memory: Disabled\n",
      "[02/28/2025-03:40:51] [I] errorOnTimingCacheMiss: Disabled\n",
      "[02/28/2025-03:40:51] [I] Preview Features: Use default preview flags.\n",
      "[02/28/2025-03:40:51] [I] MaxAuxStreams: -1\n",
      "[02/28/2025-03:40:51] [I] BuilderOptimizationLevel: -1\n",
      "[02/28/2025-03:40:51] [I] MaxTactics: -1\n",
      "[02/28/2025-03:40:51] [I] Calibration Profile Index: 0\n",
      "[02/28/2025-03:40:51] [I] Weight Streaming: Disabled\n",
      "[02/28/2025-03:40:51] [I] Runtime Platform: Same As Build\n",
      "[02/28/2025-03:40:51] [I] Debug Tensors: \n",
      "[02/28/2025-03:40:51] [I] Input(s)s format: fp32:CHW\n",
      "[02/28/2025-03:40:51] [I] Output(s)s format: fp32:CHW\n",
      "[02/28/2025-03:40:51] [I] Input build shapes: model\n",
      "[02/28/2025-03:40:51] [I] Input calibration shapes: model\n",
      "[02/28/2025-03:40:51] [I] === System Options ===\n",
      "[02/28/2025-03:40:51] [I] Device: 0\n",
      "[02/28/2025-03:40:51] [I] DLACore: \n",
      "[02/28/2025-03:40:51] [I] Plugins:\n",
      "[02/28/2025-03:40:51] [I] setPluginsToSerialize:\n",
      "[02/28/2025-03:40:51] [I] dynamicPlugins:\n",
      "[02/28/2025-03:40:51] [I] ignoreParsedPluginLibs: 0\n",
      "[02/28/2025-03:40:51] [I] \n",
      "[02/28/2025-03:40:51] [I] === Inference Options ===\n",
      "[02/28/2025-03:40:51] [I] Batch: Explicit\n",
      "[02/28/2025-03:40:51] [I] Input inference shapes: model\n",
      "[02/28/2025-03:40:51] [I] Iterations: 10\n",
      "[02/28/2025-03:40:51] [I] Duration: 3s (+ 200ms warm up)\n",
      "[02/28/2025-03:40:51] [I] Sleep time: 0ms\n",
      "[02/28/2025-03:40:51] [I] Idle time: 0ms\n",
      "[02/28/2025-03:40:51] [I] Inference Streams: 1\n",
      "[02/28/2025-03:40:51] [I] ExposeDMA: Disabled\n",
      "[02/28/2025-03:40:51] [I] Data transfers: Enabled\n",
      "[02/28/2025-03:40:51] [I] Spin-wait: Disabled\n",
      "[02/28/2025-03:40:51] [I] Multithreading: Disabled\n",
      "[02/28/2025-03:40:51] [I] CUDA Graph: Disabled\n",
      "[02/28/2025-03:40:51] [I] Separate profiling: Disabled\n",
      "[02/28/2025-03:40:51] [I] Time Deserialize: Disabled\n",
      "[02/28/2025-03:40:51] [I] Time Refit: Disabled\n",
      "[02/28/2025-03:40:51] [I] NVTX verbosity: 0\n",
      "[02/28/2025-03:40:51] [I] Persistent Cache Ratio: 0\n",
      "[02/28/2025-03:40:51] [I] Optimization Profile Index: 0\n",
      "[02/28/2025-03:40:51] [I] Weight Streaming Budget: 100.000000%\n",
      "[02/28/2025-03:40:51] [I] Inputs:\n",
      "[02/28/2025-03:40:51] [I] Debug Tensor Save Destinations:\n",
      "[02/28/2025-03:40:51] [I] === Reporting Options ===\n",
      "[02/28/2025-03:40:51] [I] Verbose: Disabled\n",
      "[02/28/2025-03:40:51] [I] Averages: 10 inferences\n",
      "[02/28/2025-03:40:51] [I] Percentiles: 90,95,99\n",
      "[02/28/2025-03:40:51] [I] Dump refittable layers:Disabled\n",
      "[02/28/2025-03:40:51] [I] Dump output: Disabled\n",
      "[02/28/2025-03:40:51] [I] Profile: Disabled\n",
      "[02/28/2025-03:40:51] [I] Export timing to JSON file: \n",
      "[02/28/2025-03:40:51] [I] Export output to JSON file: \n",
      "[02/28/2025-03:40:51] [I] Export profile to JSON file: \n",
      "[02/28/2025-03:40:51] [I] \n",
      "[02/28/2025-03:40:51] [I] === Device Information ===\n",
      "[02/28/2025-03:40:51] [I] Available Devices: \n",
      "[02/28/2025-03:40:51] [I]   Device 0: \"NVIDIA GeForce RTX 4060 Ti\" UUID: GPU-74930255-dea9-5b13-8a8e-40af3141e53b\n",
      "[02/28/2025-03:40:51] [I] Selected Device: NVIDIA GeForce RTX 4060 Ti\n",
      "[02/28/2025-03:40:51] [I] Selected Device ID: 0\n",
      "[02/28/2025-03:40:51] [I] Selected Device UUID: GPU-74930255-dea9-5b13-8a8e-40af3141e53b\n",
      "[02/28/2025-03:40:51] [I] Compute Capability: 8.9\n",
      "[02/28/2025-03:40:51] [I] SMs: 34\n",
      "[02/28/2025-03:40:51] [I] Device Global Memory: 15958 MiB\n",
      "[02/28/2025-03:40:51] [I] Shared Memory per SM: 100 KiB\n",
      "[02/28/2025-03:40:51] [I] Memory Bus Width: 128 bits (ECC disabled)\n",
      "[02/28/2025-03:40:51] [I] Application Compute Clock Rate: 2.565 GHz\n",
      "[02/28/2025-03:40:51] [I] Application Memory Clock Rate: 9.001 GHz\n",
      "[02/28/2025-03:40:51] [I] \n",
      "[02/28/2025-03:40:51] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[02/28/2025-03:40:51] [I] \n",
      "[02/28/2025-03:40:51] [I] TensorRT version: 10.8.0\n",
      "[02/28/2025-03:40:51] [I] Loading standard plugins\n",
      "[02/28/2025-03:40:51] [I] [TRT] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 24, GPU 127 (MiB)\n",
      "[02/28/2025-03:40:53] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +2771, GPU +446, now: CPU 2997, GPU 573 (MiB)\n",
      "[02/28/2025-03:40:53] [I] Start parsing network model.\n",
      "[02/28/2025-03:40:53] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/28/2025-03:40:53] [I] [TRT] Input filename:   onnx_models/quant_fast_unet.onnx\n",
      "[02/28/2025-03:40:53] [I] [TRT] ONNX IR version:  0.0.10\n",
      "[02/28/2025-03:40:53] [I] [TRT] Opset version:    1\n",
      "[02/28/2025-03:40:53] [I] [TRT] Producer name:    onnx.quantize\n",
      "[02/28/2025-03:40:53] [I] [TRT] Producer version: 0.1.0\n",
      "[02/28/2025-03:40:53] [I] [TRT] Domain:           \n",
      "[02/28/2025-03:40:53] [I] [TRT] Model version:    0\n",
      "[02/28/2025-03:40:53] [I] [TRT] Doc string:       \n",
      "[02/28/2025-03:40:53] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/28/2025-03:40:53] [I] Finished parsing network model. Parse time: 0.0361565\n",
      "[02/28/2025-03:40:53] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[02/28/2025-03:41:00] [I] [TRT] Compiler backend is used during engine build.\n",
      "[02/28/2025-03:45:00] [I] [TRT] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[02/28/2025-03:45:00] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[02/28/2025-03:45:01] [I] [TRT] Total Host Persistent Memory: 183488 bytes\n",
      "[02/28/2025-03:45:01] [I] [TRT] Total Device Persistent Memory: 4798464 bytes\n",
      "[02/28/2025-03:45:01] [I] [TRT] Max Scratch Memory: 272696832 bytes\n",
      "[02/28/2025-03:45:01] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 187 steps to complete.\n",
      "[02/28/2025-03:45:01] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 15.2957ms to assign 10 blocks to 187 nodes requiring 1258619392 bytes.\n",
      "[02/28/2025-03:45:01] [I] [TRT] Total Activation Memory: 1258618880 bytes\n",
      "[02/28/2025-03:45:01] [I] [TRT] Total Weights Memory: 46871168 bytes\n",
      "[02/28/2025-03:45:01] [I] [TRT] Compiler backend is used during engine execution.\n",
      "[02/28/2025-03:45:01] [I] [TRT] Engine generation completed in 247.854 seconds.\n",
      "[02/28/2025-03:45:01] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 2049 MiB\n",
      "[02/28/2025-03:45:01] [I] Engine built in 247.948 sec.\n",
      "[02/28/2025-03:45:01] [I] Created engine with size: 49.0454 MiB\n",
      "[02/28/2025-03:45:02] [I] [TRT] Loaded engine size: 49 MiB\n",
      "[02/28/2025-03:45:02] [I] Engine deserialized in 0.0351817 sec.\n",
      "[02/28/2025-03:45:02] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1205, now: CPU 0, GPU 1254 (MiB)\n",
      "[02/28/2025-03:45:02] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[02/28/2025-03:45:02] [I] Created execution context with device memory size: 1200.31 MiB\n",
      "[02/28/2025-03:45:02] [I] Using random values for input l_x_\n",
      "[02/28/2025-03:45:02] [I] Input binding for l_x_ with dimensions 1x1x64x256x256 is created.\n",
      "[02/28/2025-03:45:02] [I] Output binding for decoder_1 with dimensions 1x14x64x256x256 is created.\n",
      "[02/28/2025-03:45:02] [I] Starting inference\n",
      "[02/28/2025-03:45:05] [I] Warmup completed 3 queries over 200 ms\n",
      "[02/28/2025-03:45:05] [I] Timing trace has 32 queries over 3.20594 s\n",
      "[02/28/2025-03:45:05] [I] \n",
      "[02/28/2025-03:45:05] [I] === Trace details ===\n",
      "[02/28/2025-03:45:05] [I] Trace averages of 10 runs:\n",
      "[02/28/2025-03:45:05] [I] Average on 10 runs - GPU latency: 97.1531 ms - Host latency: 116.587 ms (enqueue 3.30611 ms)\n",
      "[02/28/2025-03:45:05] [I] Average on 10 runs - GPU latency: 97.1675 ms - Host latency: 116.629 ms (enqueue 4.87628 ms)\n",
      "[02/28/2025-03:45:05] [I] Average on 10 runs - GPU latency: 97.1659 ms - Host latency: 116.637 ms (enqueue 4.7396 ms)\n",
      "[02/28/2025-03:45:05] [I] \n",
      "[02/28/2025-03:45:05] [I] === Performance summary ===\n",
      "[02/28/2025-03:45:05] [I] Throughput: 9.98149 qps\n",
      "[02/28/2025-03:45:05] [I] Latency: min = 116.383 ms, max = 116.862 ms, mean = 116.61 ms, median = 116.609 ms, percentile(90%) = 116.707 ms, percentile(95%) = 116.768 ms, percentile(99%) = 116.862 ms\n",
      "[02/28/2025-03:45:05] [I] Enqueue Time: min = 1.21045 ms, max = 5.22131 ms, mean = 4.19313 ms, median = 4.74084 ms, percentile(90%) = 5.05322 ms, percentile(95%) = 5.18677 ms, percentile(99%) = 5.22131 ms\n",
      "[02/28/2025-03:45:05] [I] H2D Latency: min = 1.33969 ms, max = 1.38916 ms, mean = 1.35664 ms, median = 1.35406 ms, percentile(90%) = 1.37195 ms, percentile(95%) = 1.37646 ms, percentile(99%) = 1.38916 ms\n",
      "[02/28/2025-03:45:05] [I] GPU Compute Time: min = 97.0415 ms, max = 97.239 ms, mean = 97.1592 ms, median = 97.1683 ms, percentile(90%) = 97.2083 ms, percentile(95%) = 97.2237 ms, percentile(99%) = 97.239 ms\n",
      "[02/28/2025-03:45:05] [I] D2H Latency: min = 17.9058 ms, max = 18.311 ms, mean = 18.0937 ms, median = 18.0917 ms, percentile(90%) = 18.158 ms, percentile(95%) = 18.2423 ms, percentile(99%) = 18.311 ms\n",
      "[02/28/2025-03:45:05] [I] Total Host Walltime: 3.20594 s\n",
      "[02/28/2025-03:45:05] [I] Total GPU Compute Time: 3.10909 s\n",
      "[02/28/2025-03:45:05] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[02/28/2025-03:45:05] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v100800] [b43] # /usr/src/tensorrt/bin/trtexec --onnx=onnx_models/quant_fast_unet.onnx --saveEngine=onnx_models/quant_fast_unet_fp16.engine --fp16\n"
     ]
    }
   ],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec --onnx=onnx_models/quant_fast_unet.onnx --saveEngine=onnx_models/quant_fast_unet_fp16.engine --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0bf36f-f6a8-42af-b7fc-cba01265f9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libnvinfer-bin libnvinfer-dev libnvinfer-dispatch-dev libnvinfer-dispatch10\n",
      "  libnvinfer-headers-dev libnvinfer-headers-plugin-dev libnvinfer-lean-dev\n",
      "  libnvinfer-lean10 libnvinfer-plugin-dev libnvinfer-plugin10\n",
      "  libnvinfer-samples libnvinfer-vc-plugin-dev libnvinfer-vc-plugin10\n",
      "  libnvinfer10 libnvonnxparsers-dev libnvonnxparsers10 python3-libnvinfer\n",
      "  python3-libnvinfer-dev python3-libnvinfer-dispatch python3-libnvinfer-lean\n",
      "The following NEW packages will be installed:\n",
      "  libnvinfer-bin libnvinfer-dev libnvinfer-dispatch-dev libnvinfer-dispatch10\n",
      "  libnvinfer-headers-dev libnvinfer-headers-plugin-dev libnvinfer-lean-dev\n",
      "  libnvinfer-lean10 libnvinfer-plugin-dev libnvinfer-plugin10\n",
      "  libnvinfer-samples libnvinfer-vc-plugin-dev libnvinfer-vc-plugin10\n",
      "  libnvinfer10 libnvonnxparsers-dev libnvonnxparsers10 python3-libnvinfer\n",
      "  python3-libnvinfer-dev python3-libnvinfer-dispatch python3-libnvinfer-lean\n",
      "  tensorrt\n",
      "0 upgraded, 21 newly installed, 0 to remove and 49 not upgraded.\n",
      "Need to get 4454 MB of archives.\n",
      "After this operation, 11.4 GB of additional disk space will be used.\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer10 10.8.0.43-1+cuda12.8 [1966 MB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-lean10 10.8.0.43-1+cuda12.8 [15.5 MB]\n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-plugin10 10.8.0.43-1+cuda12.8 [13.7 MB]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-vc-plugin10 10.8.0.43-1+cuda12.8 [233 kB]\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dispatch10 10.8.0.43-1+cuda12.8 [216 kB]\n",
      "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvonnxparsers10 10.8.0.43-1+cuda12.8 [1383 kB]\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-bin 10.8.0.43-1+cuda12.8 [458 kB]\n",
      "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-headers-dev 10.8.0.43-1+cuda12.8 [109 kB]\n",
      "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dev 10.8.0.43-1+cuda12.8 [2019 MB]\n",
      "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dispatch-dev 10.8.0.43-1+cuda12.8 [123 kB]\n",
      "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-headers-plugin-dev 10.8.0.43-1+cuda12.8 [6056 B]\n",
      "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-lean-dev 10.8.0.43-1+cuda12.8 [230 MB]\n",
      "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-plugin-dev 10.8.0.43-1+cuda12.8 [15.7 MB]\n",
      "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-vc-plugin-dev 10.8.0.43-1+cuda12.8 [97.5 kB]\n",
      "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvonnxparsers-dev 10.8.0.43-1+cuda12.8 [2153 kB]\n",
      "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-samples 10.8.0.43-1+cuda12.8 [187 MB]\n",
      "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer 10.8.0.43-1+cuda12.8 [761 kB]\n",
      "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer-lean 10.8.0.43-1+cuda12.8 [484 kB]\n",
      "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer-dispatch 10.8.0.43-1+cuda12.8 [485 kB]\n",
      "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  python3-libnvinfer-dev 10.8.0.43-1+cuda12.8 [2958 B]\n",
      "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  tensorrt 10.8.0.43-1+cuda12.8 [2946 B]\n",
      "Fetched 4454 MB in 2min 46s (26.8 MB/s)                                        \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libnvinfer10.\n",
      "(Reading database ... 40256 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libnvinfer10_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer10 (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-lean10.\n",
      "Preparing to unpack .../01-libnvinfer-lean10_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-lean10 (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-plugin10.\n",
      "Preparing to unpack .../02-libnvinfer-plugin10_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-plugin10 (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-vc-plugin10.\n",
      "Preparing to unpack .../03-libnvinfer-vc-plugin10_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-vc-plugin10 (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-dispatch10.\n",
      "Preparing to unpack .../04-libnvinfer-dispatch10_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-dispatch10 (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvonnxparsers10.\n",
      "Preparing to unpack .../05-libnvonnxparsers10_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvonnxparsers10 (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-bin.\n",
      "Preparing to unpack .../06-libnvinfer-bin_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-bin (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-headers-dev.\n",
      "Preparing to unpack .../07-libnvinfer-headers-dev_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-headers-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-dev.\n",
      "Preparing to unpack .../08-libnvinfer-dev_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-dispatch-dev.\n",
      "Preparing to unpack .../09-libnvinfer-dispatch-dev_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-dispatch-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-headers-plugin-dev.\n",
      "Preparing to unpack .../10-libnvinfer-headers-plugin-dev_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-headers-plugin-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-lean-dev.\n",
      "Preparing to unpack .../11-libnvinfer-lean-dev_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-lean-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-plugin-dev.\n",
      "Preparing to unpack .../12-libnvinfer-plugin-dev_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-plugin-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-vc-plugin-dev.\n",
      "Preparing to unpack .../13-libnvinfer-vc-plugin-dev_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-vc-plugin-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvonnxparsers-dev.\n",
      "Preparing to unpack .../14-libnvonnxparsers-dev_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvonnxparsers-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-samples.\n",
      "Preparing to unpack .../15-libnvinfer-samples_10.8.0.43-1+cuda12.8_all.deb ...\n",
      "Unpacking libnvinfer-samples (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer.\n",
      "Preparing to unpack .../16-python3-libnvinfer_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer-lean.\n",
      "Preparing to unpack .../17-python3-libnvinfer-lean_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer-lean (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer-dispatch.\n",
      "Preparing to unpack .../18-python3-libnvinfer-dispatch_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer-dispatch (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer-dev.\n",
      "Preparing to unpack .../19-python3-libnvinfer-dev_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Selecting previously unselected package tensorrt.\n",
      "Preparing to unpack .../20-tensorrt_10.8.0.43-1+cuda12.8_amd64.deb ...\n",
      "Unpacking tensorrt (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-headers-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer10 (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-plugin10 (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-vc-plugin10 (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvonnxparsers10 (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-dispatch10 (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-dispatch-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-lean10 (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvonnxparsers-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer-dispatch (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-headers-plugin-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-lean-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer-lean (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-plugin-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-vc-plugin-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-bin (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up libnvinfer-samples (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer-dev (10.8.0.43-1+cuda12.8) ...\n",
      "Setting up tensorrt (10.8.0.43-1+cuda12.8) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get -y install tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732168b2-c858-4f18-8baf-6eefb666ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Package tensorrt is not available, but is referred to by another package.\n",
      "This may mean that the package is missing, has been obsoleted, or\n",
      "is only available from another source\n",
      "\n",
      "E: Version '10.0.1' for 'tensorrt' was not found\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get -y install tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7cd37-65ec-4cb1-b245-62e0f8098e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get -y install cudnn9-cuda-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ed8153-4b2c-448b-ae48-1710aa202a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycuda\n",
      "  Downloading pycuda-2025.1.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?done\n",
      "  Getting requirements to build wheeldone\n",
      "doneproject.toml) ... \u001b[?25l\n",
      "Requirement already satisfied: mako in /venv/main/lib/python3.10/site-packages (from pycuda) (1.3.9)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /venv/main/lib/python3.10/site-packages (from pycuda) (4.3.6)\n",
      "Collecting pytools>=2011.2\n",
      "  Downloading pytools-2025.1.1-py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: typing-extensions>=4.5 in /venv/main/lib/python3.10/site-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /venv/main/lib/python3.10/site-packages (from mako->pycuda) (3.0.2)\n",
      "Building wheels for collected packages: pycuda\n",
      "done\n",
      "\u001b[?25h  Created wheel for pycuda: filename=pycuda-2025.1-cp310-cp310-linux_x86_64.whl size=660661 sha256=46f5dd78dc33628a5975d331b9e4b6962fabd563de5e117ce14c5e45f6d148f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/65/53/5f/f5f184c26b7cc503acb77f3456531a6e1fac0ce30c774b9d82\n",
      "Successfully built pycuda\n",
      "Installing collected packages: pytools, pycuda\n",
      "Successfully installed pycuda-2025.1 pytools-2025.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3caf50-4fb3-43f7-80c9-d669c80b935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "f = open(\"onnx_models/quant_fast_unet_fp16.engine\", \"rb\")\n",
    "runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) \n",
    "\n",
    "engine = runtime.deserialize_cuda_engine(f.read())\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe8e394-6df8-4ea5-acee-06d1f395dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "BATCH_SIZE = 1\n",
    "target_dtype = np.float32\n",
    "# need to set input and output precisions to FP16 to fully enable it\n",
    "output = np.empty([1, 14, 64, 256, 256], dtype = target_dtype) \n",
    "\n",
    "input_batch = np.array([1, 1, 64, 256, 256], dtype=target_dtype)\n",
    "\n",
    "# allocate device memory\n",
    "d_input = cuda.mem_alloc(1 * input_batch.nbytes)\n",
    "d_output = cuda.mem_alloc(1 * output.nbytes)\n",
    "\n",
    "tensor_names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "assert(len(tensor_names) == 2)\n",
    "\n",
    "context.set_tensor_address(tensor_names[0], int(d_input))\n",
    "context.set_tensor_address(tensor_names[1], int(d_output))\n",
    "\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "\n",
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66401a4b-bb59-459d-80a7-fd576bf59bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(batch): # result gets copied into output\n",
    "    # transfer input data to device\n",
    "    cuda.memcpy_htod_async(d_input, batch, stream)\n",
    "    # execute model\n",
    "    # start=time.time()\n",
    "    # for i in range(100):\n",
    "    context.execute_async_v3(stream.handle)\n",
    "    # print(time.time() - start)\n",
    "    # transfer predictions back\n",
    "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "    # syncronize threads\n",
    "    # stream.synchronize()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c060489-2ef2-4ec8-824a-d7f2dc9286b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/28/2025-06:15:20] [TRT] [E] IExecutionContext::enqueueV3: Error Code 1: Cuda Runtime (an illegal memory access was encountered)\n"
     ]
    },
    {
     "ename": "LogicError",
     "evalue": "cuMemcpyDtoHAsync failed: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      7\u001b[0m context\u001b[38;5;241m.\u001b[39mexecute_async_v3(stream\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(time.time() - start)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# transfer predictions back\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemcpy_dtoh_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# syncronize threads\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# stream.synchronize()\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mLogicError\u001b[0m: cuMemcpyDtoHAsync failed: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "pred = predict(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6b6b7-7116-4028-b824-b36c227e9325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
