{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "734be6bd-6587-4cb9-87d5-92b9c0bb672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: conda: command not found\n"
     ]
    }
   ],
   "source": [
    "!conda create -n fast_unet python==3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012399be-45cb-4588-91c9-ed12289fdaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Mon_Apr__3_17:16:06_PDT_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.105\n",
      "Build cuda_12.1.r12.1/compiler.32688072_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80258898-ea8b-498a-84ba-d159dae7a21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.10/site-packages (2.6.0)\n",
      "Collecting torch-tensorrt\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch_tensorrt-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting tensorrt\n",
      "  Downloading tensorrt-10.9.0.34.tar.gz (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "donePreparing metadata (setup.py) ... \u001b[?25l\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.10/site-packages (from torch-tensorrt) (2.2.3)\n",
      "  Downloading tensorrt-10.7.0.post1.tar.gz (35 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting tensorrt-cu12<10.8.0,>=10.7.0.post1\n",
      "  Downloading tensorrt_cu12-10.7.0.post1.tar.gz (18 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting tensorrt-cu12-bindings<10.8.0,>=10.7.0\n",
      "  Downloading tensorrt_cu12_bindings-10.7.0.post1-cp310-none-manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=23 in /venv/main/lib/python3.10/site-packages (from torch-tensorrt) (24.2)\n",
      "Collecting tensorrt-cu12-libs<10.8.0,>=10.7.0\n",
      "  Downloading tensorrt_cu12_libs-10.7.0.post1.tar.gz (710 bytes)\n",
      "  Installing build dependencies ... \u001b[?done\n",
      "done5h  Getting requirements to build wheel ... \u001b[?25l\n",
      "donePreparing metadata (pyproject.toml) ... \u001b[?25l\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Building wheels for collected packages: tensorrt, tensorrt-cu12, tensorrt-cu12-libs\n",
      "doneng wheel for tensorrt (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for tensorrt: filename=tensorrt-10.7.0.post1-py2.py3-none-any.whl size=42186 sha256=185aa7fc2a685e92f9dc1313776be461976876afb3aeccba1fc02d5458a7c581\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/3a/4a/7cbc5f10f993bc284e6db8bb2e72cac9f1b5811da7026f72e3\n",
      "donesorrt-cu12 (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for tensorrt-cu12: filename=tensorrt_cu12-10.7.0.post1-py2.py3-none-any.whl size=17648 sha256=694dd26917a8f99ce77456a7898c117339edcd6cf4000ca8eb099d0691d82d96\n",
      "  Stored in directory: /root/.cache/pip/wheels/13/40/e7/5dfa276afaf6a78b52d5728547a0fc994811f637f87e3c18e3\n",
      "doneilding wheel for tensorrt-cu12-libs (pyproject.toml) ... \u001b[?25l\n",
      "  Created wheel for tensorrt-cu12-libs: filename=tensorrt_cu12_libs-10.7.0.post1-py2.py3-none-manylinux_2_17_x86_64.whl size=2069981220 sha256=ab4b8ef46a113f2e704b4c755c4db89789d70069c806d50683e17811060a09e2\n",
      "  Stored in directory: /root/.cache/pip/wheels/f4/72/5a/fb0beda45fbeb8b24e490aa68c0e35a274179a04f5ae08e39a\n",
      "Successfully built tensorrt tensorrt-cu12 tensorrt-cu12-libs\n",
      "Installing collected packages: tensorrt-cu12-bindings, tensorrt-cu12-libs, tensorrt-cu12, tensorrt, torch-tensorrt\n",
      "Successfully installed tensorrt-10.7.0.post1 tensorrt-cu12-10.7.0.post1 tensorrt-cu12-bindings-10.7.0.post1 tensorrt-cu12-libs-10.7.0.post1 torch-tensorrt-2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch torch-tensorrt tensorrt --extra-index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63bf9e0c-663a-4972-b26c-1aed82e258dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting nvidia-modelopt[all]\n",
      "  Downloading https://pypi.nvidia.com/nvidia-modelopt/nvidia_modelopt-0.25.0-py3-none-manylinux2014_x86_64.whl (616 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.5/616.5 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting rich\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting cloudpickle>=1.6.0\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.10/site-packages (from nvidia-modelopt[all]) (24.2)\n",
      "Collecting pydantic>=2.0\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-modelopt-core==0.25.0\n",
      "  Downloading https://pypi.nvidia.com/nvidia-modelopt-core/nvidia_modelopt_core-0.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting numpy<2\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /venv/main/lib/python3.10/site-packages (from nvidia-modelopt[all]) (4.67.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting onnxruntime-gpu~=1.20.1\n",
      "  Downloading onnxruntime_gpu-1.20.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (291.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.5/291.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting cupy-cuda12x\n",
      "  Downloading cupy_cuda12x-13.4.0-cp310-cp310-manylinux2014_x86_64.whl (104.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting onnx-graphsurgeon\n",
      "  Downloading https://pypi.nvidia.com/onnx-graphsurgeon/onnx_graphsurgeon-0.5.6-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 KB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting onnx\n",
      "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting torchprofile>=0.0.4\n",
      "  Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
      "Collecting diffusers>=0.27.2\n",
      "  Downloading diffusers-0.32.2-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Collecting transformers>=4.40.2\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.2 in /venv/main/lib/python3.10/site-packages (from nvidia-modelopt[all]) (2.6.0)\n",
      "Collecting peft>=0.12.0\n",
      "  Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /venv/main/lib/python3.10/site-packages (from nvidia-modelopt[all]) (0.21.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pynvml\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Collecting accelerate>=0.27.2\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub>=0.24.0 in /venv/main/lib/python3.10/site-packages (from nvidia-modelopt[all]) (0.28.1)\n",
      "Collecting onnxmltools\n",
      "  Downloading onnxmltools-1.13.0-py2.py3-none-any.whl (328 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 KB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting safetensors\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting datasets>=2.16.1\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 KB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting onnxconverter-common\n",
      "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting cppimport\n",
      "  Downloading cppimport-22.8.2.tar.gz (26 kB)\n",
      "  Installing build dependencies ..done\n",
      "done5h  Getting requirements to build wheel ... \u001b[?25l\n",
      "donePreparing metadata (pyproject.toml) ... \u001b[?25l\n",
      "Collecting pulp\n",
      "  Downloading PuLP-3.0.2-py3-none-any.whl (17.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from accelerate>=0.27.2->nvidia-modelopt[all]) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.10/site-packages (from accelerate>=0.27.2->nvidia-modelopt[all]) (6.0.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.11.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting fsspec[http]<=2024.12.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (2.32.3)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from datasets>=2.16.1->nvidia-modelopt[all]) (3.17.0)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: Pillow in /venv/main/lib/python3.10/site-packages (from diffusers>=0.27.2->nvidia-modelopt[all]) (11.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.24.0->nvidia-modelopt[all]) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.24.0->nvidia-modelopt[all]) (2025.2.0)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-6.30.0-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 KB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /venv/main/lib/python3.10/site-packages (from onnxruntime-gpu~=1.20.1->nvidia-modelopt[all]) (1.13.1)\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.3.1.170)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (9.1.0.70)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch>=2.2->nvidia-modelopt[all]) (2.21.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy->onnxruntime-gpu~=1.20.1->nvidia-modelopt[all]) (1.3.0)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pybind11\n",
      "  Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 KB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting mako\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting fastrlock>=0.5\n",
      "  Downloading fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /venv/main/lib/python3.10/site-packages (from rich->nvidia-modelopt[all]) (2.19.1)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 KB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 KB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.5.0-py3-none-any.whl (15 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->nvidia-modelopt[all]) (3.4.1)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting zipp>=3.20\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch>=2.2->nvidia-modelopt[all]) (3.0.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 KB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.10/site-packages (from pandas->datasets>=2.16.1->nvidia-modelopt[all]) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 KB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.1->nvidia-modelopt[all]) (1.17.0)\n",
      "Building wheels for collected packages: cppimport\n",
      "doneng wheel for cppimport (pyproject.toml) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for cppimport: filename=cppimport-22.8.2-py3-none-any.whl size=17699 sha256=077d2ae65017b55a569acfb67977c7a9fc6c6179f5d1789b0e3eb3da8bbec6e1\n",
      "  Stored in directory: /root/.cache/pip/wheels/38/0d/cd/d3a3135529beaf8c8fde3957e56ff89c25cd14ab18358bd724\n",
      "Successfully built cppimport\n",
      "Installing collected packages: pytz, nvidia-ml-py, flatbuffers, fastrlock, zipp, xxhash, tzdata, safetensors, regex, pynvml, pydantic-core, pybind11, pyarrow, pulp, protobuf, propcache, nvidia-modelopt-core, numpy, ninja, multidict, mdurl, mako, humanfriendly, fsspec, frozenlist, dill, cloudpickle, attrs, async-timeout, annotated-types, aiohappyeyeballs, yarl, scipy, pydantic, pandas, onnx, multiprocess, markdown-it-py, importlib-metadata, cupy-cuda12x, cppimport, coloredlogs, aiosignal, tokenizers, rich, onnxruntime-gpu, onnxconverter-common, onnx-graphsurgeon, diffusers, aiohttp, transformers, onnxmltools, nvidia-modelopt, accelerate, torchprofile, peft, datasets\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "Successfully installed accelerate-1.4.0 aiohappyeyeballs-2.5.0 aiohttp-3.11.13 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-5.0.1 attrs-25.1.0 cloudpickle-3.1.1 coloredlogs-15.0.1 cppimport-22.8.2 cupy-cuda12x-13.4.0 datasets-3.3.2 diffusers-0.32.2 dill-0.3.8 fastrlock-0.8.3 flatbuffers-25.2.10 frozenlist-1.5.0 fsspec-2024.12.0 humanfriendly-10.0 importlib-metadata-8.6.1 mako-1.3.9 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 ninja-1.11.1.3 numpy-1.26.4 nvidia-ml-py-12.570.86 nvidia-modelopt-0.25.0 nvidia-modelopt-core-0.25.0 onnx-1.17.0 onnx-graphsurgeon-0.5.6 onnxconverter-common-1.14.0 onnxmltools-1.13.0 onnxruntime-gpu-1.20.2 pandas-2.2.3 peft-0.14.0 propcache-0.3.0 protobuf-3.20.2 pulp-3.0.2 pyarrow-19.0.1 pybind11-2.13.6 pydantic-2.10.6 pydantic-core-2.27.2 pynvml-12.0.0 pytz-2025.1 regex-2024.11.6 rich-13.9.4 safetensors-0.5.3 scipy-1.15.2 tokenizers-0.21.0 torchprofile-0.0.4 transformers-4.49.0 tzdata-2025.1 xxhash-3.5.0 yarl-1.18.3 zipp-3.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"nvidia-modelopt[all]\" -U --extra-index-url https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3af4f3a-7649-459e-aea8-ad961bf59aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /venv/main/lib/python3.10/site-packages (1.17.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /venv/main/lib/python3.10/site-packages (from onnx) (3.20.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /venv/main/lib/python3.10/site-packages (from onnx) (1.26.4)\n",
      "Collecting onnxscript\n",
      "  Downloading onnxscript-0.2.2-py3-none-any.whl (694 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m694.9/694.9 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting ml_dtypes\n",
      "  Downloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /venv/main/lib/python3.10/site-packages (from onnxscript) (24.2)\n",
      "Requirement already satisfied: onnx>=1.16 in /venv/main/lib/python3.10/site-packages (from onnxscript) (1.17.0)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /venv/main/lib/python3.10/site-packages (from onnxscript) (4.12.2)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.10/site-packages (from onnxscript) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /venv/main/lib/python3.10/site-packages (from onnx>=1.16->onnxscript) (3.20.2)\n",
      "Installing collected packages: ml_dtypes, onnxscript\n",
      "Successfully installed ml_dtypes-0.5.1 onnxscript-0.2.2\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting optimum\n",
      "  Downloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 KB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /venv/main/lib/python3.10/site-packages (from timm) (0.21.0)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.10/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.10/site-packages (from timm) (2.6.0)\n",
      "Requirement already satisfied: safetensors in /venv/main/lib/python3.10/site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub in /venv/main/lib/python3.10/site-packages (from timm) (0.28.1)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.10/site-packages (from optimum) (24.2)\n",
      "Requirement already satisfied: transformers>=4.29 in /venv/main/lib/python3.10/site-packages (from optimum) (4.49.0)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.10/site-packages (from optimum) (1.26.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.12.0)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /venv/main/lib/python3.10/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch->timm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch->timm) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch->timm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch->timm) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.3.1.170)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.5.8)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch->timm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch->timm) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.10/site-packages (from transformers>=4.29->optimum) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /venv/main/lib/python3.10/site-packages (from transformers>=4.29->optimum) (0.21.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /venv/main/lib/python3.10/site-packages (from torchvision->timm) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
      "Installing collected packages: timm, optimum\n",
      "Successfully installed optimum-1.24.0 timm-1.0.15\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx\n",
    "!pip install onnxscript\n",
    "!pip install timm optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "428cf84b-031b-4b9d-a0b6-afec5f8bba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast_unet.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls /workspace/onnx_models/fast_unet.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc3d1ca-d0cf-4ef9-80e9-be85e4ba7a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No custom ops found. If that's not correct, please make sure that the 'tensorrt' python package is correctly installed and that the paths to 'libcudnn*.so' and TensorRT 'lib/' are in 'LD_LIBRARY_PATH'. If the custom op is not directly available as a plugin in TensorRT, please also make sure that the path to the compiled '.so' TensorRT plugin is also being given via the  '--trt_plugins' flag (requires TRT 10+).\n",
      "INFO:root:Model /workspace/onnx_models/fast_unet_fp32.onnx with opset_version 16 is loaded.\n",
      "INFO:root:Quantization Mode: int8\n",
      "INFO:root:Quantizable op types in the model: ['Conv']\n",
      "INFO:root:Building non-residual Add input map ...\n",
      "INFO:root:Searching for hard-coded patterns like MHA, LayerNorm, etc. to avoid quantization.\n",
      "INFO:root:Building KGEN/CASK targeted partitions ...\n",
      "INFO:root:Classifying the partition nodes ...\n",
      "INFO:root:Total number of nodes: 91\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "\u001b[0;93m2025-03-10 03:33:21.600564851 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600613622 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600619443 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600637853 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600643263 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600647633 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600652113 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600655954 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.0.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600661034 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600665234 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600669374 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600674914 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600679924 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600684684 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600689035 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600693535 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.1.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600698545 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600703005 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600707035 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600712495 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600717075 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600721136 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600725566 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600729666 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.2.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600734226 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600738246 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600742176 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600747686 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600752176 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600757047 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600761057 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600765267 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.3.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600770097 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600774207 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600778087 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600783537 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600787967 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600792418 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600796338 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600800298 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.4.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600805398 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600809698 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600814248 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600819568 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600824299 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600828279 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600832279 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600836199 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.5.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600840829 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600844839 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600848759 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600854559 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600859210 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600863200 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600866980 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600870850 [W:onnxruntime:, graph.cc:1348 Graph] Initializer encoder.stages.6.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600875560 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600879510 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600883560 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600891441 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600896141 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600900071 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600903921 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600908121 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.0.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600912691 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600916781 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600920621 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600926292 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600930872 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600934902 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600938782 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600942722 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.1.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600947442 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600951232 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600955112 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600960813 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600965053 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600968973 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600972833 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600976983 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.2.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600980923 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600984693 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600988813 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600994334 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.600998484 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601002344 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601006224 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601009994 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.3.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601013914 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601017744 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601021504 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601026914 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601030935 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601034805 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601038595 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601042315 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.4.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601046265 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.0.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601050035 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.0.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601053795 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.0.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601058935 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.0.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601063096 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.1.conv.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601067016 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.1.conv.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601070766 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.1.norm.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601074506 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.stages.5.convs.1.norm.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601078756 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.0.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601082746 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.0.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601086976 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.1.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601092376 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.1.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601096327 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.2.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601100107 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.2.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601104027 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.3.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601107837 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.3.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601111707 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.4.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601115527 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.4.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601119417 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.5.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601124677 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.transpconvs.5.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601128737 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.seg_layers.5.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "\u001b[0;93m2025-03-10 03:33:21.601132528 [W:onnxruntime:, graph.cc:1348 Graph] Initializer decoder.seg_layers.5.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\u001b[m\n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "INFO:root:Deleting QDQ nodes from marked inputs to make certain operations fusible ...\n",
      "INFO:root:Total number of quantized nodes: 33\n",
      "INFO:root:Quantized type counts: {'Conv': 27, 'Concat': 6}\n",
      "INFO:root:Quantized onnx model is saved as /workspace/onnx_models/quant_fast_unet_int8.onnx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Example numpy file for single-input ONNX\n",
    "calib_data = np.random.randn(1, 1, 64, 256, 256)\n",
    "calib_data = calib_data.astype(np.float32)\n",
    "np.save(\"calib_data.npy\", calib_data)\n",
    "\n",
    "# Example numpy file for single/multi-input ONNX\n",
    "# Dict key should match the input names of ONNX\n",
    "# calib_data = {\n",
    "#     \"input_name\": np.random.randn(*shape),\n",
    "#     \"input_name2\": np.random.randn(*shape2),\n",
    "# }\n",
    "# np.savez(\"/workspace/calib_data.npz\", calib_data)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, './TensorRT-Model-Optimizer')\n",
    "\n",
    "import modelopt.onnx.quantization as moq\n",
    "import numpy as np\n",
    "\n",
    "calibration_data_path = 'calib_data.npy'\n",
    "# onnx_path = \"vit_base_patch16_224.onnx\"\n",
    "#\n",
    "calibration_data = np.load(calibration_data_path)\n",
    "\n",
    "moq.quantize(\n",
    "    onnx_path=\"/workspace/onnx_models/fast_unet_fp32.onnx\",\n",
    "    calibration_data=calibration_data,\n",
    "    calibration_method='max',\n",
    "    output_path=\"/workspace/onnx_models/quant_fast_unet_int8.onnx\",\n",
    "    quantize_mode=\"int8\",\n",
    "    high_precision_dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b59e403e-8000-47d1-a617-e5fb8dfa8d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast_unet.onnx\tquant_fast_unet_fp16.engine\n"
     ]
    }
   ],
   "source": [
    "!ls /workspace/onnx_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "950f67b6-abb9-4e8d-8c8d-abe3ae03b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /workspace/onnx_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "038d9b02-3789-45bf-9691-bc31515bff5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TensorRT-Model-Optimizer'...\n",
      "remote: Enumerating objects: 4912, done.\u001b[K\n",
      "remote: Counting objects: 100% (1078/1078), done.\u001b[K\n",
      "remote: Compressing objects: 100% (811/811), done.\u001b[K\n",
      "remote: Total 4912 (delta 678), reused 436 (delta 254), pack-reused 3834 (from 1)\u001b[K\n",
      "Receiving objects: 100% (4912/4912), 23.68 MiB | 31.21 MiB/s, done.\n",
      "Resolving deltas: 100% (3431/3431), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/TensorRT-Model-Optimizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af39d57-f016-4063-b891-576f41c26657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:trtexec --onnx=/tmp/tmpm8e9bpr8/onnx/quant_fast_unet_int8.onnx --stronglyTyped --saveEngine=/tmp/tmpm8e9bpr8/quant_fast_unet_int8/quant_fast_unet_int8.engine --skipInference --builderOptimizationLevel=4 --verbose --exportLayerInfo=/tmp/tmpm8e9bpr8/quant_fast_unet_int8/quant_fast_unet_int8.engine.graph.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading onnx model from path: /workspace/onnx_models/quant_fast_unet_int8.onnx\n",
      "Compiling the TensorRT engine. This may take a few minutes...\n",
      "Writing onnx model to path: /tmp/tmpm8e9bpr8/onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:build_engine: 197180.02343177795 ms\n",
      "INFO:root:trtexec --loadEngine=/tmp/tmp6akw0685/engine --warmUp=500 --avgRuns=100 --iterations=100 --dumpProfile --separateProfileRun --exportProfile=/tmp/tmp6akw0685/profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation completed.\n",
      "Size of the TensorRT engine: 54.56 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:profile_engine: 16576.071977615356 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference latency reported by device_model: 72.3352 ms\n"
     ]
    }
   ],
   "source": [
    "from modelopt.torch._deploy._runtime import RuntimeRegistry\n",
    "from modelopt.torch._deploy.device_model import DeviceModel\n",
    "from modelopt.torch._deploy.utils import OnnxBytes\n",
    "# Configure deployment\n",
    "deployment = {\n",
    "    \"runtime\": \"TRT\",\n",
    "    \"version\": \"10.3\",\n",
    "    \"precision\": 'stronglyTyped',\n",
    "}\n",
    "\n",
    "# Create an ONNX bytes object\n",
    "onnx_bytes = OnnxBytes('/workspace/onnx_models/quant_fast_unet_int8.onnx').to_bytes()\n",
    "\n",
    "# Get the runtime client\n",
    "client = RuntimeRegistry.get(deployment)\n",
    "\n",
    "# Compile the TRT model\n",
    "print(\"Compiling the TensorRT engine. This may take a few minutes...\")\n",
    "compiled_model = client.ir_to_compiled(onnx_bytes)\n",
    "print(\"Compilation completed.\")\n",
    "\n",
    "# Print size of the compiled model\n",
    "engine_size = len(compiled_model)\n",
    "print(f\"Size of the TensorRT engine: {engine_size / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# Create the device model\n",
    "device_model = DeviceModel(client, compiled_model, metadata={})\n",
    "print(f\"Inference latency reported by device_model: {device_model.get_latency()} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "709572df-37e4-44eb-981b-53cca55c9877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading onnx model from path: /workspace/onnx_models/quant_fast_unet_int8.onnx\n",
      "Compiling the TensorRT engine. This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:trtexec --onnx=/tmp/tmpnvpafdr6/onnx/quant_fast_unet_int8.onnx --fp16 --int8 --saveEngine=/tmp/tmpnvpafdr6/quant_fast_unet_int8/quant_fast_unet_int8.engine --skipInference --builderOptimizationLevel=4 --verbose --exportLayerInfo=/tmp/tmpnvpafdr6/quant_fast_unet_int8/quant_fast_unet_int8.engine.graph.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing onnx model to path: /tmp/tmpnvpafdr6/onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:build_engine: 412503.8480758667 ms\n",
      "INFO:root:trtexec --loadEngine=/tmp/tmpo3vxl8md/engine --warmUp=500 --avgRuns=100 --iterations=100 --dumpProfile --separateProfileRun --exportProfile=/tmp/tmpo3vxl8md/profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation completed.\n",
      "Size of the TensorRT engine: 49.99 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:profile_engine: 8237.624168395996 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference latency reported by device_model: 28.671385714285716 ms\n"
     ]
    }
   ],
   "source": [
    "from modelopt.torch._deploy._runtime import RuntimeRegistry\n",
    "from modelopt.torch._deploy.device_model import DeviceModel\n",
    "from modelopt.torch._deploy.utils import OnnxBytes\n",
    "# Configure deployment\n",
    "deployment = {\n",
    "    \"runtime\": \"TRT\",\n",
    "    \"version\": \"10.3\",\n",
    "    \"precision\": 'int8',\n",
    "}\n",
    "\n",
    "# Create an ONNX bytes object\n",
    "onnx_bytes = OnnxBytes('/workspace/onnx_models/quant_fast_unet_int8.onnx').to_bytes()\n",
    "\n",
    "# Get the runtime client\n",
    "client = RuntimeRegistry.get(deployment)\n",
    "\n",
    "# Compile the TRT model\n",
    "print(\"Compiling the TensorRT engine. This may take a few minutes...\")\n",
    "compiled_model = client.ir_to_compiled(onnx_bytes)\n",
    "print(\"Compilation completed.\")\n",
    "\n",
    "# Print size of the compiled model\n",
    "engine_size = len(compiled_model)\n",
    "print(f\"Size of the TensorRT engine: {engine_size / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# Create the device model\n",
    "device_model = DeviceModel(client, compiled_model, metadata={})\n",
    "print(f\"Inference latency reported by device_model: {device_model.get_latency()} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43691caa-36c4-43fb-a01b-032095f01aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading onnx model from path: /workspace/onnx_models/quant_fast_unet_int8.onnx\n",
      "Compiling the TensorRT engine. This may take a few minutes...\n",
      "Writing onnx model to path: /tmp/tmprg6zufbb/onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:trtexec --onnx=/tmp/tmprg6zufbb/onnx/quant_fast_unet_int8.onnx --saveEngine=/tmp/tmprg6zufbb/quant_fast_unet_int8/quant_fast_unet_int8.engine --skipInference --builderOptimizationLevel=3 --verbose --exportLayerInfo=/tmp/tmprg6zufbb/quant_fast_unet_int8/quant_fast_unet_int8.engine.graph.json\n",
      "INFO:root:build_engine: 65652.44674682617 ms\n",
      "INFO:root:trtexec --loadEngine=/tmp/tmpo7jym83b/engine --warmUp=500 --avgRuns=100 --iterations=100 --dumpProfile --separateProfileRun --exportProfile=/tmp/tmpo7jym83b/profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation completed.\n",
      "Size of the TensorRT engine: 54.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:profile_engine: 16538.774490356445 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference latency reported by device_model: 73.03704285714286 ms\n"
     ]
    }
   ],
   "source": [
    "from modelopt.torch._deploy._runtime import RuntimeRegistry\n",
    "from modelopt.torch._deploy.device_model import DeviceModel\n",
    "from modelopt.torch._deploy.utils import OnnxBytes\n",
    "# Configure deployment\n",
    "deployment = {\n",
    "    \"runtime\": \"TRT\",\n",
    "    \"version\": \"10.3\",\n",
    "    \"precision\": 'fp32',\n",
    "}\n",
    "\n",
    "# Create an ONNX bytes object\n",
    "onnx_bytes = OnnxBytes('/workspace/onnx_models/quant_fast_unet_int8.onnx').to_bytes()\n",
    "\n",
    "# Get the runtime client\n",
    "client = RuntimeRegistry.get(deployment)\n",
    "\n",
    "# Compile the TRT model\n",
    "print(\"Compiling the TensorRT engine. This may take a few minutes...\")\n",
    "compiled_model = client.ir_to_compiled(onnx_bytes)\n",
    "print(\"Compilation completed.\")\n",
    "\n",
    "# Print size of the compiled model\n",
    "engine_size = len(compiled_model)\n",
    "print(f\"Size of the TensorRT engine: {engine_size / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# Create the device model\n",
    "device_model = DeviceModel(client, compiled_model, metadata={})\n",
    "print(f\"Inference latency reported by device_model: {device_model.get_latency()} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43c7aff-ef75-4a60-a5ed-b9c8430c5b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading onnx model from path: /workspace/onnx_models/quant_fast_unet_int8.onnx\n",
      "Compiling the TensorRT engine. This may take a few minutes...\n",
      "Writing onnx model to path: /tmp/tmpxgb0g9la/onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:trtexec --onnx=/tmp/tmpxgb0g9la/onnx/quant_fast_unet_int8.onnx --fp16 --saveEngine=/tmp/tmpxgb0g9la/quant_fast_unet_int8/quant_fast_unet_int8.engine --skipInference --builderOptimizationLevel=3 --verbose --exportLayerInfo=/tmp/tmpxgb0g9la/quant_fast_unet_int8/quant_fast_unet_int8.engine.graph.json\n",
      "INFO:root:build_engine: 115142.45390892029 ms\n",
      "INFO:root:trtexec --loadEngine=/tmp/tmpjeaqfth7/engine --warmUp=500 --avgRuns=100 --iterations=100 --dumpProfile --separateProfileRun --exportProfile=/tmp/tmpjeaqfth7/profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation completed.\n",
      "Size of the TensorRT engine: 49.02 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:profile_engine: 8169.643402099609 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference latency reported by device_model: 28.815328571428573 ms\n"
     ]
    }
   ],
   "source": [
    "from modelopt.torch._deploy._runtime import RuntimeRegistry\n",
    "from modelopt.torch._deploy.device_model import DeviceModel\n",
    "from modelopt.torch._deploy.utils import OnnxBytes\n",
    "# Configure deployment\n",
    "deployment = {\n",
    "    \"runtime\": \"TRT\",\n",
    "    \"version\": \"10.3\",\n",
    "    \"precision\": 'fp16',\n",
    "}\n",
    "\n",
    "# Create an ONNX bytes object\n",
    "onnx_bytes = OnnxBytes('/workspace/onnx_models/quant_fast_unet_int8.onnx').to_bytes()\n",
    "\n",
    "# Get the runtime client\n",
    "client = RuntimeRegistry.get(deployment)\n",
    "\n",
    "# Compile the TRT model\n",
    "print(\"Compiling the TensorRT engine. This may take a few minutes...\")\n",
    "compiled_model = client.ir_to_compiled(onnx_bytes)\n",
    "print(\"Compilation completed.\")\n",
    "\n",
    "# Print size of the compiled model\n",
    "engine_size = len(compiled_model)\n",
    "print(f\"Size of the TensorRT engine: {engine_size / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# Create the device model\n",
    "device_model = DeviceModel(client, compiled_model, metadata={})\n",
    "print(f\"Inference latency reported by device_model: {device_model.get_latency()} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "913db75a-037d-49d4-b736-520eecce7879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-09 19:25:56--  https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.3.0/local_repo/nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5_1.0-1_amd64.deb\n",
      "Resolving developer.nvidia.com (developer.nvidia.com)... 2.17.112.69, 2.17.112.73\n",
      "Connecting to developer.nvidia.com (developer.nvidia.com)|2.17.112.69|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://developer.download.nvidia.com/compute/machine-learning/tensorrt/10.3.0/local_repo/nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5_1.0-1_amd64.deb [following]\n",
      "--2025-03-09 19:25:57--  https://developer.download.nvidia.com/compute/machine-learning/tensorrt/10.3.0/local_repo/nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5_1.0-1_amd64.deb\n",
      "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 2.17.211.104, 84.53.132.89, 84.53.132.136, ...\n",
      "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|2.17.211.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2710742922 (2.5G) [application/x-deb]\n",
      "Saving to: ‘nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5_1.0-1_amd64.deb’\n",
      "\n",
      "nv-tensorrt-local-r 100%[===================>]   2.52G  4.41MB/s    in 10m 11s \n",
      "\n",
      "2025-03-09 19:36:10 (4.23 MB/s) - ‘nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5_1.0-1_amd64.deb’ saved [2710742922/2710742922]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.3.0/local_repo/nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5_1.0-1_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c548faa-b710-43df-95a4-ee8175227490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No LSB modules are available.\n",
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 22.04.5 LTS\n",
      "Release:\t22.04\n",
      "Codename:\tjammy\n"
     ]
    }
   ],
   "source": [
    "!lsb_release -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94caefcf-db0d-4da2-a28d-a6136089cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os=\"ubuntuxx04\"\n",
    "tag=\"10.x.x-cuda-x.x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb908ba0-add1-42c0-8363-e7a8d761243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Reading database ... 40294 files and directories currently installed.)\n",
      "Preparing to unpack nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5_1.0-1_amd64.deb ...\n",
      "Unpacking nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5 (1.0-1) over (1.0-1) ...\n",
      "Setting up nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5 (1.0-1) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo dpkg -i nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5_1.0-1_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62018ea6-db81-4f01-840e-60ce2385839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo cp /var/nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5/nv-tensorrt-local-620E7D29-keyring.gpg /usr/share/keyrings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6629dff-257a-475d-958a-bc22d2d26535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v100900] [b34] # /usr/src/tensorrt/bin/trtexec\n",
      "=== Model Options ===\n",
      "  --onnx=<file>               ONNX model\n",
      "\n",
      "=== Build Options ===\n",
      "  --minShapes=spec                   Build with dynamic shapes using a profile with the min shapes provided\n",
      "  --optShapes=spec                   Build with dynamic shapes using a profile with the opt shapes provided\n",
      "  --maxShapes=spec                   Build with dynamic shapes using a profile with the max shapes provided\n",
      "  --minShapesCalib=spec              Calibrate with dynamic shapes using a profile with the min shapes provided\n",
      "  --optShapesCalib=spec              Calibrate with dynamic shapes using a profile with the opt shapes provided\n",
      "  --maxShapesCalib=spec              Calibrate with dynamic shapes using a profile with the max shapes provided\n",
      "                                     Note: All three of min, opt and max shapes must be supplied.\n",
      "                                           However, if only opt shapes is supplied then it will be expanded so\n",
      "                                           that min shapes and max shapes are set to the same values as opt shapes.\n",
      "                                           Input names can be wrapped with escaped single quotes (ex: 'Input:0').\n",
      "                                     Example input shapes spec: input0:1x3x256x256,input1:1x3x128x128\n",
      "                                     For scalars (0-D shapes), use input0:scalar or simply input0: with nothing after the colon.\n",
      "                                     Each input shape is supplied as a key-value pair where key is the input name and\n",
      "                                     value is the dimensions (including the batch dimension) to be used for that input.\n",
      "                                     Each key-value pair has the key and value separated using a colon (:).\n",
      "                                     Multiple input shapes can be provided via comma-separated key-value pairs, and each input name can\n",
      "                                     contain at most one wildcard ('*') character.\n",
      "  --inputIOFormats=spec              Type and format of each of the input tensors (default = all inputs in fp32:chw)\n",
      "                                     See --outputIOFormats help for the grammar of type and format list.\n",
      "                                     Note: If this option is specified, please set comma-separated types and formats for all\n",
      "                                           inputs following the same order as network inputs ID (even if only one input\n",
      "                                           needs specifying IO format) or set the type and format once for broadcasting.\n",
      "  --outputIOFormats=spec             Type and format of each of the output tensors (default = all outputs in fp32:chw)\n",
      "                                     Note: If this option is specified, please set comma-separated types and formats for all\n",
      "                                           outputs following the same order as network outputs ID (even if only one output\n",
      "                                           needs specifying IO format) or set the type and format once for broadcasting.\n",
      "                                     IO Formats: spec  ::= IOfmt[\",\"spec]\n",
      "                                                 IOfmt ::= type:fmt\n",
      "                                                 type  ::= \"fp32\"|\"fp16\"|\"bf16\"|\"int32\"|\"int64\"|\"int8\"|\"uint8\"|\"bool\"\n",
      "                                                 fmt   ::= (\"chw\"|\"chw2\"|\"chw4\"|\"hwc8\"|\"chw16\"|\"chw32\"|\"dhwc8\"|\n",
      "                                                            \"cdhw32\"|\"hwc\"|\"dla_linear\"|\"dla_hwc4\")[\"+\"fmt]\n",
      "  --memPoolSize=poolspec             Specify the size constraints of the designated memory pool(s)\n",
      "                                     Supports the following base-2 suffixes: B (Bytes), G (Gibibytes), K (Kibibytes), M (Mebibytes).\n",
      "                                     If none of suffixes is appended, the defualt unit is in MiB.\n",
      "                                     Note: Also accepts decimal sizes, e.g. 0.25M. Will be rounded down to the nearest integer bytes.\n",
      "                                     In particular, for dlaSRAM the bytes will be rounded down to the nearest power of 2.\n",
      "                                     Pool constraint: poolspec ::= poolfmt[\",\"poolspec]\n",
      "                                                      poolfmt ::= pool:size\n",
      "                                                      pool ::= \"workspace\"|\"dlaSRAM\"|\"dlaLocalDRAM\"|\"dlaGlobalDRAM\"|\"tacticSharedMem\"\n",
      "  --profilingVerbosity=mode          Specify profiling verbosity. mode ::= layer_names_only|detailed|none (default = layer_names_only).\n",
      "                                     Please only assign once.\n",
      "  --avgTiming=M                      Set the number of times averaged in each iteration for kernel selection (default = 8)\n",
      "  --refit                            Mark the engine as refittable. This will allow the inspection of refittable layers \n",
      "                                     and weights within the engine.\n",
      "  --stripWeights                     Strip weights from plan. This flag works with either refit or refit with identical weights. Default\n",
      "                                     to latter, but you can switch to the former by enabling both --stripWeights and --refit at the same\n",
      "                                     time.\n",
      "  --stripAllWeights                  Alias for combining the --refit and --stripWeights options. It marks all weights as refittable,\n",
      "                                     disregarding any performance impact. Additionally, it strips all refittable weights after the \n",
      "                                     engine is built.\n",
      "  --weightless                       [Deprecated] this knob has been deprecated. Please use --stripWeights\n",
      "  --versionCompatible, --vc          Mark the engine as version compatible. This allows the engine to be used with newer versions\n",
      "                                     of TensorRT on the same host OS, as well as TensorRT's dispatch and lean runtimes.\n",
      "  --pluginInstanceNorm, --pi         Set `kNATIVE_INSTANCENORM` to false in the ONNX parser. This will cause the ONNX parser to use\n",
      "                                     a plugin InstanceNorm implementation over the native implementation when parsing.\n",
      "  --useRuntime=runtime               TensorRT runtime to execute engine. \"lean\" and \"dispatch\" require loading VC engine and do\n",
      "                                     not support building an engine.\n",
      "                                         runtime::= \"full\"|\"lean\"|\"dispatch\"\n",
      "  --leanDLLPath=<file>               External lean runtime DLL to use in version compatiable mode.\n",
      "  --excludeLeanRuntime               When --versionCompatible is enabled, this flag indicates that the generated engine should\n",
      "                                     not include an embedded lean runtime. If this is set, the user must explicitly specify a\n",
      "                                     valid lean runtime to use when loading the engine.\n",
      "  --monitorMemory                    Enable memory monitor report for debugging usage. (default = disabled)\n",
      "  --sparsity=spec                    Control sparsity (default = disabled). \n",
      "                                     Sparsity: spec ::= \"disable\", \"enable\", \"force\"\n",
      "                                     Note: Description about each of these options is as below\n",
      "                                           disable = do not enable sparse tactics in the builder (this is the default)\n",
      "                                           enable  = enable sparse tactics in the builder (but these tactics will only be\n",
      "                                                     considered if the weights have the right sparsity pattern)\n",
      "                                           force   = enable sparse tactics in the builder and force-overwrite the weights to have\n",
      "                                                     a sparsity pattern (even if you loaded a model yourself)\n",
      "                                                     [Deprecated] this knob has been deprecated.\n",
      "                                                     Please use <polygraphy surgeon prune> to rewrite the weights.\n",
      "  --noTF32                           Disable tf32 precision (default is to enable tf32, in addition to fp32)\n",
      "  --fp16                             Enable fp16 precision, in addition to fp32 (default = disabled)\n",
      "  --bf16                             Enable bf16 precision, in addition to fp32 (default = disabled)\n",
      "  --int8                             Enable int8 precision, in addition to fp32 (default = disabled)\n",
      "  --fp8                              Enable fp8 precision, in addition to fp32 (default = disabled)\n",
      "  --int4                             Enable int4 precision, in addition to fp32 (default = disabled)\n",
      "  --best                             Enable all precisions to achieve the best performance (default = disabled)\n",
      "  --stronglyTyped                    Create a strongly typed network. (default = disabled)\n",
      "  --directIO                         [Deprecated] Avoid reformatting at network boundaries. (default = disabled)\n",
      "  --precisionConstraints=spec        Control precision constraint setting. (default = none)\n",
      "                                     Precision Constraints: spec ::= \"none\" | \"obey\" | \"prefer\"\n",
      "                                         none = no constraints\n",
      "                                         prefer = meet precision constraints set by --layerPrecisions/--layerOutputTypes if possible\n",
      "                                         obey = meet precision constraints set by --layerPrecisions/--layerOutputTypes or fail\n",
      "                                                otherwise\n",
      "  --layerPrecisions=spec             Control per-layer precision constraints. Effective only when precisionConstraints is set to\n",
      "                                     \"obey\" or \"prefer\". (default = none)\n",
      "                                     The specs are read left-to-right, and later ones override earlier ones. Each layer name can\n",
      "                                     contain at most one wildcard ('*') character.\n",
      "                                     Per-layer precision spec ::= layerPrecision[\",\"spec]\n",
      "                                                         layerPrecision ::= layerName\":\"precision\n",
      "                                                         precision ::= \"fp32\"|\"fp16\"|\"bf16\"|\"int32\"|\"int8\"\n",
      "  --layerOutputTypes=spec            Control per-layer output type constraints. Effective only when precisionConstraints is set to\n",
      "                                     \"obey\" or \"prefer\". (default = none\n",
      "                                     The specs are read left-to-right, and later ones override earlier ones. Each layer name can\n",
      "                                     contain at most one wildcard ('*') character. If a layer has more than\n",
      "                                     one output, then multiple types separated by \"+\" can be provided for this layer.\n",
      "                                     Per-layer output type spec ::= layerOutputTypes[\",\"spec]\n",
      "                                                           layerOutputTypes ::= layerName\":\"type\n",
      "                                                           type ::= \"fp32\"|\"fp16\"|\"bf16\"|\"int32\"|\"int8\"[\"+\"type]\n",
      "  --layerDeviceTypes=spec            Specify layer-specific device type.\n",
      "                                     The specs are read left-to-right, and later ones override earlier ones. If a layer does not have\n",
      "                                     a device type specified, the layer will opt for the default device type.\n",
      "                                     Per-layer device type spec ::= layerDeviceTypePair[\",\"spec]\n",
      "                                                           layerDeviceTypePair ::= layerName\":\"deviceType\n",
      "                                                           deviceType ::= \"GPU\"|\"DLA\"\n",
      "  --calib=<file>                     Read INT8 calibration cache file\n",
      "  --safe                             Enable build safety certified engine, if DLA is enable, --buildDLAStandalone will be specified\n",
      "                                     automatically (default = disabled)\n",
      "  --buildDLAStandalone               Enable build DLA standalone loadable which can be loaded by cuDLA, when this option is enabled, \n",
      "                                     --allowGPUFallback is disallowed and --skipInference is enabled by default. Additionally, \n",
      "                                     specifying --inputIOFormats and --outputIOFormats restricts I/O data type and memory layout\n",
      "                                     (default = disabled)\n",
      "  --allowGPUFallback                 When DLA is enabled, allow GPU fallback for unsupported layers (default = disabled)\n",
      "  --restricted                       Enable safety scope checking with kSAFETY_SCOPE build flag\n",
      "  --saveEngine=<file>                Save the serialized engine\n",
      "  --loadEngine=<file>                Load a serialized engine\n",
      "  --asyncFileReader=<file>           Load a serialized engine using async stream reader\n",
      "  --getPlanVersionOnly               Print TensorRT version when loaded plan was created. Works without deserialization of the plan.\n",
      "                                     Use together with --loadEngine. Supported only for engines created with 8.6 and forward.\n",
      "  --tacticSources=tactics            Specify the tactics to be used by adding (+) or removing (-) tactics from the default \n",
      "                                     tactic sources (default = all available tactics).\n",
      "                                     Note: Currently only cuDNN, cuBLAS, cuBLAS-LT, and edge mask convolutions are listed as optional\n",
      "                                           tactics.\n",
      "                                     Tactic Sources: tactics ::= tactic[\",\"tactics]\n",
      "                                                     tactic  ::= (+|-)lib\n",
      "                                                     lib     ::= \"CUBLAS\"|\"CUBLAS_LT\"|\"CUDNN\"|\"EDGE_MASK_CONVOLUTIONS\"\n",
      "                                                                 |\"JIT_CONVOLUTIONS\"\n",
      "                                     For example, to disable cudnn and enable cublas: --tacticSources=-CUDNN,+CUBLAS\n",
      "  --noBuilderCache                   Disable timing cache in builder (default is to enable timing cache)\n",
      "  --noCompilationCache               Disable Compilation cache in builder, and the cache is part of timing cache (default is to enable compilation cache)\n",
      "  --errorOnTimingCacheMiss           Emit error when a tactic being timed is not present in the timing cache (default = false)\n",
      "  --timingCacheFile=<file>           Save/load the serialized global timing cache\n",
      "  --preview=features                 Specify preview feature to be used by adding (+) or removing (-) preview features from the default\n",
      "                                     Preview Features: features ::= feature[\",\"features]\n",
      "                                                       feature  ::= (+|-)flag\n",
      "                                                       flag     ::= \"aliasedPluginIO1003\"\n",
      "                                                                    |\"profileSharing0806\"\n",
      "  --builderOptimizationLevel         Set the builder optimization level. (default is 3)\n",
      "                                     A Higher level allows TensorRT to spend more time searching for better optimization strategy.\n",
      "                                     Valid values include integers from 0 to the maximum optimization level, which is currently 5.\n",
      "  --maxTactics                       Set the maximum number of tactics to time when there is a choice of tactics. (default is -1)\n",
      "                                     Larger number of tactics allow TensorRT to spend more building time on evaluating tactics.\n",
      "                                     Default value -1 means TensorRT can decide the number of tactics based on its own heuristic.\n",
      "  --hardwareCompatibilityLevel=mode  Make the engine file compatible with other GPU architectures. (default = none)\n",
      "                                     Hardware Compatibility Level: mode ::= \"none\" | \"ampere+\" | \"sameComputeCapability\"\n",
      "                                         none = no compatibility\n",
      "                                         ampere+ = compatible with Ampere and newer GPUs\n",
      "                                         sameComputeCapability = compatible with GPUs that have the same Compute Capability version\n",
      "  --runtimePlatform=platform         Set the target platform for runtime execution. (default = SameAsBuild)\n",
      "                                     When this option is enabled, --skipInference is enabled by default.\n",
      "                                     RuntimePlatfrom: platform ::= \"SameAsBuild\" | \"WindowsAMD64\"\n",
      "                                         SameAsBuild = no requirement for cross-platform compatibility.\n",
      "                                         WindowsAMD64 = set the target platform for engine execution as Windows AMD64 system\n",
      "  --tempdir=<dir>                    Overrides the default temporary directory TensorRT will use when creating temporary files.\n",
      "                                     See IRuntime::setTemporaryDirectory API documentation for more information.\n",
      "  --tempfileControls=controls        Controls what TensorRT is allowed to use when creating temporary executable files.\n",
      "                                     Should be a comma-separated list with entries in the format (in_memory|temporary):(allow|deny).\n",
      "                                     in_memory: Controls whether TensorRT is allowed to create temporary in-memory executable files.\n",
      "                                     temporary: Controls whether TensorRT is allowed to create temporary executable files in the\n",
      "                                                filesystem (in the directory given by --tempdir).\n",
      "                                     For example, to allow in-memory files and disallow temporary files:\n",
      "                                         --tempfileControls=in_memory:allow,temporary:deny\n",
      "                                     If a flag is unspecified, the default behavior is \"allow\".\n",
      "  --maxAuxStreams=N                  Set maximum number of auxiliary streams per inference stream that TRT is allowed to use to run \n",
      "                                     kernels in parallel if the network contains ops that can run in parallel, with the cost of more \n",
      "                                     memory usage. Set this to 0 for optimal memory usage. (default = using heuristics)\n",
      "  --profile                          Build with dynamic shapes using a profile with the min/max/opt shapes provided. Can be specified\n",
      "                                         multiple times to create multiple profiles with contiguous index.\n",
      "                                     (ex: --profile=0 --minShapes=<spec> --optShapes=<spec> --maxShapes=<spec> --profile=1 ...)\n",
      "  --calibProfile                     Select the optimization profile to calibrate by index. (default = 0)\n",
      "  --allowWeightStreaming             Enable a weight streaming engine. Must be specified with --stronglyTyped. TensorRT will disable\n",
      "                                     weight streaming at runtime unless --weightStreamingBudget is specified.\n",
      "  --markDebug                        Specify list of names of tensors to be marked as debug tensors. Separate names with a comma\n",
      "  --tilingOptimizationLevel          Set the tiling optimization level. (default is 0)\n",
      "                                     A Higher level allows TensorRT to spend more time searching for better optimization strategy.\n",
      "                                     Valid values include integers from 0 to the maximum tiling optimization level(4).\n",
      "  --l2LimitForTiling                 Set the L2 cache usage limit for tiling optimization(default is -1)\n",
      "\n",
      "=== Inference Options ===\n",
      "  --shapes=spec               Set input shapes for dynamic shapes inference inputs.\n",
      "                              Note: Input names can be wrapped with escaped single quotes (ex: 'Input:0').\n",
      "                              Example input shapes spec: input0:1x3x256x256, input1:1x3x128x128\n",
      "                              For scalars (0-D shapes), use input0:scalar or simply input0: with nothing after the colon.\n",
      "                              Each input shape is supplied as a key-value pair where key is the input name and\n",
      "                              value is the dimensions (including the batch dimension) to be used for that input.\n",
      "                              Each key-value pair has the key and value separated using a colon (:).\n",
      "                              Multiple input shapes can be provided via comma-separated key-value pairs, and each input \n",
      "                              name can contain at most one wildcard ('*') character.\n",
      "  --loadInputs=spec           Load input values from files (default = generate random inputs). Input names can be wrapped with single quotes (ex: 'Input:0')\n",
      "                              Input values spec ::= Ival[\",\"spec]\n",
      "                                           Ival ::= name\":\"file\n",
      "                              Consult the README for more information on generating files for custom inputs.\n",
      "  --iterations=N              Run at least N inference iterations (default = 10)\n",
      "  --warmUp=N                  Run for N milliseconds to warmup before measuring performance (default = 200)\n",
      "  --duration=N                Run performance measurements for at least N seconds wallclock time (default = 3)\n",
      "                              If -1 is specified, inference will keep running unless stopped manually\n",
      "  --sleepTime=N               Delay inference start with a gap of N milliseconds between launch and compute (default = 0)\n",
      "  --idleTime=N                Sleep N milliseconds between two continuous iterations(default = 0)\n",
      "  --infStreams=N              Instantiate N execution contexts to run inference concurrently (default = 1)\n",
      "  --exposeDMA                 Serialize DMA transfers to and from device (default = disabled).\n",
      "  --noDataTransfers           Disable DMA transfers to and from device (default = enabled).\n",
      "  --useManagedMemory          Use managed memory instead of separate host and device allocations (default = disabled).\n",
      "  --useSpinWait               Actively synchronize on GPU events. This option may decrease synchronization time but increase CPU usage and power (default = disabled)\n",
      "  --threads                   Enable multithreading to drive engines with independent threads or speed up refitting (default = disabled) \n",
      "  --useCudaGraph              Use CUDA graph to capture engine execution and then launch inference (default = disabled).\n",
      "                              This flag may be ignored if the graph capture fails.\n",
      "  --timeDeserialize           Time the amount of time it takes to deserialize the network and exit.\n",
      "  --timeRefit                 Time the amount of time it takes to refit the engine before inference.\n",
      "  --separateProfileRun        Do not attach the profiler in the benchmark run; if profiling is enabled, a second profile run will be executed (default = disabled)\n",
      "  --skipInference             Exit after the engine has been built and skip inference perf measurement (default = disabled)\n",
      "  --persistentCacheRatio      Set the persistentCacheLimit in ratio, 0.5 represent half of max persistent L2 size (default = 0)\n",
      "  --useProfile                Set the optimization profile for the inference context (default = 0 ).\n",
      "  --allocationStrategy=spec   Specify how the internal device memory for inference is allocated.\n",
      "                              Strategy: spec ::= \"static\"|\"profile\"|\"runtime\"\n",
      "                                  static = Allocate device memory based on max size across all profiles.\n",
      "                                  profile = Allocate device memory based on max size of the current profile.\n",
      "                                  runtime = Allocate device memory based on the actual input shapes.\n",
      "  --saveDebugTensors          Specify list of names of tensors to turn on the debug state\n",
      "                              and filename to save raw outputs to.\n",
      "                              These tensors must be specified as debug tensors during build time.\n",
      "                              Input values spec ::= Ival[\",\"spec]\n",
      "                                           Ival ::= name\":\"file\n",
      "  --weightStreamingBudget     Set the maximum amount of GPU memory TensorRT is allowed to use for weights.\n",
      "                              It can take on the following values:\n",
      "                                  -2: (default) Disable weight streaming at runtime.\n",
      "                                  -1: TensorRT will automatically decide the budget.\n",
      "                                   0-100%: Percentage of streamable weights that reside on the GPU.\n",
      "                                           0% saves the most memory but will have the worst performance.\n",
      "                                           Requires the '%' character.\n",
      "                                  >=0B: The exact amount of streamable weights that reside on the GPU. Supports the \n",
      "                                       following base-2 suffixes: B (Bytes), G (Gibibytes), K (Kibibytes), M (Mebibytes).\n",
      "\n",
      "=== Reporting Options ===\n",
      "  --verbose                   Use verbose logging (default = false)\n",
      "  --avgRuns=N                 Report performance measurements averaged over N consecutive iterations (default = 10)\n",
      "  --percentile=P1,P2,P3,...   Report performance for the P1,P2,P3,... percentages (0<=P_i<=100, 0 representing max perf, and 100 representing min perf; (default = 90,95,99%)\n",
      "  --dumpRefit                 Print the refittable layers and weights from a refittable engine\n",
      "  --dumpOutput                Print the output tensor(s) of the last inference iteration (default = disabled)\n",
      "  --dumpRawBindingsToFile     Print the input/output tensor(s) of the last inference iteration to file(default = disabled)\n",
      "  --dumpProfile               Print profile information per layer (default = disabled)\n",
      "  --dumpLayerInfo             Print layer information of the engine to console (default = disabled)\n",
      "  --dumpOptimizationProfile   Print the optimization profile(s) information (default = disabled)\n",
      "  --exportTimes=<file>        Write the timing results in a json file (default = disabled)\n",
      "  --exportOutput=<file>       Write the output tensors to a json file (default = disabled)\n",
      "  --exportProfile=<file>      Write the profile information per layer in a json file (default = disabled)\n",
      "  --exportLayerInfo=<file>    Write the layer information of the engine in a json file (default = disabled)\n",
      "\n",
      "=== System Options ===\n",
      "  --device=N                  Select cuda device N (default = 0)\n",
      "  --useDLACore=N              Select DLA core N for layers that support DLA (default = none)\n",
      "  --staticPlugins             Plugin library (.so) to load statically (can be specified multiple times)\n",
      "  --dynamicPlugins            Plugin library (.so) to load dynamically and may be serialized with the engine if they are included in --setPluginsToSerialize (can be specified multiple times)\n",
      "  --setPluginsToSerialize     Plugin library (.so) to be serialized with the engine (can be specified multiple times)\n",
      "  --ignoreParsedPluginLibs    By default, when building a version-compatible engine, plugin libraries specified by the ONNX parser \n",
      "                              are implicitly serialized with the engine (unless --excludeLeanRuntime is specified) and loaded dynamically. \n",
      "                              Enable this flag to ignore these plugin libraries instead.\n",
      "\n",
      "=== Help ===\n",
      "  --help, -h                  Print this message\n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v100900] [b34] # /usr/src/tensorrt/bin/trtexec\n"
     ]
    }
   ],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b129125-2fdf-41e2-bda0-7c1aa82f501e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cudnn_samples_v8  python3.10\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17dcd90-b7da-4d18-bbea-46c6f76de414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 file:/var/nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5  InRelease [1572 B]\n",
      "Get:1 file:/var/nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5  InRelease [1572 B]\n",
      "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease               \n",
      "Get:3 file:/var/nv-tensorrt-local-repo-ubuntu2204-10.3.0-cuda-12.5  Packages [5343 B]\n",
      "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease   \n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf63e66e-a4f6-4cad-ae4a-c03e76011e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libnvinfer-dev libnvinfer-dispatch-dev libnvinfer-headers-plugin-dev\n",
      "  libnvinfer-lean-dev libnvinfer-plugin-dev libnvinfer-samples\n",
      "  libnvinfer-vc-plugin-dev libnvonnxparsers-dev python3-libnvinfer\n",
      "  python3-libnvinfer-dev python3-libnvinfer-dispatch python3-libnvinfer-lean\n",
      "The following NEW packages will be installed:\n",
      "  libnvinfer-dispatch-dev libnvinfer-headers-plugin-dev libnvinfer-lean-dev\n",
      "  libnvinfer-plugin-dev libnvinfer-samples libnvinfer-vc-plugin-dev\n",
      "  libnvonnxparsers-dev python3-libnvinfer python3-libnvinfer-dev\n",
      "  python3-libnvinfer-dispatch python3-libnvinfer-lean tensorrt\n",
      "The following packages will be upgraded:\n",
      "  libnvinfer-dev\n",
      "1 upgraded, 12 newly installed, 0 to remove and 52 not upgraded.\n",
      "9 not fully installed or removed.\n",
      "Need to get 0 B/2459 MB of archives.\n",
      "After this operation, 6656 MB of additional disk space will be used.\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "(Reading database ... 40294 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libnvinfer-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-dev (10.9.0.34-1+cuda12.8) over (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-dispatch-dev.\n",
      "Preparing to unpack .../01-libnvinfer-dispatch-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-dispatch-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-headers-plugin-dev.\n",
      "Preparing to unpack .../02-libnvinfer-headers-plugin-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-headers-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-lean-dev.\n",
      "Preparing to unpack .../03-libnvinfer-lean-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-lean-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-plugin-dev.\n",
      "Preparing to unpack .../04-libnvinfer-plugin-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-vc-plugin-dev.\n",
      "Preparing to unpack .../05-libnvinfer-vc-plugin-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvinfer-vc-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvonnxparsers-dev.\n",
      "Preparing to unpack .../06-libnvonnxparsers-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking libnvonnxparsers-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package libnvinfer-samples.\n",
      "Preparing to unpack .../07-libnvinfer-samples_10.9.0.34-1+cuda12.8_all.deb ...\n",
      "Unpacking libnvinfer-samples (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer.\n",
      "Preparing to unpack .../08-python3-libnvinfer_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer-lean.\n",
      "Preparing to unpack .../09-python3-libnvinfer-lean_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer-lean (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer-dispatch.\n",
      "Preparing to unpack .../10-python3-libnvinfer-dispatch_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer-dispatch (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package python3-libnvinfer-dev.\n",
      "Preparing to unpack .../11-python3-libnvinfer-dev_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Selecting previously unselected package tensorrt.\n",
      "Preparing to unpack .../12-tensorrt_10.9.0.34-1+cuda12.8_amd64.deb ...\n",
      "Unpacking tensorrt (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-headers-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-plugin10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-vc-plugin10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvonnxparsers10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-dispatch10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-dispatch-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-lean10 (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvonnxparsers-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer-dispatch (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-headers-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-lean-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer-lean (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-vc-plugin-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-bin (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up libnvinfer-samples (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up python3-libnvinfer-dev (10.9.0.34-1+cuda12.8) ...\n",
      "Setting up tensorrt (10.9.0.34-1+cuda12.8) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get -y install tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "877fa306-906b-4f2e-a6f3-4d15bc251672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorrt\t10.9.0.34-1+cuda12.8\n"
     ]
    }
   ],
   "source": [
    "!dpkg-query -W tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9067648e-9ef8-44ea-be88-4b09c1b2ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s /usr/src/tensorrt/bin/trtexec /usr/local/bin/trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4d9fd-db17-430d-9a57-bdfb37640db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16f84b3d-58f9-4a0d-a7cb-b75ae4e0fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v100300] # trtexec\n",
      "=== Model Options ===\n",
      "  --onnx=<file>               ONNX model\n",
      "\n",
      "=== Build Options ===\n",
      "  --minShapes=spec                   Build with dynamic shapes using a profile with the min shapes provided\n",
      "  --optShapes=spec                   Build with dynamic shapes using a profile with the opt shapes provided\n",
      "  --maxShapes=spec                   Build with dynamic shapes using a profile with the max shapes provided\n",
      "  --minShapesCalib=spec              Calibrate with dynamic shapes using a profile with the min shapes provided\n",
      "  --optShapesCalib=spec              Calibrate with dynamic shapes using a profile with the opt shapes provided\n",
      "  --maxShapesCalib=spec              Calibrate with dynamic shapes using a profile with the max shapes provided\n",
      "                                     Note: All three of min, opt and max shapes must be supplied.\n",
      "                                           However, if only opt shapes is supplied then it will be expanded so\n",
      "                                           that min shapes and max shapes are set to the same values as opt shapes.\n",
      "                                           Input names can be wrapped with escaped single quotes (ex: 'Input:0').\n",
      "                                     Example input shapes spec: input0:1x3x256x256,input1:1x3x128x128\n",
      "                                     For scalars (0-D shapes), use input0:scalar or simply input0: with nothing after the colon.\n",
      "                                     Each input shape is supplied as a key-value pair where key is the input name and\n",
      "                                     value is the dimensions (including the batch dimension) to be used for that input.\n",
      "                                     Each key-value pair has the key and value separated using a colon (:).\n",
      "                                     Multiple input shapes can be provided via comma-separated key-value pairs, and each input name can\n",
      "                                     contain at most one wildcard ('*') character.\n",
      "  --inputIOFormats=spec              Type and format of each of the input tensors (default = all inputs in fp32:chw)\n",
      "                                     See --outputIOFormats help for the grammar of type and format list.\n",
      "                                     Note: If this option is specified, please set comma-separated types and formats for all\n",
      "                                           inputs following the same order as network inputs ID (even if only one input\n",
      "                                           needs specifying IO format) or set the type and format once for broadcasting.\n",
      "  --outputIOFormats=spec             Type and format of each of the output tensors (default = all outputs in fp32:chw)\n",
      "                                     Note: If this option is specified, please set comma-separated types and formats for all\n",
      "                                           outputs following the same order as network outputs ID (even if only one output\n",
      "                                           needs specifying IO format) or set the type and format once for broadcasting.\n",
      "                                     IO Formats: spec  ::= IOfmt[\",\"spec]\n",
      "                                                 IOfmt ::= type:fmt\n",
      "                                               type  ::= \"fp32\"|\"fp16\"|\"bf16\"|\"int32\"|\"int64\"|\"int8\"|\"uint8\"|\"bool\"\n",
      "                                               fmt   ::= (\"chw\"|\"chw2\"|\"chw4\"|\"hwc8\"|\"chw16\"|\"chw32\"|\"dhwc8\"|\n",
      "                                                          \"cdhw32\"|\"hwc\"|\"dla_linear\"|\"dla_hwc4\")[\"+\"fmt]\n",
      "  --memPoolSize=poolspec             Specify the size constraints of the designated memory pool(s)\n",
      "                                     Supports the following base-2 suffixes: B (Bytes), G (Gibibytes), K (Kibibytes), M (Mebibytes).\n",
      "                                     If none of suffixes is appended, the defualt unit is in MiB.\n",
      "                                     Note: Also accepts decimal sizes, e.g. 0.25M. Will be rounded down to the nearest integer bytes.\n",
      "                                     In particular, for dlaSRAM the bytes will be rounded down to the nearest power of 2.\n",
      "                                   Pool constraint: poolspec ::= poolfmt[\",\"poolspec]\n",
      "                                                      poolfmt ::= pool:size\n",
      "                                                    pool ::= \"workspace\"|\"dlaSRAM\"|\"dlaLocalDRAM\"|\"dlaGlobalDRAM\"|\"tacticSharedMem\"\n",
      "  --profilingVerbosity=mode          Specify profiling verbosity. mode ::= layer_names_only|detailed|none (default = layer_names_only).\n",
      "                                     Please only assign once.\n",
      "  --avgTiming=M                      Set the number of times averaged in each iteration for kernel selection (default = 8)\n",
      "  --refit                            Mark the engine as refittable. This will allow the inspection of refittable layers \n",
      "                                     and weights within the engine.\n",
      "  --stripWeights                     Strip weights from plan. This flag works with either refit or refit with identical weights. Default\n",
      "                                     to latter, but you can switch to the former by enabling both --stripWeights and --refit at the same\n",
      "                                     time.\n",
      "  --stripAllWeights                  Alias for combining the --refit and --stripWeights options. It marks all weights as refittable,\n",
      "                                     disregarding any performance impact. Additionally, it strips all refittable weights after the \n",
      "                                     engine is built.\n",
      "  --weightless                       [Deprecated] this knob has been deprecated. Please use --stripWeights\n",
      "  --versionCompatible, --vc          Mark the engine as version compatible. This allows the engine to be used with newer versions\n",
      "                                     of TensorRT on the same host OS, as well as TensorRT's dispatch and lean runtimes.\n",
      "  --pluginInstanceNorm, --pi         Set `kNATIVE_INSTANCENORM` to false in the ONNX parser. This will cause the ONNX parser to use\n",
      "                                     a plugin InstanceNorm implementation over the native implementation when parsing.\n",
      "  --useRuntime=runtime               TensorRT runtime to execute engine. \"lean\" and \"dispatch\" require loading VC engine and do\n",
      "                                     not support building an engine.\n",
      "                                           runtime::= \"full\"|\"lean\"|\"dispatch\"\n",
      "  --leanDLLPath=<file>               External lean runtime DLL to use in version compatiable mode.\n",
      "  --excludeLeanRuntime               When --versionCompatible is enabled, this flag indicates that the generated engine should\n",
      "                                     not include an embedded lean runtime. If this is set, the user must explicitly specify a\n",
      "                                     valid lean runtime to use when loading the engine.\n",
      "  --sparsity=spec                    Control sparsity (default = disabled). \n",
      "                                   Sparsity: spec ::= \"disable\", \"enable\", \"force\"\n",
      "                                     Note: Description about each of these options is as below\n",
      "                                           disable = do not enable sparse tactics in the builder (this is the default)\n",
      "                                           enable  = enable sparse tactics in the builder (but these tactics will only be\n",
      "                                                     considered if the weights have the right sparsity pattern)\n",
      "                                           force   = enable sparse tactics in the builder and force-overwrite the weights to have\n",
      "                                                     a sparsity pattern (even if you loaded a model yourself)\n",
      "                                                     [Deprecated] this knob has been deprecated.\n",
      "                                                     Please use <polygraphy surgeon prune> to rewrite the weights.\n",
      "  --noTF32                           Disable tf32 precision (default is to enable tf32, in addition to fp32)\n",
      "  --fp16                             Enable fp16 precision, in addition to fp32 (default = disabled)\n",
      "  --bf16                             Enable bf16 precision, in addition to fp32 (default = disabled)\n",
      "  --int8                             Enable int8 precision, in addition to fp32 (default = disabled)\n",
      "  --fp8                              Enable fp8 precision, in addition to fp32 (default = disabled)\n",
      "  --int4                             Enable int4 precision, in addition to fp32 (default = disabled)\n",
      "  --best                             Enable all precisions to achieve the best performance (default = disabled)\n",
      "  --stronglyTyped                    Create a strongly typed network. (default = disabled)\n",
      "  --directIO                         Avoid reformatting at network boundaries. (default = disabled)\n",
      "  --precisionConstraints=spec        Control precision constraint setting. (default = none)\n",
      "                                       Precision Constraints: spec ::= \"none\" | \"obey\" | \"prefer\"\n",
      "                                         none = no constraints\n",
      "                                         prefer = meet precision constraints set by --layerPrecisions/--layerOutputTypes if possible\n",
      "                                         obey = meet precision constraints set by --layerPrecisions/--layerOutputTypes or fail\n",
      "                                                otherwise\n",
      "  --layerPrecisions=spec             Control per-layer precision constraints. Effective only when precisionConstraints is set to\n",
      "                                   \"obey\" or \"prefer\". (default = none)\n",
      "                                   The specs are read left-to-right, and later ones override earlier ones. Each layer name can\n",
      "                                     contain at most one wildcard ('*') character.\n",
      "                                   Per-layer precision spec ::= layerPrecision[\",\"spec]\n",
      "                                                       layerPrecision ::= layerName\":\"precision\n",
      "                                                       precision ::= \"fp32\"|\"fp16\"|\"bf16\"|\"int32\"|\"int8\"\n",
      "  --layerOutputTypes=spec            Control per-layer output type constraints. Effective only when precisionConstraints is set to\n",
      "                                   \"obey\" or \"prefer\". (default = none\n",
      "                                   The specs are read left-to-right, and later ones override earlier ones. Each layer name can\n",
      "                                     contain at most one wildcard ('*') character. If a layer has more than\n",
      "                                   one output, then multiple types separated by \"+\" can be provided for this layer.\n",
      "                                   Per-layer output type spec ::= layerOutputTypes[\",\"spec]\n",
      "                                                         layerOutputTypes ::= layerName\":\"type\n",
      "                                                         type ::= \"fp32\"|\"fp16\"|\"bf16\"|\"int32\"|\"int8\"[\"+\"type]\n",
      "  --layerDeviceTypes=spec            Specify layer-specific device type.\n",
      "                                     The specs are read left-to-right, and later ones override earlier ones. If a layer does not have\n",
      "                                     a device type specified, the layer will opt for the default device type.\n",
      "                                   Per-layer device type spec ::= layerDeviceTypePair[\",\"spec]\n",
      "                                                         layerDeviceTypePair ::= layerName\":\"deviceType\n",
      "                                                           deviceType ::= \"GPU\"|\"DLA\"\n",
      "  --calib=<file>                     Read INT8 calibration cache file\n",
      "  --safe                             Enable build safety certified engine, if DLA is enable, --buildDLAStandalone will be specified\n",
      "                                     automatically (default = disabled)\n",
      "  --buildDLAStandalone               Enable build DLA standalone loadable which can be loaded by cuDLA, when this option is enabled, \n",
      "                                     --allowGPUFallback is disallowed and --skipInference is enabled by default. Additionally, \n",
      "                                     specifying --inputIOFormats and --outputIOFormats restricts I/O data type and memory layout\n",
      "                                     (default = disabled)\n",
      "  --allowGPUFallback                 When DLA is enabled, allow GPU fallback for unsupported layers (default = disabled)\n",
      "  --consistency                      Perform consistency checking on safety certified engine\n",
      "  --restricted                       Enable safety scope checking with kSAFETY_SCOPE build flag\n",
      "  --saveEngine=<file>                Save the serialized engine\n",
      "  --loadEngine=<file>                Load a serialized engine\n",
      "  --getPlanVersionOnly               Print TensorRT version when loaded plan was created. Works without deserialization of the plan.\n",
      "                                     Use together with --loadEngine. Supported only for engines created with 8.6 and forward.\n",
      "  --tacticSources=tactics            Specify the tactics to be used by adding (+) or removing (-) tactics from the default \n",
      "                                     tactic sources (default = all available tactics).\n",
      "                                     Note: Currently only cuDNN, cuBLAS, cuBLAS-LT, and edge mask convolutions are listed as optional\n",
      "                                           tactics.\n",
      "                                   Tactic Sources: tactics ::= [\",\"tactic]\n",
      "                                                     tactic  ::= (+|-)lib\n",
      "                                                   lib     ::= \"CUBLAS\"|\"CUBLAS_LT\"|\"CUDNN\"|\"EDGE_MASK_CONVOLUTIONS\"\n",
      "                                                               |\"JIT_CONVOLUTIONS\"\n",
      "                                     For example, to disable cudnn and enable cublas: --tacticSources=-CUDNN,+CUBLAS\n",
      "  --noBuilderCache                   Disable timing cache in builder (default is to enable timing cache)\n",
      "  --noCompilationCache               Disable Compilation cache in builder, and the cache is part of timing cache (default is to enable compilation cache)\n",
      "  --errorOnTimingCacheMiss           Emit error when a tactic being timed is not present in the timing cache (default = false)\n",
      "  --timingCacheFile=<file>           Save/load the serialized global timing cache\n",
      "  --preview=features                 Specify preview feature to be used by adding (+) or removing (-) preview features from the default\n",
      "                                   Preview Features: features ::= [\",\"feature]\n",
      "                                                       feature  ::= (+|-)flag\n",
      "                                                     flag     ::= \"aliasedPluginIO1003\"\n",
      "                                                                  |\"profileSharing0806\"\n",
      "  --builderOptimizationLevel         Set the builder optimization level. (default is 3)\n",
      "                                     Higher level allows TensorRT to spend more building time for more optimization options.\n",
      "                                     Valid values include integers from 0 to the maximum optimization level, which is currently 5.\n",
      "  --hardwareCompatibilityLevel=mode  Make the engine file compatible with other GPU architectures. (default = none)\n",
      "                                   Hardware Compatibility Level: mode ::= \"none\" | \"ampere+\"\n",
      "                                         none = no compatibility\n",
      "                                         ampere+ = compatible with Ampere and newer GPUs\n",
      "  --runtimePlatform=platform         Set the target platform for runtime execution. (default = SameAsBuild)\n",
      "                                     When this option is enabled, --skipInference is enabled by default.\n",
      "                                   RuntimePlatfrom: platform ::= \"SameAsBuild\" | \"WindowsAMD64\"\n",
      "                                         SameAsBuild = no requirement for cross-platform compatibility.\n",
      "                                         WindowsAMD64 = set the target platform for engine execution as Windows AMD64 system\n",
      "  --tempdir=<dir>                    Overrides the default temporary directory TensorRT will use when creating temporary files.\n",
      "                                     See IRuntime::setTemporaryDirectory API documentation for more information.\n",
      "  --tempfileControls=controls        Controls what TensorRT is allowed to use when creating temporary executable files.\n",
      "                                     Should be a comma-separated list with entries in the format (in_memory|temporary):(allow|deny).\n",
      "                                     in_memory: Controls whether TensorRT is allowed to create temporary in-memory executable files.\n",
      "                                     temporary: Controls whether TensorRT is allowed to create temporary executable files in the\n",
      "                                                filesystem (in the directory given by --tempdir).\n",
      "                                     For example, to allow in-memory files and disallow temporary files:\n",
      "                                         --tempfileControls=in_memory:allow,temporary:deny\n",
      "                                     If a flag is unspecified, the default behavior is \"allow\".\n",
      "  --maxAuxStreams=N                  Set maximum number of auxiliary streams per inference stream that TRT is allowed to use to run \n",
      "                                     kernels in parallel if the network contains ops that can run in parallel, with the cost of more \n",
      "                                     memory usage. Set this to 0 for optimal memory usage. (default = using heuristics)\n",
      "  --profile                          Build with dynamic shapes using a profile with the min/max/opt shapes provided. Can be specified\n",
      "                                         multiple times to create multiple profiles with contiguous index.\n",
      "                                     (ex: --profile=0 --minShapes=<spec> --optShapes=<spec> --maxShapes=<spec> --profile=1 ...)\n",
      "  --calibProfile                     Select the optimization profile to calibrate by index. (default = 0)\n",
      "  --allowWeightStreaming             Enable a weight streaming engine. Must be specified with --stronglyTyped. TensorRT will disable\n",
      "                                     weight streaming at runtime unless --weightStreamingBudget is specified.\n",
      "  --markDebug                        Specify list of names of tensors to be marked as debug tensors. Separate names with a comma\n",
      "\n",
      "=== Inference Options ===\n",
      "  --shapes=spec               Set input shapes for dynamic shapes inference inputs.\n",
      "                              Note: Input names can be wrapped with escaped single quotes (ex: 'Input:0').\n",
      "                              Example input shapes spec: input0:1x3x256x256, input1:1x3x128x128\n",
      "                              For scalars (0-D shapes), use input0:scalar or simply input0: with nothing after the colon.\n",
      "                              Each input shape is supplied as a key-value pair where key is the input name and\n",
      "                              value is the dimensions (including the batch dimension) to be used for that input.\n",
      "                              Each key-value pair has the key and value separated using a colon (:).\n",
      "                              Multiple input shapes can be provided via comma-separated key-value pairs, and each input \n",
      "                              name can contain at most one wildcard ('*') character.\n",
      "  --loadInputs=spec           Load input values from files (default = generate random inputs). Input names can be wrapped with single quotes (ex: 'Input:0')\n",
      "                            Input values spec ::= Ival[\",\"spec]\n",
      "                                         Ival ::= name\":\"file\n",
      "                              Consult the README for more information on generating files for custom inputs.\n",
      "  --iterations=N              Run at least N inference iterations (default = 10)\n",
      "  --warmUp=N                  Run for N milliseconds to warmup before measuring performance (default = 200)\n",
      "  --duration=N                Run performance measurements for at least N seconds wallclock time (default = 3)\n",
      "                              If -1 is specified, inference will keep running unless stopped manually\n",
      "  --sleepTime=N               Delay inference start with a gap of N milliseconds between launch and compute (default = 0)\n",
      "  --idleTime=N                Sleep N milliseconds between two continuous iterations(default = 0)\n",
      "  --infStreams=N              Instantiate N execution contexts to run inference concurrently (default = 1)\n",
      "  --exposeDMA                 Serialize DMA transfers to and from device (default = disabled).\n",
      "  --noDataTransfers           Disable DMA transfers to and from device (default = enabled).\n",
      "  --useManagedMemory          Use managed memory instead of separate host and device allocations (default = disabled).\n",
      "  --useSpinWait               Actively synchronize on GPU events. This option may decrease synchronization time but increase CPU usage and power (default = disabled)\n",
      "  --threads                   Enable multithreading to drive engines with independent threads or speed up refitting (default = disabled) \n",
      "  --useCudaGraph              Use CUDA graph to capture engine execution and then launch inference (default = disabled).\n",
      "                              This flag may be ignored if the graph capture fails.\n",
      "  --timeDeserialize           Time the amount of time it takes to deserialize the network and exit.\n",
      "  --timeRefit                 Time the amount of time it takes to refit the engine before inference.\n",
      "  --separateProfileRun        Do not attach the profiler in the benchmark run; if profiling is enabled, a second profile run will be executed (default = disabled)\n",
      "  --skipInference             Exit after the engine has been built and skip inference perf measurement (default = disabled)\n",
      "  --persistentCacheRatio      Set the persistentCacheLimit in ratio, 0.5 represent half of max persistent L2 size (default = 0)\n",
      "  --useProfile                Set the optimization profile for the inference context (default = 0 ).\n",
      "  --allocationStrategy=spec   Specify how the internal device memory for inference is allocated.\n",
      "                            Strategy: spec ::= \"static\", \"profile\", \"runtime\"\n",
      "                                  static = Allocate device memory based on max size across all profiles.\n",
      "                                  profile = Allocate device memory based on max size of the current profile.\n",
      "                                  runtime = Allocate device memory based on the actual input shapes.\n",
      "  --saveDebugTensors          Specify list of names of tensors to turn on the debug state\n",
      "                              and filename to save raw outputs to.\n",
      "                              These tensors must be specified as debug tensors during build time.\n",
      "                            Input values spec ::= Ival[\",\"spec]\n",
      "                                         Ival ::= name\":\"file\n",
      "  --weightStreamingBudget     Set the maximum amount of GPU memory TensorRT is allowed to use for weights.\n",
      "                              It can take on the following values:\n",
      "                                -2: (default) Disable weight streaming at runtime.\n",
      "                                -1: TensorRT will automatically decide the budget.\n",
      "                                 0-100%: Percentage of streamable weights that reside on the GPU.\n",
      "                                         0% saves the most memory but will have the worst performance.\n",
      "                                         Requires the % character.\n",
      "                                >=0B: The exact amount of streamable weights that reside on the GPU. Supports the \n",
      "                                     following base-2 suffixes: B (Bytes), G (Gibibytes), K (Kibibytes), M (Mebibytes).\n",
      "\n",
      "=== Reporting Options ===\n",
      "  --verbose                   Use verbose logging (default = false)\n",
      "  --avgRuns=N                 Report performance measurements averaged over N consecutive iterations (default = 10)\n",
      "  --percentile=P1,P2,P3,...   Report performance for the P1,P2,P3,... percentages (0<=P_i<=100, 0 representing max perf, and 100 representing min perf; (default = 90,95,99%)\n",
      "  --dumpRefit                 Print the refittable layers and weights from a refittable engine\n",
      "  --dumpOutput                Print the output tensor(s) of the last inference iteration (default = disabled)\n",
      "  --dumpRawBindingsToFile     Print the input/output tensor(s) of the last inference iteration to file(default = disabled)\n",
      "  --dumpProfile               Print profile information per layer (default = disabled)\n",
      "  --dumpLayerInfo             Print layer information of the engine to console (default = disabled)\n",
      "  --dumpOptimizationProfile   Print the optimization profile(s) information (default = disabled)\n",
      "  --exportTimes=<file>        Write the timing results in a json file (default = disabled)\n",
      "  --exportOutput=<file>       Write the output tensors to a json file (default = disabled)\n",
      "  --exportProfile=<file>      Write the profile information per layer in a json file (default = disabled)\n",
      "  --exportLayerInfo=<file>    Write the layer information of the engine in a json file (default = disabled)\n",
      "\n",
      "=== System Options ===\n",
      "  --device=N                  Select cuda device N (default = 0)\n",
      "  --useDLACore=N              Select DLA core N for layers that support DLA (default = none)\n",
      "  --staticPlugins             Plugin library (.so) to load statically (can be specified multiple times)\n",
      "  --dynamicPlugins            Plugin library (.so) to load dynamically and may be serialized with the engine if they are included in --setPluginsToSerialize (can be specified multiple times)\n",
      "  --setPluginsToSerialize     Plugin library (.so) to be serialized with the engine (can be specified multiple times)\n",
      "  --ignoreParsedPluginLibs    By default, when building a version-compatible engine, plugin libraries specified by the ONNX parser \n",
      "                              are implicitly serialized with the engine (unless --excludeLeanRuntime is specified) and loaded dynamically. \n",
      "                              Enable this flag to ignore these plugin libraries instead.\n",
      "\n",
      "=== Help ===\n",
      "  --help, -h                  Print this message\n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v100300] # trtexec\n"
     ]
    }
   ],
   "source": [
    "!trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6c505d-cfc2-4605-a974-8010dce0fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-10 02:41:34--  https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz\n",
      "Resolving developer.nvidia.com (developer.nvidia.com)... 2.17.35.66, 2.17.35.153\n",
      "Connecting to developer.nvidia.com (developer.nvidia.com)|2.17.35.66|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://developer.download.nvidia.com/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz [following]\n",
      "--2025-03-10 02:41:35--  https://developer.download.nvidia.com/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz\n",
      "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 2.18.188.11, 2.18.188.32\n",
      "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|2.18.188.11|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 4381146404 (4.1G) [application/x-gzip]\n",
      "Saving to: ‘TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz’\n",
      "\n",
      "TensorRT-10.3.0.26. 100%[===================>]   4.08G  3.85MB/s    in 19m 25s \n",
      "\n",
      "2025-03-10 03:01:01 (3.59 MB/s) - ‘TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz’ saved [4381146404/4381146404]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ced2fbd-8e00-4241-a7d0-6822e1813bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!tar -xvzf TensorRT-10.3.0.26.Linux.x86_64-gnu.cuda-12.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18949160-5786-4140-b444-8205aedb1f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf /home/TensorRT-10.3.0.26/bin/trtexec /usr/local/bin/trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f37cd0b-be54-4b78-a570-4a4c951d3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!TENSORRT_PATH=$(pwd)/TensorRT-10.3.0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da547e83-5009-49cf-9fd5-7b8aab09de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"export TENSORRT_HOME=${TENSORRT_PATH}\" >> ~/.bashrc\n",
    "!echo \"export PATH=\\$TENSORRT_HOME/bin:\\$PATH\" >> ~/.bashrc\n",
    "!echo \"export LD_LIBRARY_PATH=\\$TENSORRT_HOME/lib:\\$LD_LIBRARY_PATH\" >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10ab49d5-8de8-49a0-8e9a-ee48182c2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "182bd871-f29b-4941-8010-2b62e6f102f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Requirement './tensorrt-10.3.0-cp39-none-linux_x86_64.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: tensorrt-10.3.0-cp39-none-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd TensorRT-10.3.0.26/python/\n",
    "!pip install ./tensorrt-10.3.0-cp39-none-linux_x86_64.whl\n",
    "!cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a7c2f5c-6c84-449a-85bc-329e8f8f5d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==1.0.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types==0.7.0 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.7.0)\n",
      "Collecting cachetools==5.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting certifi==2021.10.8\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.2/149.2 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting charset-normalizer==2.0.12\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting chart-studio==1.1.0\n",
      "  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting cloudpickle==3.0.0\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: coloredlogs==15.0.1 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (15.0.1)\n",
      "Collecting contextlib2==21.6.0\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: cppimport==22.8.2 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (22.8.2)\n",
      "Collecting cycler==0.11.0\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting dicom2nifti==2.3.4\n",
      "  Downloading dicom2nifti-2.3.4.tar.gz (35 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting diffusers==0.30.3\n",
      "  Downloading diffusers-0.30.3-py3-none-any.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting einops==0.4.1\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Collecting filelock==3.16.1\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting flatbuffers==24.3.25\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting fonttools==4.31.1\n",
      "  Downloading fonttools-4.31.1-py3-none-any.whl (899 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.4/899.4 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting fsspec==2024.9.0\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 KB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting future==0.18.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 KB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "Collecting google-auth==2.6.0\n",
      "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 KB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting google-auth-oauthlib==0.4.6\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting grpcio==1.44.0\n",
      "  Downloading grpcio-1.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting huggingface-hub==0.25.0\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl (436 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 KB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: humanfriendly==10.0 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (10.0)\n",
      "Collecting idna==3.3\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 KB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting imageio==2.16.1\n",
      "  Downloading imageio-2.16.1-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting importlib-metadata==4.11.2\n",
      "  Downloading importlib_metadata-4.11.2-py3-none-any.whl (17 kB)\n",
      "Collecting Jinja2==3.1.4\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 KB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting joblib==1.1.0\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 KB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting kiwisolver==1.4.0\n",
      "  Downloading kiwisolver-1.4.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting linecache2==1.0.0\n",
      "  Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting Markdown==3.3.6\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown-it-py==3.0.0 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 34)) (3.0.0)\n",
      "Collecting matplotlib==3.5.1\n",
      "  Downloading matplotlib-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mdurl==0.1.2 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (0.1.2)\n",
      "Collecting MedPy==0.4.0\n",
      "  Downloading MedPy-0.4.0.tar.gz (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 KB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "Collecting ml_collections==0.1.1\n",
      "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "Collecting monai==1.3.2\n",
      "  Downloading monai-1.3.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: mpmath==1.3.0 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (1.3.0)\n",
      "Collecting netron==7.8.9\n",
      "  Downloading netron-7.8.9-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "Collecting networkx==2.6.3\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nibabel==3.2.2\n",
      "  Downloading nibabel-3.2.2-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Collecting ninja==1.11.1.1\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 KB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting numpy==1.22.4\n",
      "  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 50)) (9.1.0.70)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-modelopt==0.17.0\n",
      "  Downloading nvidia_modelopt-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mmm\n",
      "Collecting nvidia-nccl-cu12==2.20.5\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.68\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting oauthlib==3.2.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.5/151.5 KB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting onnx==1.16.1\n",
      "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting onnx-graphsurgeon==0.5.2\n",
      "  Downloading onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: onnxconverter-common==1.14.0 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 62)) (1.14.0)\n",
      "Collecting onnxmltools==1.12.0\n",
      "  Downloading onnxmltools-1.12.0-py2.py3-none-any.whl (329 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting onnxruntime-gpu==1.18.0\n",
      "  Downloading onnxruntime_gpu-1.18.0-cp310-cp310-manylinux_2_28_x86_64.whl (199.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting onnxruntime_extensions==0.12.0\n",
      "  Downloading onnxruntime_extensions-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "Collecting opencv-python==4.5.5.64\n",
      "  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting packaging==21.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pandas==1.3.5\n",
      "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting patsy==0.5.2\n",
      "  Downloading patsy-0.5.2-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.7/233.7 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting Pillow==9.0.1\n",
      "  Downloading Pillow-9.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "Collecting plotly==5.6.0\n",
      "  Downloading plotly-5.6.0-py2.py3-none-any.whl (27.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.7/27.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "Collecting polygraphy==0.49.9\n",
      "  Downloading polygraphy-0.49.9-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.9/346.9 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf==3.20.2 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 73)) (3.20.2)\n",
      "Collecting psutil==6.0.0\n",
      "  Downloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.5/290.5 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting ptflops==0.6.8\n",
      "  Downloading ptflops-0.6.8.tar.gz (12 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting PuLP==2.9.0\n",
      "  Downloading PuLP-2.9.0-py3-none-any.whl (17.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting pyasn1==0.4.8\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 KB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pyasn1-modules==0.2.8\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 KB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pybind11==2.13.6 in /venv/main/lib/python3.10/site-packages (from -r requirements.txt (line 79)) (2.13.6)\n",
      "Collecting pydantic==2.9.2\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Collecting pydantic_core==2.23.4\n",
      "  Using cached pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Collecting pydicom==2.3.0\n",
      "  Downloading pydicom-2.3.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "Collecting Pygments==2.18.0\n",
      "  Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "Collecting pynvml==11.5.3\n",
      "  Downloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pyparsing==3.0.7\n",
      "  Downloading pyparsing-3.0.7-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 KB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dateutil==2.8.2\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 KB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pytz==2021.3\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.5/503.5 KB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting PyWavelets==1.3.0\n",
      "  Downloading PyWavelets-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "Collecting PyYAML==6.0\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 KB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting regex==2024.9.11\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 KB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting requests==2.27.1\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting requests-oauthlib==1.3.1\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting retrying==1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "doneing metadata (setup.py) ... \u001b[?25l\n",
      "Collecting rich==13.8.1\n",
      "  Downloading rich-13.8.1-py3-none-any.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 KB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa==4.8\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting safetensors==0.4.5\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 KB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting scikit-image==0.19.2\n",
      "  Downloading scikit_image-0.19.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting scikit-learn==1.0.2\n",
      "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting scipy==1.10.1\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting seaborn==0.11.2\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 KB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement SimpleITK==2.0.2 (from versions: 1.0.1, 1.2.0, 2.1.0, 2.1.1.1, 2.1.1.2, 2.2.0, 2.2.1, 2.3.0, 2.3.1, 2.4.0, 2.4.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for SimpleITK==2.0.2\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7463f4-821e-4107-aed7-77d0a499bfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
